@inproceedings{Fong2010,
  author = {Peter Kin-Fong Fong and Robert P. Biuk-Aghai},
  title = {What Did They Do?  Deriving High-Level Edit Histories
	in Wikis},
  booktitle = "Proceedings of the 6th International Symposium on Wikis",
  series = {WikiSym 2010},
  publisher = "{ACM} Press",
  year = 2010
}

@inproceedings{Sabel2007,
  author = {Mikalai Sabel},
  title = {Structuring Wiki Revision History},
  booktitle = "Proceedings of the 3rd International Symposium on Wikis",
  series = {WikiSym 2007},
  publisher = "{ACM} Press",
  year = 2007,
  annote = {Analyzes a tree of revisions, like WikiTravel.
    They use edit distance to find a prior revision to which a new
    revision is most similar [Ekstrand2009].},
}

@inproceedings{Ekstrand2009,
  author = {Michael D. Ekstrand and John T. Riedl},
  title = {rv you're dumb: Identifying Discarded Work in
    {Wiki} Article History},
  booktitle = "Proceedings of the 5th International Symposium on Wikis",
  series = {WikiSym 2009},
  publisher = "{ACM} Press",
  year = 2009,
  annote = {Analyzes a tree of revisions, like WikiTravel.
    Cited by Fong2010.},
}


@inproceedings{Tang2008,
  author ={Libby Veng-Sam Tang and Robert P. Biuk-Aghai and Simon Fong},
  title = {A Method for Measuring Co-authorship Relationships
      in {MediaWiki}},
  booktitle = {Proceedings of the 4th International Symposium on Wikis},
  series = {WikiSym 2008},
  publisher = "{ACM} Press",
  year = 2008,
  annote = {This seems to just assume that anyone in the edit history
      is an author of the article.  It's more about analyzing the
      social network that arises from groups of people editing the
      same groups of articles.}
}

@techreport{Hunt1976,
  author = {J. W. Hunt and M. D. McIlroy},
  year = 1976,
  title = {An Algorithm for Differential File Comparison},
  type = {Computing Science Technical Report},
  institution = {Bell Laboratories},
  number = 41,
  annote = {Paper for the \texttt{diff} utility.  Uses longest common
    subsequence approach.}
}

@article{Tichy1984,
    author = "Walter F. Tichy",
    title = "The String-to-String Correction Problem with Block Move",
    journal = "{ACM} Transactions on Computer Systems",
    volume = 2,
    number = 4,
    pages = {309--321},
    year = 1984,
    annote = {Loops through the target string, and greedily chooses
        the longest match it can find.
        Introduces the idea of having an index of positions
        for all symbols in the alphabet (first improvement).
        Also uses suffix trees, making the final algorithm
        have total runnint time and space requirements of O(m + n).
    }
}

@book{Gusfield1999,
    author = {Dan Gusfield},
    title = {Algorithms on Strings, Trees and Sequences:
	Computer Science and Computational Biology},
    publisher = {Cambridge University Press},
    year = 1999,
}

@article{Wagner1974,
    author = {Robert A. Wagner and Michael J. Fischer},
    title = {The String-to-String Correction Problem},
    journal = {Journal of the Association for Computing Machinery},
    volume = 21,
    number = 1,
    year = 1974,
    month = Jan,
    pages = {168--173},
}

@article{Levenshtein1966,
    author = "V. I. Levenshtein",
    title = "Binary Codes Capable of Correcting Insertions and Reversals",
    journal = "Soviet Physics Doklady",
    volume = 10,
    pages = {707--710},
    year =1966,
}

@article{Damerau1964,
  author = {Fred J. Damerau},
  title = {A Technique for Computer Detection and Correction of
      Spelling Errors},
  journal = {Communications of the ACM},
  volume = 7,
  number = 3,
  year = 1964,
  month = Mar,
}

@book{Sankoff1999,
  editor = {David Sankoff and Joseph B. Kruskal},
  title = {Time Warps, String Edits, and Macromolecules:
    the Theory and Practice of Sequence Comparison},
  publisher = {CSLI Publications},
  year = 1999,
  annote = {Originally published 1983 by Addison-Wesley},
}


@article{Myers1986,
    author = "Eugene W. Myers",
    title = "An {O(ND)} Difference Algorithm and its Variations",
    journal = "Algorithmica",
    volume = 1,
    number = 2,
    pages = {251--266},
    year = 1986,
    annote = {This paper shows that finding the longest common
        subsequence and shortest edit script are equivalent to
        finding the shortest and longest paths in an edit graph.
        He uses this observation to develop a new algorithm
        which has time and space requirements of O(M+N+D),
        where D is the size of the minimum edit script.},
}

@inproceedings{Burns1997,
    author = "Randal C. Burns and Darrell D.E. Long",
    title = "A Linear Time, Constant Space Differencing Algorithm",
    booktitle = {IPCCC 1997: Performance, Computing, and Communication
	Conference},
	pages = "429--436",
    publisher = "{IEEE} International",
    year = 1997,
    annote = {
	In part II, they show that a greedy algorithm is optimal,
	but takes time O(l1 * l2).  Their assumption is that all
	symbols appear in the original revision, so that the full
	edit script is simply a collection of Move operations.
	Their proof shows that in this case, the optimal encoding
	and the greedy method have the same size edit script.

	In II.B, they talk about the hash of string prefixes.
	They use a footprint, rather than the actual actual symbols.
	The number of matching footprints can grow at O(l1 * l2);
	consider case where original document is "the the" and
	new document is duplicated 2 times.
	The hash itself grows at O(l2).

	Part III.  To get linear time performance, they only
	consider matches that are beyond the offset of the
	previous match.  The actual algorithm uses a hash table
	constructed by looping through both versions simultaneously.

        Their final algorithm runs in O(M+N) and uses constant space, O(1).
    }
}

@inproceedings{Obst1987,
    author = {Wolfgang Obst},
    title = {Delta Technique and String-to-String Correction},
    booktitle = {{ESEC} 1987: Proceedings of the 1st European Software Engineering
	Conference},
    pages = {64--68},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    volume = 289,
    year = 1987,
    annote = {
	Introduces the idea of hashing on string prefixes?
        Actually, Tichy1984 already mentions having an index
        for each symbol in the alphabet.
        Probably, this paper introduces the idea of using
        string prefixes to improve performance.
    }
}

@inproceedings{Reichenberger1991,
    author = {Christoph Reichenberger},
    title = {Delta Storage for Arbitrary Non-Text Files},
    booktitle = {Proceedings of the 3rd International Workshop
	on Software Configuration Management},
    pages = {144--152},
    publisher = {ACM},
    year = 1991,
    annote = {
	References Obst1987 for hashing of string prefixes.
        Tichy1984 already includes an index of every symbol
        in the alphabet, so maybe Obst introduces the
        idea of multi-symbol prefixes to improve performance.

	The key insight added by this paper is that you can
	coalesce individual byte delete operations while
	you are looping over the list of words.
	This works because they are choosing the longest
	possible match, working from left to right,
	rather than the longest possible match in the whole
	document.

        Seems to describe Tichy1984 as having the one-byte Inserts,
        which I suppose is right given how Tichy wrote the
        description of the algorithm.
    }
}


@inproceedings{Neuwirth1992,
    author = {Christine M. Neuwirth and Ravinder Chandhok
	and David S. Kaufer and Paul Erion and
	    James Morris and Dale Miller},
    title = {Flexible Diff-ing In A Collaborative Writing System},
    booktitle = {Proceedings of the 1992 ACM
	Conference on Computer-Supported Cooperative Work},
    series = {CSCW 1992},
    pages = {147--154},
    publisher = {ACM},
    year = 1992,
}

@article{Hirschberg1977,
    author = "Daniel S. Hirschberg",
    title = "Algorithms for the Longest Common Subsequence Problem",
    journal = {Journal of the Association for Computing Machinery},
    volume = 24, number = 4, year = 1977,
    pages = "664--675"
}

@misc{Kinzler2011,
    author = {Daniel Kinzler},
    title = {Personal communication},
    year = 2011,
    month = Jan,
    annote = {Daniel suggested just using stop-word lists
	when I described my problem of short matches like ``of.''
	Apparently, he has also been working on computational
	linguistics, and this approach works better than the
	blunt ``no phrases less than 4 words.''
    },
}

@article{Smith1981,
    author = "T. F. Smith and M. S. Waterman",
    title = "Identification of Common Molecular Subsequences",
    journal = "Journal of Molecular Biology",
    volume = 147,
    pages = {195--197},
    year = 1981,
    annote = {This was a Letters to the Editor!
      There is a Wikipedia article on this algorithm.

      This algorithm looks for best scoring alignments,
      and then resets the scores to find the next best
      local alignment.  There is a resemblance to our work
      here, except that we do not allow insertions and deletions
      in the middle of our match.
    }
}

@article{Lowrance1975,
    author = {Roy Lowrance and Robert A. Wagner},
    title = {An Extension of the String-to-String Correction Problem},
    journal = {Journal of the Association for Computing Machinery},
    volume = 22,
    number = 2,
    pages = {177--183},
    month = Apr,
    year = 1975,
    annote = {Introduces swaps.},
}

@inproceedings{Wagner1975,
  author = {Robert A. Wagner},
  title = {On the Complexity of the Extended String-to-String
    Correction Problem},
  booktitle = "Proceedings of the 7th Annual ACM Symposium on
    Theory of Computing",
  series = {STOC 1975},
  publisher = "{ACM}",
  year = 1975,
  annote = {Also in Sankoff1999, Ch 7.},
}

