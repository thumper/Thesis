\section{Related Work}

What is vandalism: \cite{wiki:vandalism}, \cite{Viegas2004},
\cite{Priedhorsky2007}
The official definition for vandalism on the Wikipedia defines it as
``a \textit{deliberate} attempt to compromise the integrity
of Wikipedia''~\cite{wiki:vandalism}.
This definition is open to interpretation, so it can't be cleanly
formulated.
Proxies for this definition abound:
\begin{itemize}
\item \textbf{revert} -- test earlier revisions for textual equality.
\item \textbf{comment revert} -- the comment indicates that the edit
    is a revert.
\item \textbf{rollback} -- the comment indicates that it is a
    rollback; rollbacks can only be performed by editors with
    administrative capabilities.
\item \textbf{edit quality} -- compare textually against future
    revisions to discover how the edit fares.
\item \textbf{manual annotation} -- we know vandalism when we see it;
    bad because people reasonably disagree about what rises to the
    level of vandalism.
\end{itemize}

Most works conservatively choose reverted revisions as being labeled
as \textit{vandalism}, but this is certainly has problems since there
are other cases of vandalism that will be incorrectly labeled as
\textit{regular}.
A few groups have gone to the effort of manually annotating small
corpora of edits, but this introduces the bias of the researcher in
what constitutes vandalism (which is really more of an evolving
community standard), as well as not being scalable to larger corpora.
Potthast demonstrates that it is possible to overcome both of these
obstacles by crowd-sourcing a larger annotated corpus via
Amazon's Mechanical Turk system~\cite{Potthast2010a}.

Chin\etal focus on NLP techniques, but used supervised active
learning (lookup that phrase) to obtain new annotations that
provided valuable counter-examples to their machine learning
algorithm~\cite{Chin2010}.

Smets\etal experiment with Naive Bayes applied to a Bag-of-Words
model, but also pursue a compression-based
classifier~\cite{Smets2008}.
Itakura and Clarke follow up with a different compression-based
classifier, and focus separately on insertion spam and
text replacement spam\~cite{Itakura2009}.

User reputation systems~\cite{WikiTrust07,WikiMTWtrust07,www07}
have been proposed as an underlying technology for vandalism
prevention or detection.
Druck\etal~\cite{Druck2008} argues that a user reputation system isn't
sufficient for predicting article quality, and conducts an evaluation
demonstrating that a classifier including features about the content
of the edit performs much better at predicting reverts.
The features used are simpler than the high dimensionality of
the bag-of-words approach, but some of the features still require
examination and processing of the revision content.

Based on the idea that birds of a feather flock together,
West\etal~\cite{West2010} builds a system to track reputations
for users, articles, categories, and countries; these and other metadata
are used as features in a classifier for vandalism detection with
a performance that rivals the other lines of inquiry.


A question which hasn't been resolved in the literature is how best to
evaluate vandalism detection tools.
Belani~\cite{Belani2010} is rather exhaustive in using five different
evaluations for his Naive Bayes classifier: reliability diagrams,
RMSE, McNeman's Test, receiver operating characteristic,
and a precision-recall graph.
The PAN~2010 Workshop on Vandalism Detection presents some discussion
on this matter in trying to judge nine competition
entries~\cite{Potthast2010b}, as well as demonstrating that a
meta-classifier based on the nine entries is able to significantly
outperform all the invidual entries.


