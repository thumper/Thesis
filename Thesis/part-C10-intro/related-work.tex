\section{Related Work}

As the Wikipedia becomes a standard resource for the Internet
public, there is rising interest in quality measures.
A series of incidents show that the Wikipedia can be manipulated,
despite the ``many eyes'' reviewing the site:
a prank biography~\cite{Seigenthaler05,NewYorkTimes05a,NewYorkTimes05b},
congressional aides adjusting political
biographies~\cite{TheSun06,NewZelandHerald06,BBC06},
a user pretending to be a professor~\cite{BBC07},
and a slew of other self-interested parties making
inappropriate edits~\cite{Wired07,Wikiscanner07,NPR08}.
Articles have been written questioning the
general credibility of the Wikipedia~\cite{NewYorkTimes06,TheNewYorker06},
and a scientific study addressing the question
has been published~\cite{Giles05}.

Today, there are several lines of research pursuing
vandalism detection specific to the Wikipedia~\cite{Potthast2010b}.
These solutions all apply machine learning techniques
to annotated data sets, treating the task as a
``supervised learning'' problem.
At the time our research was started, there were no annotated
data sets available.


The idea of assigning trust to specific sections of text of Wikipedia
articles as a guide to readers has been previously proposed in the scientific
literature~\cite{WikiMTWtrust06,Cr06,McGuinness06}, as well as in white
papers~\cite{King07} and blogs~\cite{PaoloMassa07}; these papers also contain
the idea of using text background color to visualize trust values.


Other studies of Wikipedia quality have focused on trust as
article-level, rather than word-level, information.
These studies can be used to answer the question of whether an
article is of good quality, or reliable overall, but cannot be used to
locate within an article the portions of text which deserve the most
scrutiny, as our approach can.
In~\cite{WikiTrust06}, which inspired~\cite{McGuinness06}, the
revision history of a Wikipedia article is used to compute a trust
value for the entire article.
In~\cite{Emigh05b,Mingus07}, metrics derived via natural language processing
are used to classify articles according to their quality.
In~\cite{Lih04}, the number of edits and unique editors are used to
estimate article quality.
The use of revert times for quality estimation has been proposed
in~\cite{Viegas04}, where a visualization of the Wikipedia editing
process is presented; an approach based on edit frequency and dynamics
is discussed in~\cite{WilkinsonHuberman07}.
There is a fast-growing body of literature reporting on
statistical studies of the evolution of Wikipedia content,
including~\cite{Viegas04,Voss05,Ortega07}; we refer to~\cite{Ortega07} for an
insightful overview of this line of work.
The history flow of text contributed by Wikipedia authors has
been studied with flow visualization methods in~\cite{Viegas04}.

