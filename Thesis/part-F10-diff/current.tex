\subsection{Summary of Completed Research}

We now describe the solution
developed as part of our initial work into a
content-driven reputation~\cite{www07}.
To motivate our author attribution, we consider the problem of
trying to measure how many words contributed by author \editor{i}
survive to version \version{j}.
At a high-level, we achieve this by noting the text which
is new in \version{i}, and track the text even when it is deleted. 
When a later author makes an edit we match the new revision text
against the previous revision text, but also against recently
deleted text.
By tracking deleted text, we ensure that a piece of text
will retain the attribution to the original author of the text
even if it is restored by a different author.

More precisely,
we define $\txt(i,j)$
over a
sequence of versions $v_0, v_1, \ldots, v_n$,
for $0 < i \leq j \leq n$, as the amount of text
(measured in number of words) that is introduced
by \revision{i} in \version{i},
and that is still present (and due to the same author \editor{i})
in \version{j}.
In particular, $\txt(i,i)$ is the amount of new text added by \revision{i}.
Superficially, it might seem that to compute $\txt(i,j)$, we need to
consider only three versions of the document: \version{i-1},
\version{i}, and
\version{j}.
From $\version{i-1}$ and $\version{i}$
we can derive the text that is added at edit
$\revision{i}: \version{i-1} \voom \version{i}$,
and we can then check how much of it
survives until $\version{j}$.
This approach, however, is not appropriate for a versioned document
like a wiki page, where authors are allowed to inspect --- and restore
--- text from any previous version of a document.
For example, consider the case in which edit
$\revision{i-1}: \version{i-2} \voom \version{i-1}$ is the work
of a spammer, who erases entirely the text of $\version{i-2}$, and replaces
it with spurious material; such spam insertions are a common
occurrence in open wikis.
When author $\editor{i}$ views the page, she realizes that it has been damaged,
and she reverts it to the previous version, so that
$\version{i} = \version{i-2}$.
If we derived the text added by $\revision{i}$ by considering $v_{i-1}$ and
$v_i$ only, it would appear to us that $\editor{i}$ is the original author of
all the text in $v_i$, but this is clearly not the case: she simply
restored pre-existing text.
To compute the text added at a edit $\revision{i}: v_{i-1} \voom v_i$,
we keep track of both the text that is in $v_{i-1}$, and the text that used to
be present in previous versions and has subsequently been deleted.

Our algorithm proceeds as follows.
We call a {\em chunk\/} a list $c = [(w_1,q_1), \ldots, (w_k,q_k)]$,
where for $1 \leq j \leq k$, $w_j$ is a word,
and $q_j \in \set{1, \ldots, n}$ is a version number.
A chunk represents a list of contiguous words, each labeled with the
version where it originates.
The algorithm computes, for each version $v_i$, $1 \leq i \leq n$, its
{\em chunk list\/} $C_i = [c^i_0, c^i_1, \ldots, c^i_k]$.
The chunk $c^i_0$ is the {\em live\/} chunk, and it consists of the text
of $v_i$, with each word labeled with the version where the word was
introduced; thus, if $v_i = [w^i_1, \ldots, w^i_{m_i}]$,
we have $c^i_0 = [(w_1,q_1), \ldots, (w_{m_i},q_{m_i})]$,
for some $q_1, \ldots, q_{m_i}$.
The chunks $c^i_1, \ldots, c^i_k$ are {\em dead\/} chunks, and they
represent contiguous portions of text that used to be present in some
version of the document prior to $i$.
%% Thus, the chunk list of a document represents the current text of the
%% document, along with any text that used to be part of the document,
%% and that has been deleted.
Given the chunk list $C_i = [c^i_0, c^i_1, \ldots, c^i_k]$ for
document $v_i$, we can compute $\txt(j,i)$ via
%
\begin{equation} \label{eq-textlife}
  \txt(j,i) =
  \bigl|\bigl\{(u,j) \bigm| \exists u . (u,j) \in c^i_0\bigr\}\bigr|,
\end{equation}
%
when $1 \leq j \leq i \leq n$.

To compute $C_i$ for all versions $i$ of a document, we propose an
algorithm that proceeds as follows.
For the initial version, we let
$C_1 = [[(w^1_1, 1), (w^1_2, 1), \ldots, (w^1_{m_1}, 1)]]$.
For $1 \leq i < n$, the algorithm computes $C_{i+1}$ from
$C_i = [c^i_0, c^i_1, \ldots, c^i_k]$ and $v_{i+1}$.
To compute the live chunk $c^{i+1}_0$, we match contiguous portions of
text in $v_{i+1}$ with contiguous text in any of the chunks in $C_i$;
the matching words in $v_{i+1}$ are labeled with the version index that
labels them in $C_i$, and represent words that were introduced prior
to version $v_{i+1}$.
Any words of $v_{i+1}$ that cannot be thus matched are considered new
in $v_{i+1}$, and are labeled with version index $i+1$.
The dead chunks $c^{i+1}_1, \ldots, c^{i+1}_l$ of $C_{i+1}$ are then
obtained as the portions of the chunks in $C_i$ that were not matched
by any text in $v_{i+1}$.
%
We allow the same text in $C_i$ to be matched multiple timed in
$v_{i+1}$: if a contributor copies multiple times text present in
$v_i$ or in prior versions in order to obtain $v_{i+1}$, the
replicated text should not be counted as new in $v_{i+1}$.
Considering replicated text as new
would open the door to a {\em duplication attack,} whereby
an attacker duplicates text in a revision $\revision{i}$, and then removes the
original text in a revision $\revision{k}: v_{k-1} \voom v_k$ with $k > i$.
From version $v_k$ onwards, the text would be attributed to the
attacker rather than to the original author.

Matching $v_{i+1}$ with $C_i$ is a matter of finding matches between
text strings, and several algorithms have been presented in the
literature to accomplish this in an efficient manner
(see, e.g.,
\cite{HuntMcIlroy75,Hirchberg77,TichyEditDist,Myers86,BurnsLong97}).
We experimented extensively, and the algorithm that gave the best
combination of efficiency and accuracy was a variation of a standard
greedy algorithm.
In standard greedy algorithms, such as
\cite{Hirchberg77,Myers86,BurnsLong97}, longest matches are determined
first; in our algorithm, we define a notion of match {\em quality,}
and we determine first matches of highest quality.
To define match quality, we let $m_{i+1}$ be the length of $v_{i+1}$,
and we let $m'$ be the length of the chunk of $C_i$ where the match is
found (all length and indices are measured in number of words).
Let $l$ be the length of the match, and assume that the match begins
at word $k' \leq m'$ in the chunk, and at word $k_{i+1} \leq m_{i+1}$
in $v_{i+1}$.
We define match quality as follows:
%
\begin{itemize}
\item If the match occurs between $v_{i+1}$ and the live chunk, then
the quality is:
\mynote{fix}
%\[
%  \frac{l}{\min(m_{i+1},m')} - 0.3 \cdot \left|
%  \frac{k'}{m'} - \frac{k_{i+1}}{m_{i+1}} \right| \eqpun .
%\]
\item If the match occurs between $v_{i+1}$ and a dead chunk, then the
quality is~0 if $l < 4$, and is
$
  {l}/{\min(m_{i+1},m')} - 0.4
$ otherwise.
\end{itemize}
%
Thus, the higher the quality of a match, the longer the match is.
If the match is within the live chunk, a match has higher quality if the
text appears in the same relative position in $v_{i+1}$ and in $v_i$.
Matches with dead chunks have somewhat lower quality than matches with
the live chunk: this corresponds to the fact that, if some text can be
traced both to the previous version (the live chunk), and to some
text that was previously deleted, the most likely match is with the
text of the previous version.
Moreover, matches with dead chunks have to be at least of length four;
this avoids misclassifying common words in new text as re-introductions
of previously-deleted text.
The coefficients in the above definition of quality have been
determined experimentally, comparing human judgements of authorship to
the algorithmically computed ones for many pages of the Italian
Wikipedia.
% The text survival algorithm we developed is efficient: the main
% bottleneck, when computing text authorship, is not the running time of
% the algorithm, but rather, the time required to retrieve all versions
% of a page from the MySQL database in which Wikipedia pages are
% stored.\footnote{The measurement was done on a PC with AMD Athlon~64
% 3000+ CPU, two hard drives configured in RAID~1 (mirroring), and 1~GB
% of memory.}



