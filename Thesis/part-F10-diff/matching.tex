\section{Matching Text}
\label{sec:diff-greedy}

Our overall goal is to track \intro{text evolution},
by which we mean observing how text shifts
around and is added to or deleted from ---
understanding how it is changed from version to version
by examining the differences.
This is exactly the output from algorithms solving the
string-to-string correction problem.

The simplest string-to-string correction algorithms
note only insertions and deletions, but we are also interested
in noting whether text has been copied (or even just moved)
to another part of the article so that we can assign authorship
correctly.
We think several properties are desirable
when determining how text is reorganized:
\begin{itemize}
\item When text is duplicated within an article, we prefer to assign
    authorship to the author of the original copy.
    This prevents a vandal from duplicating text and then deleting the
    original copy.
    To achieve this goal, we must use an algorithm that allows
    block moves of text and allows the same source text to be
    matched multiple times in the target revision.
\item If the same text is found multiple times in both the previous
    and new revision, there are multiple ways to describe how the
    text has evolved.
    We would like to give preference to the most plausible explanation:
    that the text was not rearranged.
    To do so, we prefer to match chunks of text in the
    same relative order in their respective document
    versions.\footnote{This might seem irrelevant, given that duplicated
    text is given the same author.  This property becomes important
    when computing the edit distance from one version to another,
    where text in the same relative position does not increase the
    edit distance.  If the difference reflected block moves that
    crossed over each other, then there might be a positive edit
    distance contributed by the swapping of equivalent text,
    depending on the definition of edit distance used.}
\item We do not want to over-reward the first author to use common
    words (\eg ``the,'' ``of'').
\item When text is deleted in one version and then restored in a later
    version, we prefer the original author.
    This prevents edit wars or vandalism from disrupting the
    authorship of text.
\item When looking for matches, we prefer to match against chunks
    of text which have been \textit{live} most recently.
    That is, when matching against text from multiple revisions,
    we prefer to match against the most recent revision that is
    a good fit.
    This property accounts for editors who review the recent history
    of an article and restore text that was deleted, as often happens
    when vandalism occurs.
\item We prefer longer matches.
\end{itemize}


As described in Section~\ref{sec:diff-related}, there is extensive
literature on matching text between two strings.
Existing algorithms use somewhat different specifications than
we have outlined above, generally optimizing for the size of the edit
script or for the longest matches, and always only from a single
source string to the target string.
To achieve our goals, we develop a variation of
the greedy algorithms
by modifying the greedy step; instead of selecting a match
based on length, we use a notion of \intro{match quality}.

\index{basic differencing algorithm}
The procedure for our greedy-based differencing
algorithm\footnote{Appendix~\ref{app:basicdiffsrc-perl} lists a
\perl language implementation of this basic algorithm.}
is:
\begin{enumerate}
\item Compute all possible matches (of every length):
\begin{eqnarray*}
\mathrm{Matches}(\version{a}, \version{b}) = \{ (i,j,k) & |
            & \exists i \in \mathbb{N}, \exists j \in \mathbb{N},
              \exists k \in \mathbb{N} \ .\  (
    0 \le f < k, \\
    & & \words{\version{a}} = [ p_1, p_2, \ldots, p_{l_a} ], \\
    & & \words{\version{b}} = [ q_1, q_2, \ldots, q_{l_b} ] \ . \\
    & & p_{i+f} = q_{j+f} ) \}.
\end{eqnarray*}

\item For each match, compute a quality score and insert the match into
    a priority queue, so that the highest quality matches will
    be drawn first.
\item Draw a match out of the priority queue.
    If any part of the match has already been previously matched
    in the target string, then discard this match.
    Otherwise, record the match as a block \textbf{Move}
    from the source string to the target string.
    Repeat this step until the priority queue is empty.
\item Check for all unmatched blocks of text in the source
    string and record them as \textbf{Delete} operations
    in the final edit script.
\item Check for all unmatched blocks of text in the target
    string and record them as \textbf{Insert} operations
    in the final edit script.
\end{enumerate}

Building our list of matches (as given by $\mathrm{Matches}$)
roughly follows the work of other greedy differencing
algorithms~\cite{Reichenberger1991,Burns1997}:
build a hashtable of string prefixes that stores the list
of locations where each prefix can be found in the source string.
Then consider each position of the target string(s) by finding
the list of matching string prefixes from the hashtable.
For each match, find the extent of the match
(beyond the length of the string prefix)
and add every matching substring into the priority queue
as a separate match.

The key to matches meeting the list of criteria we have
defined at the beginning of this section is in the quality
function.\footnote{Except for multiple matching of source blocks, which
cannot be handled by the quality function.  The source in
Appendix~\ref{app:basicdiffsrc-perl} enables this feature by a flag.}
To prefer matches where the blocks are in similar positions,
we can select a quality function which compares the relative
positions of a match.
Consider a source string of length $l_1$ and a target string of
length $l_2$, with a match occuring between them of length $k$
at position $i_1$ in the source and $i_2$ in the target;
then we can define a quality to preserve the ordering of blocks as:
\begin{equation}
q_{block} = -\left| \frac{i_1 + k/2}{l_1} - \frac{i_2 + k/2}{l_2} \right|
\end{equation}
This formula computes the midpoint of each match and compares
the relative positions within the full strings; $q_{block}$ will be
zero when the matches are in the same relative position
and decreases as the blocks move away from each other.

More important than matching blocks not crossing each other,
we prefer to match longer pieces of text.
To achieve this, we use a tuple to represent the quality.
Given that a match has a length of $k$, we can represent preferring
longer matches over non-crossing blocks as $(k, q_{block})$.
The use of a tuple for the priority reflects that we prefer
matches of longer length, before considering the issue
of match quality.

So far, this discussion has applied equally to the task of
computing a difference between two strings and the problem of computing
text authorship.
In the latter, we actually
need to compute matches with multiple previous revisions
to account for text that was deleted and then later restored.
We describe these previous revisions as \intro{chunks} of text,
and number them so that chunk~0 is the most recent revision.
As stated before, we prefer matches with more recent revisions
than with older
revisions; if we let $c$ be the chunk number that a match comes from,
one possible quality tuple is $(-c, k, q_{block})$.\footnote{%
There are other possible quality tuples (\eg $(k, -c, q_{block})$,
if the absolute longest match is preferred), and
understanding the ramifications of choosing different quality
functions we leave to future research.}

Once we have an edit script that defines how text evolves from
previous revisions to the target (current) revision, we can then propagate
authorship from the previous revisions to the target.
A simple procedure for this would be:
\begin{enumerate}
\item Assign authorship of all words in the target to the current author.
\item Compute the edit script describing the text evolution from
    previous revisions.
\item For each block move in the edit script, extract the authorship
    from the source chunk and propagate it to the target.
\end{enumerate}

\subsection{Optimizations}
\label{sec:diff-optimizations}

The Wikipedia is a huge corpus of documents, and processing speed
is a crucial factor in doing timely analysis.
We can take a few steps to reduce the size of the computation
we have described so far.
Some optimizations that we have implemented for the differencing
step are:
%
\begin{description}
\item[\opt{min words}]
    We can reduce the number of potential matches examined
    by the algorithm by requiring a minimum number of words to match
    before a string will be added to the priority queue.
    WikiTrust accomplishes this by indexing word tuples in the
    hashtable of matches computed for the greedy algorithm;
    for edit distances, we index word pairs, and for text authorship
    we track word triples.

    This works out well, because we prefer not to reward authors that
    are the first to insert very common words or expressions into an
    article.
    The ideal solution would use language analysis to determine
    $n$-gram frequencies and compute
    a score (\eg tf-idf~\cite{Jones1972}) for words and phrases.


\item[\opt{max matches}]
    The hashtable constructed for the greedy algorithm is used
    to find the initial string prefixes for all possible matches.
    If there are too many matching string prefixes (for example,
    a common phrase such as ``to the'' might appear many times
    within a single article), then we believe that the string prefix
    is too common to really be considered to have been authored
    by a single individual.
    WikiTrust ignores string prefixes which have more than 50 matches.

\item[\opt{longest match}] Put only the longest possible match
    onto the priority queue, instead of every possible match (that is, don't
    place substrings of the longest match onto the priority queue).
    This saves a lot of CPU and memory by not having to store
    these substrings in the priority queue.
    Also, since the longest match is likely to be the one that is
    actually selected, there is a savings from not having to remove
    the substrings from the priority queue later.
    The complication of this optimization is that if a string on
    the priority queue has been partly matched by an earlier selection,
    then the residual non-matched parts of the string need to
    be placed into the priority queue for possible selection later.
    This makes the assumption that the quality function always
    prefers longer matches over shorter matches, so that we encounter
    matches from the priority queue in the correct order.
    Appendix~\ref{app:fasterdiffsrc-perl} demonstrates this
    modification.

\item[\opt{prev matches}]
    An optimization that we later implemented on top of
    \opt{longest match} is \opt{prev matches}:
    if a potential starting position for matched text is part of
    a longer match, then the previous position would have
    been in the set of matches returned by the index of
    matching positions.
    That is, if we are examining position $j$ in the target string
    and are considering match \textbf{Move}(i,j), we know that it
    was part of a longer match if $i-1$ is one of the matches returned
    for position $j-1$.
    In this case, we do not need to examine the match.

\item[\opt{header/trailer}] When users edit an article, the beginning
    and end of the article are not likely to change much.
    If the first few words of an article are the same from one revision to
    the next, it is reasonable to conclude that they are a match
    without having to test other possible sources of block moves
    from the article; this fits well with our desire that the resulting
    edit script try to match a human description of the edit.
    This pre-matching of the heading and trailing portions of the
    article can significantly reduce the number of potential matches
    that are computed in the initial step of the algorithm.
    Some care should be taken in the handling of authorship;
    if the header or trailer is duplicated elsewhere in the article,
    then the original authorship still needs to be retained.

\end{description}


To give an idea of the value of the optimizations, we implemented some
variations of the text differencing algorithms and measured the
execution time on differencing of the initial filtered revisions of the
article \underline{Santa Cruz Beach Boardwalk}.
We used the well-known Tichy algorithm with its own
optimizations~\cite{Tichy1984,Obst1987,Reichenberger1991}
as a baseline, and called this \diff1.
The most basic implementation of the WikiTrust algorithm we called
\diff9; it places \textit{every} potential match (including substrings
of longer matches) on a priority queue
before processing for matches, and implements the \opt{header/trailer}
optimization.
For \diff8, we instead incorporate the \opt{longest match} optimization which
places only the longest matching sections (that is, no substrings of
longer matches are considered) onto the priority queue.
Our fourth variation is \diff5, which includes the \opt{header/trailer},
\opt{prev matches}, and \opt{longest match} optimizations.
The numbering of these algorithms is to remain consistent with the
evaluation done in Chapter~\ref{ch:editquality}; see
Table~\ref{tab:diff-combinations} for a breakdown of algorithm
variations in that context.
See Appendix~\ref{app:diffsrc-ocaml} for the specific implementation details
of each algorithm; some representative run times are shown
in Table~\ref{tab:comparediff}.
The difference in run times between \diff8 and \diff9 reveal that the
\opt{header/trailer} optimization is the most significant in reducing
the run time; notice that \diff1 (the Tichy-based algorithm) performs
extremely well without the \opt{header/trailer} optimization, but also
doesn't really benefit from including it (revealed by timings in
Table~\ref{tab:difftiming}).\footnote{The Tichy algorithm matches the
initial ``header'' of the target revision as its first step, so the
\opt{header/trailer} optimization does not significantly reduce the work
performed by the algorithm.}

\begin{table}
\begin{center}
\begin{tabular}{| c | c | c || c | c | c | c |}
\hline
Source & Target & Num & Tichy & WikiTrust & WikiTrust & WikiTrust \\
Rev & Rev & Words & \diff1 & \diff5 & \diff8 & \diff9 \\
\hline
8741260 & 12175065 & 587 & 128\microsec & 514\microsec & 808\microsec & 526,492\microsec \\
12175065 & 14057051 & 589 & 125\microsec & 120\microsec & 791\microsec & 133\microsec \\
14057051 & 17039312 & 588 & 122\microsec & 127\microsec & 893\microsec & 131\microsec \\
17039312 & 20060015 & 588 & 117\microsec & 123\microsec & 1111\microsec & 132\microsec \\
20060015 & 20551181 & 588 & 129\microsec & 153\microsec & 1053\microsec & 136\microsec \\
\hline
\end{tabular}
\end{center}
\caption[Comparing the running times of diff algorithms]{%
    The execution times of four text differencing algorithms implemented
    in \ocaml, three of which are variations of the WikiTrust algoritm,
    on the initial filtered revisions of the article \underline{Santa Cruz Beach
    Boardwalk}. The particularly slow difference for \diff9 is typical for
    cases where both the beginning and end of an article are edited,
    defeating the \opt{header/trailer} optimization; in this case, an image
    was added near the beginning and a category was added to the end.
    }
\label{tab:comparediff}
\end{table}

There are additional optimizations we can make for computing
text authorship, as well.
A very simple step is to reduce the size of the list of
previous revisions given by \prevrevs{\version{i}}.
As defined in Section~\ref{sec:diff-tracking}, \prevrevs{\version{i}}
is the ordered list of all previous revisions,
$\left[ \version{1}, \version{2}, \ldots, \version{i-1} \right]$.
We use the full list of previous revisions to ensure that restored text
is assigned to the right author.
In practice, the history of an article can extend to several thousand
revisions, but an editor is likely only to look back a small number
of revisions when searching for text to restore.
We arbitrarily limit the list of previous revisions to ten:
\begin{equation*}
    \prevrevs{\version{i}} = \left[ \version{j} \colon
	max(1,i-10) \le j < i \right],
\end{equation*}
but observe that better selections can be made, as described
in~\cite{Chatterjee2008}.

\index{faster text tracking}
We can further speed up tracking authorship by reducing the number
of potential matches, similar to the optimization for the differencing
algorithm.
If we assume that the quality function always prefers matches from
more recent revisions to matches from older revisions
(as is the case with $(-c, k, q_{block})$ as a quality function),
then we can construct the edit script piecewise by differencing
only a single revision at a time.
When looking at older chunks, the fact that the target has already
been partially matched will reject many potential matches.
Appendix~\ref{app:basictextsrc-perl} shows an implementation of the
basic text tracking algorithm, and Appendix~\ref{app:fastertextsrc-perl}
shows our optimization; Table~\ref{tab:comparetext} shows representative
running times.

\begin{table}
\begin{center}
\begin{tabular}{| c | c || c || c | c |}
\hline
Target & Previous & WikiTrust & WikiTrust \\
Rev & Revs & BasicTextTracking & FasterTextTracking \\
\hline
12175065 & 1 & 12.50s & 13.93s \\
14057051 & 2 & 36.52s & 2.49s \\
17039312 & 3 & 68.79s & 3.61s \\
20060015 & 4 & 144.42s & 3.14s \\
20551181 & 5 & 933.44s & 21.50s \\
\ldots & \ldots & \ldots & \ldots \\
24358877 & 10 & 5064.09s & 26.94s \\
25306944 & 10 & 6717.15s & 33.74s \\
34009105 & 10 & 6480.42s & 3.14s \\
\hline
\end{tabular}
\end{center}
\caption[Comparing execution times of our basic and
    faster text tracking algorithms.]{
    \index{basic text tracking}
    \index{faster text tracking}
    Comparing the execution times of our
    basic and fast text tracking
    algorithms, both written in \perl, on selected versions of the article
    \underline{Santa Cruz Beach Boardwalk}.
    Note that \perl is an interpreted language, slower than the
    \ocaml implementation evaluated in Table~\ref{tab:comparediff}
    and Chapter~\ref{ch:editquality}.
    We present this data to give a flavor for the importance
    of the optimizations.
    The faster text tracking algorithm reduces amount of work
    done by only matching against one revision at a time, eliminating
    potential matches found in other revisions.
}
\label{tab:comparetext}
\end{table}


An additional optimization we can make is again with respect to the
handling of the previous revisions.
In defining the matching algorithm, we give preference to more recent
versions before falling back to matches against older revisions.
Let us consider the case where a piece of text does not match in the
most recent revision \version{k},
but does match in some earlier revision \version{i}, so that we have $i < k$.
Since the text does not match in any \version{j} for $i < j \le k$,
the text must have been deleted in \version{i+1} and
never restored.
The size of deleted text in any revision is typically much smaller
than the size of the live text, so our final optimization is
to check for matches against the deleted text rather than the full
live text of previous revisions.
Note that this can change the size of matches that are found
if a piece of text only partially matches deleted text,
so that the quality function might not select matches in a preferred way.

\begin{comment}
\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Priority queue of longest possible matches.}
\SetKwRepeat{Repeat}{repeat}{until}
$l_1 \leftarrow$ length($w_1$) \;
$l_2 \leftarrow$ length($w_2$) \;
$idx \leftarrow $ IndexWordsByPositions($w_1$) \;
\For{$i_2 \leftarrow 0$ \KwTo length($w_2$)}{
    \If{$matched[i_1,i_2] \neq 1$}{
	$matches \leftarrow idx[ w_2[i_2] ]$ \;
	\For{$m \leftarrow 0$ \KwTo  length($matches$)}{
	    $i_1 \leftarrow matches[m]$ \;
	    $k \leftarrow 0$ \;
	    \Repeat{$i_1 + k < l_1 \wedge i_2 + k < l_2 \wedge
		w_1[i_1+k] = w_2[i_2+k]$}
	    {$k \leftarrow k+1$ \; }
	    \If{$k > 2$}{
		\For{$i \leftarrow 0$ \KwTo $k-1$}{
		    $matched[i_1+i,i_2+i] = 1$ \;
		}
		$q \leftarrow$ MatchQuality($k, i_1, l_1, i_2, l_2$) \;
		$prioQ.add( q, (i_1,i_2,k) )$ \;
	    }
	}
    }
}
\caption{Fill priority queue with large matches.}
\end{algorithm}

\begin{algorithm}[H]
\KwIn{A priority queue, $prioQ$, of longest possible matches.}
\KwOut{Edit script of block matches between $w_1$ and $w_2$.}
\While{$m \leftarrow prioQ.pop()$}{
    $(i_1, i_2, k) \leftarrow m$ \;
    \tcp{Scan for unmatched subregions}
    $start \leftarrow 0$ \;
    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
	matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
    $end \leftarrow start + 1$ \;
    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
	matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
    \BlankLine
    \eIf{$end - start = k$}{
	\tcp{Found a match for our edit script}
	$editScript.add( \mathbf{MOV}(i_1, i_2, k) )$ \;
	\For{$i \leftarrow start$ \KwTo $end - 1$}{
	    $matched_1[i_1+i] = 1$ \;
	    $matched_2[i_2+i] = 1$ \;
	}
    }{
	\tcp{Found unmatched subregion, so calculate residual match}
	\While{$start < k$}{
	    $k' \leftarrow end - start$ \;
	    $q \leftarrow$ MatchQuality($k', i_1, l_1, i_2, l_2$) \;
	    $prioQ.add( q, (i_1,i_2,k') )$ \;
	    $i_1 \leftarrow i_1 + end$ \;
	    $i_2 \leftarrow i_2 + end$ \;
	    $k \leftarrow k - end$ \;
	    \tcp{Scan for unmatched subregions}
	    $start \leftarrow 0$ \;
	    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
		    matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
	    $end \leftarrow start + 1$ \;
	    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
		matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
	}
    }
}
\caption{Greedy matching between two strings.}
\end{algorithm}


\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Edit script to transform $w_1$ into $w_2$.}
compute priority queue of longest matches \;
run through priority queue selecting best matches \;

\caption{Fast differencing algorithm.}
\end{algorithm}

\end{comment}


\subsection{Thoughts On Evaluation}

Historically, text differencing algorithms are judged
on the complexity of the algorithm, or how the memory
usage scales with the size of the inputs, or in the
size of the resulting edit script.
These are important concerns, especially in the context of
trying to process the over 1.5TB of data that make up the
revision history of the English Wikipedia.
Our goal is more than efficiency, however; we are trying
to measure the work done by authors of a collaborative work.
People are far from efficient, so our solution attempts to
simply model a human conception of how text is rearranged
and reinstated from previous revisions.

The problem we are faced with, then, is how to evaluate
this measure; do our calculations correlate well with
human intuition?
Is it even possible to survey a large number of people
to generate edit scripts describing the transformation from
one revision to the next, or can edit scripts only be generated
by video taping editors caught in the act?
Although we do not evaluate the performance of our difference
algorithm directly for this metric, we speculate that it might be
possible to indirectly evaluate the performance by examining and
evaluating a different problem which uses a difference algorithm as a basis.
We investigate this idea in the evaluation of our edit quality
measures, presented in the next chapter.

