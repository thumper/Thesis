\section{Matching Text}

\mynote{Need more graceful start.}
By ``text evolution,'' we mean noting how text shifts
around and is added to or deleted from --- that is,
understanding how it is changed from version to version
by examining the differences.
This is exactly the output from algorithms solving the
string-to-string correction problem.

The simplest string-to-string correction algorithms
note only insertions and deletions, but we are also interested
in noting whether text has been copied (or even just moved)
to another part of the article so that we can assign authorship
correctly.
In fact, we think several properties are desirable
when determining how text is reorganized:
\begin{itemize}
\item When text is duplicated within an article, we prefer to assign
    authorship to the author of the original copy.
    This prevents a vandal from duplicating text and then deleting the
    original copy.
    To achieve this goal, we must use an algorithm that allows
    block moves of text and allows the same source text to be
    matched multiple times in the target revision.
\item If the same text is found multiple times in both the previous
    and new revision, there are multiple ways to describe how the
    text has evolved.
    We would like to give preference to the most plausible explanation:
    that the text was not rearranged.
    To do so, we prefer to match chunks of text in the
    same relative order in their respective document
    versions.\footnote{This might seem irrelevant, given that duplicated
    text is given the same author.  This property becomes important
    when computing the edit distance from one version to another,
    where text in the same relative position does not increase the
    edit distance.  If the difference reflected block moves that
    crossed over each other, then there would be a positive edit
    distance contributed by the swapping of equivalent text.}
\item We do not want to over-reward the first author to use common
    words (\eg ``the,'' ``of'').
\item When text is deleted in one version and then restored in a later
    version, we prefer the original author.
    This prevents edit wars or vandalism from disrupting the
    authorship of text.
\item When looking for matches, we prefer to match against chunks
    of text which have been \textit{live} most recently.
    That is, when matching against text from multiple revisions,
    we prefer to match against the most recent revision that is
    a good fit.
    This property accounts for editors who review the recent history
    of an article and restore text that was deleted, as often happens
    when vandalism occurs.
\item We prefer longer matches.
\end{itemize}


As described in Section~\ref{sec:diff-related}, there is extensive
literature on matching text between two strings.
Existing algorithms use somewhat different specifications than
we have outlined above, generally optimizing for the size of the edit
script or for the longest matches, and always only from a single
source string to the target string.
To achieve our goals, we develop a variation of
standard greedy algorithms
by modifying the greedy step; instead of selecting a match
based on length, we use a notion of \intro{match quality}.

The basic procedure for our greedy-based differencing
algorithm\footnote{Appendix~\ref{app:basicdiffsrc} lists a
\textbf{perl} language implementation of this basic algorithm.}
is:
\begin{enumerate}
\item Compute all possible matches (of every length):
\begin{eqnarray*}
\mathrm{Matches}(\version{a}, \version{b}) = \{ (i,j,k) & |
            & \exists k \in \mathbb{N}.  (
    \forall f \in \mathbb{Z}, 0 \le f < k . \\
    & & ( w^a_{i+f} \in \words{\version{a}},
    w^b_{j+f} \in \words{\version{b}}, \\
    & & w^a_{i+f} = w^b_{j+f} ) ) \}.
\end{eqnarray*}

\item For each match, compute a quality score and insert the match into
    a priority queue, so that the highest quality matches will
    be drawn first.
\item Draw a match out of the priority queue.
    If any part of the match has already been previously matched
    in the target string, then discard this match.
    Otherwise, record the match as a block \textbf{Move}
    from the source string to the target string.
    Repeat this step until the priority queue is empty.
\item Check for all unmatched blocks of text in the source
    string and record them as \textbf{Delete} operations
    in the final edit script.
\item Check for all unmatched blocks of text in the target
    string and record them as \textbf{Insert} operations
    in the final edit script.
\end{enumerate}

The key to matches meeting the list of criteria we have
defined above is in the quality function.\footnote{Except for
multiple matching of source blocks, which cannot be handled
by the quality
function.  The source in Appendix~\ref{app:basicdiffsrc}
enables this feature by a flag.}
To prefer matches where the blocks are in similar positions,
we can select a quality function, which compares the relative
positions of a match.
Suppose that a match of length $k$ occurs at $i_1$ in the source
string having length $l_1$, matching the same sequence of length $k$ at
position $i_2$ in the target string having length $l_2$,
then we can define a quality to preserve the
ordering of blocks as:
\begin{equation}
q_{block} = -\left| \frac{i_1 + k/2}{l_1} - \frac{i_2 + k/2}{l_2} \right|
\end{equation}
This formula computes the midpoint of each match and compares
the relative positions within the full strings; $q_{block}$ will be
zero when the matches are in the same relative position
and increases as the blocks move away from each other.

More important than matching blocks not crossing each other,
we prefer longer matches of text.
To achieve this, we use a tuple to represent the quality.
Given that a match has a length of $k$, we can represent preferring
longer matches over non-crossing blocks as $(k, q_{block})$.
The use of a tuple for the priority reflects that we prefer
matches of longer length, before considering the issue
of match quality.  \mynote{Is this clear?}

So far, this discussion has applied equally to the task of
computing a difference between two strings.
In the problem of computing text authorship, we actually
need to compute matches with multiple previous revisions
to account for text that was deleted and then later restored.
We describe these previous revisions as \intro{chunks} of text,
and number them so that chunk~0 is the most recent revision.
As stated before, we prefer matches with more recent revisions
than with older
revisions; if we let $c$ be the chunk that a match comes from,
one possible quality tuple is $(-c, k, q_{block})$.
There are other possible quality tuples (\eg $(k, -c, q_{block})$,
if the absolute longest match is preferred), and
understanding the ramifications of choosing different quality
functions is an area of potential further research.

We also prefer not to reward authors that first use very common words
or expressions.
The ideal solution would use language analysis to determine
$n$-gram frequencies and compute
a score (\eg tf-idf \mynote{Need citation}) for words and phrases.
To reduce our computation requirements, we chose the simpler idea
of checking the length of the match: if a match is less than three
words, then we reject the match.

Once we have an edit script that defines how text evolves from
previous revisions to the target (current) revision, we can then propagate
authorship from the previous revisions to the target.
A simple procedure for this would be:
\begin{enumerate}
\item Assign authorship of all words in the target to the current author.
\item Compute the edit script describing the text evolution from
    previous revisions.
\item For each block move in the edit script, extract the authorship
    from the source chunk and propagate it to the target.
\end{enumerate}

\subsection{Optimizations}

The Wikipedia is a huge corpus of documents, and processing speed
is a crucial factor in doing timely analysis.
We can take a few steps to reduce the size of the computation
we have described so far.

The first optimization is in computing the possible
matches in the differencing algorithm.
Instead of computing every possible match, we can initially compute
only the longest matches.
As matches are drawn from the priority queue, if we find
that a match has been previously partially matched, we can
compute the residual match at that time and insert the residual
into the priority queue.
This makes the assumption that the quality function always
prefers longer matches over shorter matches.
Appendix~\ref{app:fasterdiffsrc} demonstrates this
modification, with some representative runtimes shown
in Table~\ref{tab:comparediff}.


\begin{table}
\begin{center}
\begin{tabular}{| c | c || c || c | c |}
\hline
Source & Target & Num & WikiTrust & WikiTrust \\
Rev & Rev & Words & BasicDiff & FasterDiff \\
\hline
8741260 & 12175065 & 641 & 12912.27s & 7.47s \\
12175065 & 14057051 & 641 & 47.29s & 1.15s \\
14057051 & 17039312 & 641 & 107.47s & 1.06s \\
17039312 & 20060015 & 641 & 114.30s & 1.04s \\
20060015 & 20551181 & 660 & 77580.38s & 15.51s \\
\hline
\end{tabular}
\end{center}
\caption{Comparing the execution times of the \textbf{basic}
    and \textbf{fast} versions of our difference algorithm
    on the initial revisions of the article ``Santa Cruz Beach Boardwalk.''}
\label{tab:comparediff}
\end{table}



In computing the text authorship,
a very simple step is to reduce the size of the list of
previous revisions given by \prevrevs{\version{i}}.
As defined in Section~\ref{sec:diff-tracking}, \prevrevs{\version{i}} 
is the ordered list of all previous revisions,
$\left[ \version{1}, \version{2}, \ldots, \version{i-1} \right]$.
We use the full list of previous revisions to ensure that restored text
is assigned to the right author.
In practice, the history of an article can extend to several thousand
revisions, but an editor is likely only to look back a small number
of revisions when searching for text to restore.
We arbitrarily limit the list of previous revisions to ten:
\begin{equation*}
    \prevrevs{\version{i}} = \left[ \version{j} \colon
	max(1,i-10) \le j < i \right],
\end{equation*}
but observe that better selections can be made, as described
in~\cite{Chatterjee2008}.

We can further speed up tracking authorship by reducing the number
of potential matches, similar to the optimization for the differencing
algorithm.
If we assume that the quality function always prefers matches from
more recent revisions to matches from older revisions
(as is the case with $(-c, k, q_{block})$ as a quality function),
then we can construct the edit script piecewise by differencing
only a single revision at a time.
When looking at older chunks, the fact that the target has already
been partially matched will reject many potential matches.
Appendix~\ref{app:basictextsrc} shows an implementation of the
basic text tracking algorithm, and Appendix~\ref{app:fastertextsrc}
shows the optimization; Table~\ref{tab:comparetext} shows representative
running times.

\begin{table}
\begin{center}
\begin{tabular}{| c | c || c || c | c |}
\hline
Target & Previous & WikiTrust & WikiTrust \\
Rev & Revs & BasicTextTracking & FasterTextTracking \\
\hline
12175065 & 1 & 12.50s & 13.93s \\
14057051 & 2 & 36.52s & 2.49s \\
17039312 & 3 & 68.79s & 3.61s \\
20060015 & 4 & 144.42s & 3.14s \\
20551181 & 5 & 933.44s & 21.50s \\
\ldots & \ldots & \ldots & \ldots \\
24358877 & 10 & 5064.09s & 26.94s \\
25306944 & 10 & 6717.15s & 33.74s \\
34009105 & 10 & 6480.42s & 3.14s \\
\hline
\end{tabular}
\end{center}
\caption[Comparing execution times of our two
    text tracking algorithms.]{Comparing the execution times of our
    \textbf{basic} and \textbf{fast} text tracking
    algorithms on selected versions of the article
    ``Santa Cruz Beach Boardwalk.''}
\label{tab:comparetext}
\end{table}


Another optimization is again with respect to the handling of the
previous revisions.
In defining the matching algorithm, we give preference to more recent
versions before falling back to matches against older revisions.
Let us consider the case where a piece of text does not match in the
most recent revision \version{k},
but does match in some earlier revision \version{i}, so that we have $i < k$.
Since the text does not match in any \version{j} for $j > i$,
the text must have been deleted in \version{i+1} and
never restored.
The size of deleted text in any revision is typically much smaller
than the size of the live text, so our final optimization is
to check for matches against the deleted text rather than the full
live text of previous revisions.
Note that this can change the size of matches that are found,
so that the quality function might not select matches in a preferred way.

\begin{comment}
\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Priority queue of longest possible matches.}
\SetKwRepeat{Repeat}{repeat}{until}
$l_1 \leftarrow$ length($w_1$) \;
$l_2 \leftarrow$ length($w_2$) \;
$idx \leftarrow $ IndexWordsByPositions($w_1$) \;
\For{$i_2 \leftarrow 0$ \KwTo length($w_2$)}{
    \If{$matched[i_1,i_2] \neq 1$}{
	$matches \leftarrow idx[ w_2[i_2] ]$ \;
	\For{$m \leftarrow 0$ \KwTo  length($matches$)}{
	    $i_1 \leftarrow matches[m]$ \;
	    $k \leftarrow 0$ \;
	    \Repeat{$i_1 + k < l_1 \wedge i_2 + k < l_2 \wedge
		w_1[i_1+k] = w_2[i_2+k]$}
	    {$k \leftarrow k+1$ \; }
	    \If{$k > 2$}{
		\For{$i \leftarrow 0$ \KwTo $k-1$}{
		    $matched[i_1+i,i_2+i] = 1$ \;
		}
		$q \leftarrow$ MatchQuality($k, i_1, l_1, i_2, l_2$) \;
		$prioQ.add( q, (i_1,i_2,k) )$ \;
	    }
	}
    }
}
\caption{Fill priority queue with large matches.}
\end{algorithm}

\begin{algorithm}[H]
\KwIn{A priority queue, $prioQ$, of longest possible matches.}
\KwOut{Edit script of block matches between $w_1$ and $w_2$.}
\While{$m \leftarrow prioQ.pop()$}{
    $(i_1, i_2, k) \leftarrow m$ \;
    \tcp{Scan for unmatched subregions}
    $start \leftarrow 0$ \;
    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
	matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
    $end \leftarrow start + 1$ \;
    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
	matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
    \BlankLine
    \eIf{$end - start = k$}{
	\tcp{Found a match for our edit script}
	$editScript.add( \mathbf{MOV}(i_1, i_2, k) )$ \;
	\For{$i \leftarrow start$ \KwTo $end - 1$}{
	    $matched_1[i_1+i] = 1$ \;
	    $matched_2[i_2+i] = 1$ \;
	}
    }{
	\tcp{Found unmatched subregion, so calculate residual match}
	\While{$start < k$}{
	    $k' \leftarrow end - start$ \;
	    $q \leftarrow$ MatchQuality($k', i_1, l_1, i_2, l_2$) \;
	    $prioQ.add( q, (i_1,i_2,k') )$ \;
	    $i_1 \leftarrow i_1 + end$ \;
	    $i_2 \leftarrow i_2 + end$ \;
	    $k \leftarrow k - end$ \;
	    \tcp{Scan for unmatched subregions}
	    $start \leftarrow 0$ \;
	    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
		    matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
	    $end \leftarrow start + 1$ \;
	    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
		matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
	}
    }
}
\caption{Greedy matching between two strings.}
\end{algorithm}


\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Edit script to transform $w_1$ into $w_2$.}
compute priority queue of longest matches \;
run through priority queue selecting best matches \;

\caption{Fast differencing algorithm.}
\end{algorithm}

\end{comment}

