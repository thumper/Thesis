\section{Matching Text}
\label{sec:diff-greedy}

In defining the recursion for text authorship, we were intentionally
vague about how to compute the best match for a piece of text
from the history of revisions for an article.
Our overall goal is to track \intro{text evolution},
by which we mean observing how text shifts
around and is added to or deleted from --- that is,
understanding how it is changed from version to version
by examining the differences.
This is exactly the output from algorithms solving the
string-to-string correction problem.

The simplest string-to-string correction algorithms
note only insertions and deletions, but we are also interested
in noting whether text has been copied (or even just moved)
to another part of the article so that we can assign authorship
correctly.
In fact, we think several properties are desirable
when determining how text is reorganized:
\begin{itemize}
\item When text is duplicated within an article, we prefer to assign
    authorship to the author of the original copy.
    This prevents a vandal from duplicating text and then deleting the
    original copy.
    To achieve this goal, we must use an algorithm that allows
    block moves of text and allows the same source text to be
    matched multiple times in the target revision.
\item If the same text is found multiple times in both the previous
    and new revision, there are multiple ways to describe how the
    text has evolved.
    We would like to give preference to the most plausible explanation:
    that the text was not rearranged.
    To do so, we prefer to match chunks of text in the
    same relative order in their respective document
    versions.\footnote{This might seem irrelevant, given that duplicated
    text is given the same author.  This property becomes important
    when computing the edit distance from one version to another,
    where text in the same relative position does not increase the
    edit distance.  If the difference reflected block moves that
    crossed over each other, then there would be a positive edit
    distance contributed by the swapping of equivalent text.}
\item We do not want to over-reward the first author to use common
    words (\eg ``the,'' ``of'').
\item When text is deleted in one version and then restored in a later
    version, we prefer the original author.
    This prevents edit wars or vandalism from disrupting the
    authorship of text.
\item When looking for matches, we prefer to match against chunks
    of text which have been \textit{live} most recently.
    That is, when matching against text from multiple revisions,
    we prefer to match against the most recent revision that is
    a good fit.
    This property accounts for editors who review the recent history
    of an article and restore text that was deleted, as often happens
    when vandalism occurs.
\item We prefer longer matches.
\end{itemize}


As described in Section~\ref{sec:diff-related}, there is extensive
literature on matching text between two strings.
Existing algorithms use somewhat different specifications than
we have outlined above, generally optimizing for the size of the edit
script or for the longest matches, and always only from a single
source string to the target string.
To achieve our goals, we develop a variation of
the greedy algorithms
by modifying the greedy step; instead of selecting a match
based on length, we use a notion of \intro{match quality}.

The basic procedure for our greedy-based differencing
algorithm\footnote{Appendix~\ref{app:basicdiffsrc-perl} lists a
\textbf{perl} language implementation of this basic algorithm.}
is:
\begin{enumerate}
\item Compute all possible matches (of every length):
\begin{eqnarray*}
\mathrm{Matches}(\version{a}, \version{b}) = \{ (i,j,k) & |
            & \exists k \in \mathbb{N} \ .\  (
    \forall f \in \mathbb{Z}, 0 \le f < k \ . \\
    & & w^a_{i+f} \in \words{\version{a}} \\
    & & \wedge w^b_{j+f} \in \words{\version{b}} \\
    & & \wedge w^a_{i+f} = w^b_{j+f} ) \}.
\end{eqnarray*}

\item For each match, compute a quality score and insert the match into
    a priority queue, so that the highest quality matches will
    be drawn first.
\item Draw a match out of the priority queue.
    If any part of the match has already been previously matched
    in the target string, then discard this match.
    Otherwise, record the match as a block \textbf{Move}
    from the source string to the target string.
    Repeat this step until the priority queue is empty.
\item Check for all unmatched blocks of text in the source
    string and record them as \textbf{Delete} operations
    in the final edit script.
\item Check for all unmatched blocks of text in the target
    string and record them as \textbf{Insert} operations
    in the final edit script.
\end{enumerate}

Building our list of matches (as given by $\mathrm{Matches}$)
roughly follows the work of other greedy differencing
algorithms~\cite{Reichenberger1991,Burns1997}:
build a hashtable of string prefixes that stores the list
of locations where each prefix can be found in the target string.
Then consider each position of the source string(s) by finding
the list of matching string prefixes from the hashtable.
For each match, find the extent of the match
(beyond the length of the string prefix)
and add every matching substring into the priority queue
as a separate match.

The key to matches meeting the list of criteria we have
defined above is in the quality function.\footnote{Except for
multiple matching of source blocks, which cannot be handled
by the quality
function.  The source in Appendix~\ref{app:basicdiffsrc-perl}
enables this feature by a flag.}
To prefer matches where the blocks are in similar positions,
we can select a quality function, which compares the relative
positions of a match.
Suppose that a match of length $k$ occurs at $i_1$ in the source
string having length $l_1$, matching the same sequence of length $k$ at
position $i_2$ in the target string having length $l_2$,
then we can define a quality to preserve the
ordering of blocks as:
\begin{equation}
q_{block} = -\left| \frac{i_1 + k/2}{l_1} - \frac{i_2 + k/2}{l_2} \right|
\end{equation}
This formula computes the midpoint of each match and compares
the relative positions within the full strings; $q_{block}$ will be
zero when the matches are in the same relative position
and increases as the blocks move away from each other.

More important than matching blocks not crossing each other,
we prefer longer matches of text.
To achieve this, we use a tuple to represent the quality.
Given that a match has a length of $k$, we can represent preferring
longer matches over non-crossing blocks as $(k, q_{block})$.
The use of a tuple for the priority reflects that we prefer
matches of longer length, before considering the issue
of match quality.  \mynote{Is this clear?}

So far, this discussion has applied equally to the task of
computing a difference between two strings.
In the problem of computing text authorship, we actually
need to compute matches with multiple previous revisions
to account for text that was deleted and then later restored.
We describe these previous revisions as \intro{chunks} of text,
and number them so that chunk~0 is the most recent revision.
As stated before, we prefer matches with more recent revisions
than with older
revisions; if we let $c$ be the chunk that a match comes from,
one possible quality tuple is $(-c, k, q_{block})$.
There are other possible quality tuples (\eg $(k, -c, q_{block})$,
if the absolute longest match is preferred), and
understanding the ramifications of choosing different quality
functions is an area of potential further research.

We also prefer not to reward authors that first use very common words
or expressions.
The ideal solution would use language analysis to determine
$n$-gram frequencies and compute
a score (\eg tf-idf~\cite{Jones1972}) for words and phrases.
To reduce our computation requirements, we chose the simpler idea
of checking the length of the match: if a match is less than three
words, then we reject the match.
(Note that the hashtable of string prefixes can then be indexed by
word pairs without missing any potential matches.)

Once we have an edit script that defines how text evolves from
previous revisions to the target (current) revision, we can then propagate
authorship from the previous revisions to the target.
A simple procedure for this would be:
\begin{enumerate}
\item Assign authorship of all words in the target to the current author.
\item Compute the edit script describing the text evolution from
    previous revisions.
\item For each block move in the edit script, extract the authorship
    from the source chunk and propagate it to the target.
\end{enumerate}

\subsection{Optimizations}
\label{sec:diff-optimizations}

The Wikipedia is a huge corpus of documents, and processing speed
is a crucial factor in doing timely analysis.
We can take a few steps to reduce the size of the computation
we have described so far.
Some optimizations that we have implemented for the differencing
step are:
%
\begin{description}
\item[\textbf{min words}] 
    We can reduce the number of potential matches examined
    by the algorithm by requiring a minimum number of words to match
    before a string will be added to the priority queue.
    WikiTrust accomplishes this by index word tuples in the
    hashtable of matches computed for the greedy algorithm;
    for edit distances, we index word pairs, and for text authorship
    we track word triples.

\item[\textbf{max matches}]
    The hashtable constructed for the greedy algorithm is used
    to find the initial string prefixes for all possible matches.
    If there are too many matching string prefixes (for example,
    a common phrase such as ``to the'' might appear many times
    within a single article), then we believe that the string prefix
    is too common to really be considered to have been authored
    by a single individual.
    WikiTrust ignore string prefixes which have more than 50 matches.

\item[\textbf{longest match}] Put only the longest possible match
    onto the priority queue, instead of every possible match (that is, don't
    place substrings of the longest match onto the priority queue).
    This saves a lot of CPU and memory by not having to store
    these substrings in the priority queue.
    Also, since the longest match is likely to be the one that is
    actually selected, there is a savings from not having to remove
    the substrings from the priority queue later.
    The complication of this optimization is that if a string on
    the priority queue has been partly matched by an earlier selection,
    then the residual non-matched parts of the string need to
    be placed into the priority queue for possible selection later.
    This makes the assumption that the quality function always
    prefers longer matches over shorter matches.
    Appendix~\ref{app:fasterdiffsrc-perl} demonstrates this
    modification, with some representative runtimes shown
    in Table~\ref{tab:comparediff}.

\item[\textbf{header/trailer}] When users edit an article, the beginning
    and end of the article are not likely to change much.
    If the first few words of an article are the same from one revision to
    the next, it is reasonable to conclude that they are a match
    without having to test other possible sources of block moves
    from the article; this fits well with our desire that the resulting
    edit script try to match a human description of the edit.
    This pre-matching of the heading and trailing portions of the
    article can significantly reduce the number of potential matches
    that are computed in the initial step of the algorithm.
    Some care should be taken in the handling of authorship;
    if the header or trailer is duplicated elsewhere in the article,
    then the original authorship still needs to be retained.

\item[\textbf{prev matches}]
    For computing the edit distance between two revisions,
    our mechanism of comparing against multiple older revisions
    is unnecessary.
    In that case, we can avoid placing an substring match
    into the priority queue by checking to see if the previous
    word was the beginning of a match; in that case, we know this
    word must also be part of that same match.
    \mynote{This seems like it would skip the second word, but
    not later words of the same match!}

\end{description}

\begin{table}
\begin{center}
\begin{tabular}{| c | c || c || c | c |}
\hline
Source & Target & Num & WikiTrust & WikiTrust \\
Rev & Rev & Words & BasicDiff & FasterDiff \\
\hline
8741260 & 12175065 & 641 & 12912.27s & 7.47s \\
12175065 & 14057051 & 641 & 47.29s & 1.15s \\
14057051 & 17039312 & 641 & 107.47s & 1.06s \\
17039312 & 20060015 & 641 & 114.30s & 1.04s \\
20060015 & 20551181 & 660 & 77580.38s & 15.51s \\
\hline
\end{tabular}
\end{center}
\caption{Comparing the execution times of two simple perl
    implementations of our difference  algorithm
    on the initial revisions of the article ``Santa Cruz Beach Boardwalk.''
    The \texttt{FasterDiff} version implements our
    \textbf{longest match} optimization.
    Both versions have \textbf{min words} set to 2.
    }
\label{tab:comparediff}
\end{table}

There are additional optimizations we can make for computing
text authorship, as well.
A very simple step is to reduce the size of the list of
previous revisions given by \prevrevs{\version{i}}.
As defined in Section~\ref{sec:diff-tracking}, \prevrevs{\version{i}} 
is the ordered list of all previous revisions,
$\left[ \version{1}, \version{2}, \ldots, \version{i-1} \right]$.
We use the full list of previous revisions to ensure that restored text
is assigned to the right author.
In practice, the history of an article can extend to several thousand
revisions, but an editor is likely only to look back a small number
of revisions when searching for text to restore.
We arbitrarily limit the list of previous revisions to ten:
\begin{equation*}
    \prevrevs{\version{i}} = \left[ \version{j} \colon
	max(1,i-10) \le j < i \right],
\end{equation*}
but observe that better selections can be made, as described
in~\cite{Chatterjee2008}.

We can further speed up tracking authorship by reducing the number
of potential matches, similar to the optimization for the differencing
algorithm.
If we assume that the quality function always prefers matches from
more recent revisions to matches from older revisions
(as is the case with $(-c, k, q_{block})$ as a quality function),
then we can construct the edit script piecewise by differencing
only a single revision at a time.
When looking at older chunks, the fact that the target has already
been partially matched will reject many potential matches.
Appendix~\ref{app:basictextsrc-perl} shows an implementation of the
basic text tracking algorithm, and Appendix~\ref{app:fastertextsrc-perl}
shows the optimization; Table~\ref{tab:comparetext} shows representative
running times.

\begin{table}
\begin{center}
\begin{tabular}{| c | c || c || c | c |}
\hline
Target & Previous & WikiTrust & WikiTrust \\
Rev & Revs & BasicTextTracking & FasterTextTracking \\
\hline
12175065 & 1 & 12.50s & 13.93s \\
14057051 & 2 & 36.52s & 2.49s \\
17039312 & 3 & 68.79s & 3.61s \\
20060015 & 4 & 144.42s & 3.14s \\
20551181 & 5 & 933.44s & 21.50s \\
\ldots & \ldots & \ldots & \ldots \\
24358877 & 10 & 5064.09s & 26.94s \\
25306944 & 10 & 6717.15s & 33.74s \\
34009105 & 10 & 6480.42s & 3.14s \\
\hline
\end{tabular}
\end{center}
\caption[Comparing execution times of our two
    text tracking algorithms.]{Comparing the execution times of our
    \textbf{basic} and \textbf{fast} text tracking
    algorithms on selected versions of the article
    ``Santa Cruz Beach Boardwalk.''}
\label{tab:comparetext}
\end{table}


Another optimization is again with respect to the handling of the
previous revisions.
In defining the matching algorithm, we give preference to more recent
versions before falling back to matches against older revisions.
Let us consider the case where a piece of text does not match in the
most recent revision \version{k},
but does match in some earlier revision \version{i}, so that we have $i < k$.
Since the text does not match in any \version{j} for $j > i$,
the text must have been deleted in \version{i+1} and
never restored.
The size of deleted text in any revision is typically much smaller
than the size of the live text, so our final optimization is
to check for matches against the deleted text rather than the full
live text of previous revisions.
Note that this can change the size of matches that are found,
so that the quality function might not select matches in a preferred way.

\begin{comment}
\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Priority queue of longest possible matches.}
\SetKwRepeat{Repeat}{repeat}{until}
$l_1 \leftarrow$ length($w_1$) \;
$l_2 \leftarrow$ length($w_2$) \;
$idx \leftarrow $ IndexWordsByPositions($w_1$) \;
\For{$i_2 \leftarrow 0$ \KwTo length($w_2$)}{
    \If{$matched[i_1,i_2] \neq 1$}{
	$matches \leftarrow idx[ w_2[i_2] ]$ \;
	\For{$m \leftarrow 0$ \KwTo  length($matches$)}{
	    $i_1 \leftarrow matches[m]$ \;
	    $k \leftarrow 0$ \;
	    \Repeat{$i_1 + k < l_1 \wedge i_2 + k < l_2 \wedge
		w_1[i_1+k] = w_2[i_2+k]$}
	    {$k \leftarrow k+1$ \; }
	    \If{$k > 2$}{
		\For{$i \leftarrow 0$ \KwTo $k-1$}{
		    $matched[i_1+i,i_2+i] = 1$ \;
		}
		$q \leftarrow$ MatchQuality($k, i_1, l_1, i_2, l_2$) \;
		$prioQ.add( q, (i_1,i_2,k) )$ \;
	    }
	}
    }
}
\caption{Fill priority queue with large matches.}
\end{algorithm}

\begin{algorithm}[H]
\KwIn{A priority queue, $prioQ$, of longest possible matches.}
\KwOut{Edit script of block matches between $w_1$ and $w_2$.}
\While{$m \leftarrow prioQ.pop()$}{
    $(i_1, i_2, k) \leftarrow m$ \;
    \tcp{Scan for unmatched subregions}
    $start \leftarrow 0$ \;
    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
	matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
    $end \leftarrow start + 1$ \;
    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
	matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
    \BlankLine
    \eIf{$end - start = k$}{
	\tcp{Found a match for our edit script}
	$editScript.add( \mathbf{MOV}(i_1, i_2, k) )$ \;
	\For{$i \leftarrow start$ \KwTo $end - 1$}{
	    $matched_1[i_1+i] = 1$ \;
	    $matched_2[i_2+i] = 1$ \;
	}
    }{
	\tcp{Found unmatched subregion, so calculate residual match}
	\While{$start < k$}{
	    $k' \leftarrow end - start$ \;
	    $q \leftarrow$ MatchQuality($k', i_1, l_1, i_2, l_2$) \;
	    $prioQ.add( q, (i_1,i_2,k') )$ \;
	    $i_1 \leftarrow i_1 + end$ \;
	    $i_2 \leftarrow i_2 + end$ \;
	    $k \leftarrow k - end$ \;
	    \tcp{Scan for unmatched subregions}
	    $start \leftarrow 0$ \;
	    \While{$start < k \wedge (matched_1[i_1+start] = 1 \vee
		    matched_2[i_2+start] = 1)$}{$start \leftarrow start+1$ \;}
	    $end \leftarrow start + 1$ \;
	    \While{$end < k \wedge matched_1[i_1+start] \neq 1 \wedge
		matched_2[i_2+start] \neq 1$}{$end \leftarrow end+1$ \;}
	}
    }
}
\caption{Greedy matching between two strings.}
\end{algorithm}


\begin{algorithm}[H]
\KwIn{Two word lists, $w_1$, and $w_2$.}
\KwOut{Edit script to transform $w_1$ into $w_2$.}
compute priority queue of longest matches \;
run through priority queue selecting best matches \;

\caption{Fast differencing algorithm.}
\end{algorithm}

\end{comment}

\mynote{Maybe it would be easier to put optimizations into a
    list, since I refer to them by name in the next chapter.}

\begin{description}
\item[\textbf{min wordlen}] 
    We can reduce the number of potential matches examined
    by the algorithm by requiring a minimum number of words to match
    before a string will be added to the priority queue.
    WikiTrust accomplishes this by index word tuples in the
    hashtable of matches computed for the greedy algorithm;
    for edit distances, we index word pairs, and for text authorship
    we track word triples.

\item[\textbf{max matches}]
    The hashtable constructed for the greedy algorithm is used
    to find the initial string prefixes for all possible matches.
    If there are too many matching string prefixes (for example,
    a common phrase such as ``to the'' might appear many times
    within a single article), then we believe that the string prefix
    is too common to really be considered to have been authored
    by a single individual.
    WikiTrust ignore string prefixes which have more than 50 matches.

\item[\textbf{longest match}] Put only the longest possible match
    onto the priority queue, instead of every possible match (that is, don't
    place substrings of the longest match onto the priority queue).
    This saves a lot of CPU and memory by not having to store
    these substrings in the priority queue.
    Also, since the longest match is likely to be the one that is
    actually selected, there is a savings from not having to remove
    the substrings from the priority queue later.
    The complication of this optimization is that if a string on
    the priority queue has been partly matched by an earlier selection,
    then the residual non-matched parts of the string need to
    be placed into the priority queue for possible selection later.
    This makes the assumption that the quality function always
    prefers longer matches over shorter matches.
    Appendix~\ref{app:fasterdiffsrc-perl} demonstrates this
    modification, with some representative runtimes shown
    in Table~\ref{tab:comparediff}.

\item[\textbf{header/trailer}] When users edit an article, the beginning
    and end of the article are not likely to change much.
    If the first few words of an article are the same from one revision to
    the next, it is reasonable to conclude that they are a match
    without having to test other possible sources of block moves
    from the article; this fits well with our desire that the resulting
    edit script try to match a human description of the edit.
    This pre-matching of the heading and trailing portions of the
    article can significantly reduce the number of potential matches
    that are computed in the initial step of the algorithm.
    Some care should be taken in the handling of authorship;
    if the header or trailer is duplicated elsewhere in the article,
    then the original authorship still needs to be retained.

\end{description}


\subsection{Thoughts On Evaluation}

Historically, text differencing algorithms are judged
on the complexity of the algorithm, or how the memory
usage scales with the size of the inputs, or in the
size of the resulting edit script.
These are important concerns, especially in the context of
trying to process the over 1.5TB of data that make up the
revision history of the English Wikipedia.
Our goal is more than efficiency, however; we are trying
to measure the work done by authors of a collaborative work.
People are far from efficient, so our solution attempts to
simply model a human conception of how text is rearranged
and reinstated from previous revisions.

The problem we are faced with, then, is how to evaluate
this measure; do our calculations correlate well with
human intuition?
Is it even possible to survey a large number of people
to generate edit scripts describing the transformation from
one revision to the next, or can edit script only be generated
by video taping editors caught in the act?
Although we do not evaluate the performance of our difference
algorithm directly, we speculate that it might be possible to
indirectly evaluate the performance by examining and evaluating
a different problem which uses a difference algorithm as a basis.
We investigate this idea in the evaluation of our edit quality
measures, presented in the next chapter.

