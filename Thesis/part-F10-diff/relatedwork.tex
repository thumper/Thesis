\section{Related Work}
\label{sec:diff-related}

Traditionally, the \intro{author attribution} problem is one of
identifying the anonymous author of a work, based on characteristics of
writing style and word choice~\cite{Juola2006}.
Our context is different: we analyze a revisioned collaborative
document, where the author of each revision is known.
The difficulty in our situation is determining how to assign ``credit''
for words when they might be a reintroduction of older text.
To approach this problem, we consider difference algorithms as a
starting point.

Finding the difference between two strings is a long-studied problem
known as \intro{string-to-string correction}~\cite{Wagner1974}.
Initial work in computer science on this problem revolved around finding the
Longest Common Subsequence (LCS)~\cite{Gusfield1999}, which identifies the
sequence of symbols\footnote{Note that the symbols need not be
contiguous.}
in common between the two strings.
From there, it is easy to identify the optimal
set of insertions, deletions, and replacements to transform
one string to the other.
The well-known UNIX \texttt{diff} utility~\cite{Hunt1976} is based
on this algorithm.
Myers shows that LCS and shortest edit script are equivalent to finding
shortest/longest paths in an edit graph~\cite{Myers1986}.

There are a score of variations on the basic problem of
comparing strings; an excellent survey of the field is
presented in~\cite{Sankoff1999}.
An important concept emphasized in~\cite{Sankoff1999} is that
difference and distance analyses generally take one of
three forms: \intro{trace}, \intro{alignment} and \intro{listing}.
Traces are used to represent the common units (\eg letters or words)
between the source and target strings.
\begin{center}
  \begin{tabular}{c c c c c}
B & I & R & D & \\
| &   & | &   & $\!\!\!\!\!\!\!\!\!\!$\textbackslash \\
B & O & R & E & D \\
  \end{tabular}
\end{center}
Alignments also find the commonality between the source and target
strings, but allow more flexibility in the specification of
the non-common portions.
\begin{center}
  \begin{tabular}{c c c c c c c c c c c c c}
B & -- & I & R & -- & D & \qquad\qquad & B & I & -- & R & -- & D \\
B & O & -- & R & E & D & \qquad\qquad  & B & -- & O & R & E & D \\
  \end{tabular}
\end{center}
Listings are the most general of all, being
organized as a sequence of elementary edit operations
to transform the source string into the target string.
\begin{center}
  \begin{tabular}{l l}
  BIRD & \textit{Delete I} \\
  BRD & \textit{Insert O} \\
  BORD & \textit{Insert E} \\
  BORED & \\
  \end{tabular}
\end{center}

The original string-to-string correction problem statement
only permitted insertion, deletion, and substitution operations,
but other applications
(for instance, spelling correction) require allowing \intro{transposition}
of characters to be an available operation.
Lowrance and Wagner~\cite{Lowrance1975} tackle this natural extension,
and produce an algorithm to solve it using \textit{restricted traces}
which allow simple transpositions.
Wagner later develops another algorithm which allows unrestricted
traces, essentially calculating the listing
distance~\cite{Wagner1975,Sankoff1999}.

Our solution for WikiTrust greedily selects the \intro{best match}
(according to some criteria we define later)
from the set of all possible matches between the set of strings, marks
the match, and then proceeds to find the next available \textit{best match}.
The general principle is the same as that used in the
Smith-Waterman algorithm~\cite{Smith1981} (also~\cite[Ch.~10]{Sankoff1999})
for \textit{local sequence alignment}, which iteratively locates the
best local alignment between two nucleotide
sequences.\footnote{Nucleotide sequences are more commonly referred to
as DNA sequences, represented as sequences of A, C, T, and G to
represent the nucleotide base pairs.  Matching between two sequences of
DNA is important for understanding, among other things, the similarities
and differences between species.}
Our solution is a specialization that does not allow insertions
and deletions within a \textit{best match} (in the context of
Smith-Waterman, the score for an insertion or deletion would be $-\infty$).
While we forbid insertions and deletions to be components of
a \textit{best match}, we still have other preferences on matching:
we prefer matches to be similarly situated in their strings.
To achieve this, we modify the overall score of a proposed match to take
into account the position of the match in the source and target strings.

Concerned with storing deltas as part of a revision control system,
Tichy investigates the idea of finding the shortest edit script as
the primary goal in solving the string-to-string correction problem.
As part of achieving that goal, Tichy introduces \intro{block moves}
that describe a section of text in the source string as exactly matching
a section of text in the target string~\cite{Tichy1984}.
This is the same notion as \intro{transpositions} by
Lowrance and Wagner~\cite{Lowrance1975}, but Tichy is optimizing
for shortest edit script where there is no penalty for transpositions
of blocks of characters.
(It should also be noted that ``shortest'' specifically depends on
the encoding of a block as a single edit operation.)
Tichy's algorithm loops through the target string and greedily chooses
the longest match from the source string, which Tichy proves as
generating the shortest edit script transforming the source to the
target string.
This greedy solution is refined by subsequent work in several ways:
indexing on string prefixes~\cite{Obst1987},
efficient generation of deletion operations~\cite{Reichenberger1991},
and restricting block moves to be in sequence~\cite{Burns1997}.
Our work evolved from this line of research, but
incorporates the notion of \textit{best match}
in a way not dissimilar to that found in the Smith-Waterman
algorithm~\cite{Smith1981}.

Our work presents a different
dimension of the string-to-string correction problem.
In previous formulations, solutions are optimized for abstract
performance characteristics (\eg running time or
edit distance~\cite{Damerau1964,Levenshtein1966}); these solutions sometimes
result in edit scripts which are confusing to human readers.
This confusion arises from ``unnatural'' edit scripts that ignore boundaries
at the sentence or paragraph level to achieve efficiency, in contrast to
how humans think of content at multiple levels of abstraction.
For WikiTrust, our interest in string-to-string correction is in trying
to estimate the amount of effort put forth by editors, so we prefer edit
scripts which are more likely to describe the actions taken by human
editors rather than those which are most efficient.
To achieve this, we use the block moves of Tichy~\cite{Tichy1984}, but rather
than using greedy selection of the longest match given a specific
starting location, we perform a global greedy selection of the
\textit{best match} (\eg the longest match) anywhere within the
source and target strings, reminiscent of the Smith-Waterman
algorithm~\cite{Smith1981}.
Fong and Biuk-Aghai extend the WikiTrust work by applying
the hierarchical differencing idea of
Neuwirth~\etal~\cite{Neuwirth1992} to our differencing algorithm,
and additionally classify components
of the edit script according to common behaviors of Wikipedia editors.

Historians of computer science will note a relation
to \textit{transclusions}~\cite{Nelson81}.
Nelson's vision for hypertext included the notion
of micropayments to authors, which required detailed
and manual attributions of text.
We propose automatically detecting attribution
(equivalently, transclusions of portions from earlier revisions)
in the context of a revisioned document edited
by multiple authors.


