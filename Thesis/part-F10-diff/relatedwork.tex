\section{Related Work}
\label{sec:diff-related}

Finding the difference between two strings is a long-studied problem,
known as the \intro{string-to-string correction} problem~\cite{Wagner1974}.
Initial work on this problem revolved around finding the
Longest Common Subsequence (LCS)~\cite{Gusfield1999}, which identifies the
sequence of symbols\footnote{Note that the symbols need not be sequential.}
in common between the two strings.
From there, it is easy to identify the optimal
set of insertions, deletions, and replacements to transform
one string to the other.
The well-known UNIX \texttt{diff} utility~\cite{Hunt1976} is based
on this algorithm.

Tichy advances the idea that finding the shortest edit script is a
useful solution to the string-to-string correction problem, and
introduces \intro{block moves} as a useful operation in edit
scripts~\cite{Tichy1984}.
The algorithm loops through the target string and greedily chooses
the longest match from the source string, which Tichy proves as
generating the shortest edit script transforming the source to the
target string.
This greedy solution is refined by in several way,
by indexing on string prefixes~\cite{Obst1987},
efficient generation of deletion operations~\cite{Reichenberger1991},
and restricting block moves to be in sequence~\cite{Burns1997}.
Interestingly,
Myers shows that LCS and shortest edit script are equivalent to finding
different paths in an edit graph~\cite{Myers1986}.

Specific to the Wikipedia, our work presents a different
dimension of the string-to-string correction problem.
In previous formulations, solutions are optimized for abstract
performance characteristics (\eg running time or
edit distance~\cite{Levenshtein1966}); the Wikipedia instead computes text
differences to present for human understanding.
We use the block moves of Tichy~\cite{Tichy1984}, but rather
than using greedy selection of the longest match given a specific
starting location, we perform a global greedy selection of the
\textit{best match} (\eg the longest match) anywhere within the
source and target strings.
Fong and Biuk-Aghai extend the WikiTrust work by applying
the hierarchical differencing idea of
Neuwirth~\etal~\cite{Neuwirth1992} to our differencing algorithm,
and additionally classify components
of the edit script according to common behaviors of Wikipedia editors.

Historians of computer science will note a relation
to \textit{transclusions}~\cite{Nelson81}.
Nelson's vision for hypertext included the notion
of micropayments to authors, which required detailed
and manual attributions of text.
We propose automatically detecting attribution
(equivalently, transclusions of portions from earlier revisions)
in the context of a revisioned document edited
by multiple authors.


