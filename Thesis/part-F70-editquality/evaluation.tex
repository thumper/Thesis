\section{Evaluation}

A large challenge in creating quality measures is the
problem of evaluating the performance of the measure.
``Quality'' is an imprecise notion by itself, because it
necessarily must be evaluated with respect to some attribute.
For example, within the Wikipedia we might evaluate the quality
of a contribution along any of these dimensions:
\begin{itemize}
\item grammar
\item diction
\item neutral point of view
\item factual correctness
\end{itemize}
Our text and edit longevity measures try to go one step
further into the fuzzy world of human evaluations by using
later edits as a basis for inferring sentiment about earlier edits.
Is this a valid inference to make?
How can such a question be answered?

The surest way to measure sentiment would be to interview
users as they are making edits to the Wikipedia and documenting
their thought processes as they read an article and make the
decision to edit an article --- but this would require an enormous
effort to collect enough data for performance evaluation.
We propose that we can grossly measure the sentiment of the
community by recognizing that there is a generally agreed upon
standard of articles being of ``encyclopedic quality''
which allows people to recognize vandalism when they see it.
Accepting that premise allows us to use the PAN-WVC-10
corpus~\cite{Potthast2010a} as a manually annotated data set for
such an evaluation.

The PAN-WVC-10 corpus was used to compare the performance of
solutions for the 1st International Competition on Wikipedia
Vandalism Detection~\cite{Potthast2010b}.
We use it in a similar way here, but with an important distinction:
we use information ``from the future'' to calculate our
quality values for the annotated revisions.
Standard vandalism detection tools make their determination
immediately as the edit is made, so that any vandalism can be
quickly repaired by others.
The necessity for a quick classification precludes waiting
for future edits or rating to corroborate the edit being judged;
we term this variation of the problem \textit{immediate vandalism detection}.
By definition, our two longevity metrics use later edits to
measure the quality of the revision being judged; we call
this the \textit{historical vandalism detection} problem.
Historical vandalism detection has applications such as selecting
high quality revisions for DVD compilations or for presentation
to school children.

The PAN-WVC-10 corpus contains 32,439 edits, where each revision was
manually reviewed by at least three annotators to assign a label
of either ``regular'' or ``vandalism.''
We used the dump of the English Wikipedia from January 30, 2010
to extract the text of each annotated revision, along with the revision
before and the ten filtered revisions following so that we could compute
our text longevity and edit longevity measures for each annotated edit.
Out of the total 32,439 revisions, we were able to compute
edit longevity scores for 27,730 revisions, and text longevity
scores for 28,453 revisions.

We used the straight-forward transformation to convert
each quality score from its normal range into the range
$[0,1]$, to be interpreted as a probability that the named
revision was the result of vandalism.
As in the PAN 2010 competition~\cite{Potthast2010b}, we use the
\texttt{perf}\footnote{\url{http://kodiak.cs.cornell.edu/kddcup/software.html}}
package to evaluate the performance of our quality measures
by computing the area under the receiver operating characteristic
curve \footnote{\url{http://en.wikipedia.org/wiki/Receiver_operating_characteristic}},
and also the mean precision value.

\subsection{Difference Algorithms}

Each formula for edit distance we have defined so far is calculated
from the operations within the edit script describing the transformation
from the source revision to the target revision.
This edit script is highly dependent on the algorithm used to
compute the difference between the revisions.
To provide a more complete picture of how the choice of difference
algorithm affects the performance of the quality measures,
we present an evaluation of some variations of the algorithms.
The list of algorithm variations that we tried are as follows:

\begin{description}

\item[\textbf{diff0}] The difference algorithm implemented on the production
    WikiTrust system running live on a single 8-core CPU machine.
    It includes every optimization discussed in
    Section~\ref{sec:diff-optimizations},
    and is able to process user reputations and text trust fast enough
    to keep up with each change being made to the English Wikipedia.
    The source code implementing this version is available from
    Github.\footnote{\url{http://www.github.com/collaborativetrust/WikiTrust}}

\item[\textbf{diff1}] This version is the same as \textbf{diff0},
    except that it does not include the \textbf{header/trailer}
    optimization.
    We separate this optimization from the others because we believe
    it makes such a dramatic difference on the running time.
    This is borne out by the runtime data in Table~\ref{tab:editlongeval},
    which shows that \textbf{diff0} is roughly twice as fast
    as \textbf{diff1}, with only a small decrease in the
    ability of edit longevity to label vandalism correctly.

\item[\textbf{diff2}] This diff algorithm implements exactly the matching
    we describe in Chapter~\ref{ch:diff}, with all optimizations except
    for the \textbf{header/trailer} optimization.
    A Perl language implementation of
    this algorithm is listed in Appendix~\ref{app:fasterdiffsrc-perl}.

\item[\textbf{diff3}] This version is a modification of \textbf{diff2}
    to also include the \textbf{header/trailer} optimization.

\item[\textbf{diff4}] This version is a modification of \textbf{diff3}
    to remove the \textbf{longest match} optimization.
    This is primarily a sanity check to verify that the performance
    of \textbf{diff3} and \textbf{diff4} are the same, which
    we find to be true in Table~\ref{tab:editlongeval}.
    \mynote{There was a bug in the code so that diff3 and diff4 were
    exactly the same.  Need to rerun and verify that they have the
    same performance.}

\item[\textbf{diff5}] This version is a modification of \textbf{diff3}
    to remove the \textbf{prev matches} optimization.
    This is primarily to verify that \textbf{prev matches} is
    actually an optimization.
    \mynote{Not yet implemented.}

\end{description}



\subsection{Match Quality Formulas}

The match quality formula is the key to our greedy algorithm
choosing matches which meet the desired properties outlined
in Chapter~\ref{ch:diff}.
We experimented with the following \mynote{?howmany?} definitions,
which take as parameters the length of the match (parameter $k$),
the matching positions in the source and target revisions
(parameters $i_1$ and $i_2$), and the total length of
each revision (parameters $l_1$ and $l_2$);
a ``chunk index'' (parameter $c$) is also passed to the
match quality function, but is only used in the computation
of text longevity.
The match qualities computed by our system are tuples,
with lexicographically smaller tuples considered to be
of higher quality.

\begin{description}

\item[\textbf{mq0}] This is the quality function used in the
    live production WikiTrust system.
    \begin{equation*}
    q := \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    It computes the midpoint of each end of the match,
    and then compares their relative positions within
    the source and target revisions.
    The closer to the same relative position each end of
    the match is, the closer to zero $q'$ gets.
    The final match quality tuple used is $(-k, -c, q)$.

\item[\textbf{mq1}] This is the quality function used in the
    original WikiTrust publication~\cite{Adler2007}.
    \begin{equation*}
    q' := \cfrac{k}{\min{l_1, k_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The match quality tuple is $(-l, -c, -q')$.

    Note that this match quality will not work with
    \textbf{diff0}, \textbf{diff1}, or \textbf{diff2}.

\item[\textbf{mq2}] This is a modification of \textbf{mq1}
    to change the priority of the chunk index; this would
    only be reflected in text longevity.
    \begin{equation*}
    q' := \cfrac{k}{\min{l_1, k_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The match quality tuple is $(-k, c, -q')$.

\item[\textbf{mq3}] This modification of \textbf{mq0}
    changes the priority of the chunk index, to test
    the effect on text longevity.
    \begin{equation*}
    q := \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The final match quality tuple used is $(-k, c, q)$.

\item[\textbf{mq4}] This is the quality function used in the
    original WikiTrust publication~\cite{Adler2007}.
    \begin{equation*}
    q' := \cfrac{k}{\min{l_1, k_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The match quality tuple is $(0, -c, -q')$,
    but note that this match quality will not work with
    \textbf{diff0}, \textbf{diff1}, or \textbf{diff2}
    since it doesn't guarantee that longest matches
    will occur first.

\end{description}

\subsection{Edit Distance Formulas}

\begin{description}

\item[\textbf{ed0}] This is the edit distance computation used
    in the live production system of WikiTrust.
    Please refer to the source code, available at
    Github,\footnote{\url{http://www.github.com/collaborativetrust/WikiTrust}}
    for the details of this implementation.

\item[\textbf{ed1}] This edit distance computes the sum of the lengths
    of insertions and deletions, but ignores move operations.
    \begin{equation*}
    I_{tot} + D_{tot}
    \end{equation*}

\item[\textbf{ed2}] This edit distance computes the sum of the lengths
    of insertions, deletions, and move operations.
    \mynote{This probably isn't useful since this value is related
    to the lengths of the two strings, minus the overlap.}
    \begin{equation*}
    I_{tot} + D_{tot} + M_{tot}
    \end{equation*}

\item[\textbf{ed3}] This edit distance computes the sum of the lengths
    of insertions and deletions, but includes a correction factor
    to account for the fact that some insertion and deletion pairs
    are actually replacements.
    \begin{equation*}
    I_{tot} + D_{tot} - \frac{\min{I_{tot}, D_{tot}}}{2}
    \end{equation*}

\item[\textbf{ed4}] This is the edit distance described in
    the original WikiTrust paper~\cite{Adler2007}.
    \mynote{TODO: Add this description in.}

\end{description}

\subsection{Results}

\mynote{Results not finished yet.  Here are some initial numbers,
but I am regenerating everything based on new definitions above.}

A complication in our evaluation is our restricted setting of
\textit{filtered} revisions, where sequential revisions by the
same author are filtered out to leave only the last revision
in the sequence.
This would limit us in evaluating the performance of our
quality measures, so we modified the system in the following way:
we do not filter revisions annotated in the PAN-WVC-10 corpus,
or the immediately preceding revision (even when they have the
same author), but we do filter revisions \textit{after} the annotated
revision in the usual way.
Even with this loosening of the revision filtering, several
revisions are still not evaluated for quality; the two
primary reasons for no evaluation are a lack of subsequent
edits to base the evaluation on, or the revision was not
substantially different from the previous revision
(that is, $\dist{}{k-1,k} = 0$, which can happen when only
whitespace changes).

\subsubsection{Edit Longevity}

Edit longevity performs quite well, at over 91\% ROC-AUC;
very close to the performance of the winning solution from
the Wikipedia Vandalism Detection competition~\cite{Potthast2010b}.
The PR-AUC is more middling, near fourth place in the competition.

This result is very encouraging, because it means that can
estimate the quality of revisions in a relatively simple way.
Knowing the quality of edits, we can build up a reputation
system for authors based on the quality of their work.


\input{part-F70-editquality/quality-table-editlong}
\mynote{Comparison of the performance of edit longevity
  using three different definitions of edit distance.
  The edit quality computed was used as a predictor for the probability
  that annotated revisions from the PAN-WVC-10 corpus were vandalism.
  Each measure is evaluated by computing the area under the
  receiver operating characteristic (ROC) curve.
} 


\subsubsection{Text Longevity}

The number of revisions is good: 28,453 / 32,439.
Text longevity is not calculated when there are no
judging editors after the revision we are computing
the text longevity for.
This argues that 28,453 is an upper bound for the number
of revisions that we are able to calculate \textit{any}
quality measure for.

The ROC-AUC of 85.9\% seems good, but ranks in seventh place
in the PAN-2010 competition.
But this is quite bad, because we're doing \textit{historical
vandalism detection}, where we use data from the future to
inform our evaluation of the revision.

The most interesting result in this table of results
is the fact that the match quality function made
nearly no difference in the predictive ability of
text longevity.

\mynote{Try mq just using length.}
\mynote{mq4 was only tested with diff2 -- diff4.}

\input{part-F70-editquality/quality-table-textlong}

\subsubsection{Explaining the Performance Gap}

Why does text longevity not do so well as edit longevity?
As part of our investigation, we started looking at specific
instances of text longevity values.
In Figures~\ref{fig:ts-GeorgeWBush} and~\ref{fig:ts-SantaCruzBeachBoardwalk},
we see the text survival for two different contributions;
both do seem to have the general ``exponential'' shape
that we previously described.
Also computed in each figure is the text longevity measure based on
the 20~revisions shown in each graph, but notice that the text
longevity computed for
Figure~\ref{fig:ts-GeorgeWBush} doesn't exhibit the curve we expect.

\begin{figure}[tbph]
\centering
\framebox{\includegraphics[width=0.8\textwidth]{part-F70-editquality/graph-TS-GeorgeWBush-8574490}}
\caption{The text survival graph for the text contributed early
        in the history of article \textit{George W.~Bush}.
	The graph also shows the text survival quality
	computed based on 20~revisions.
	}
\label{fig:ts-GeorgeWBush}
\end{figure}

\begin{figure}[tbph]
\centering
\framebox{\includegraphics[width=0.8\textwidth]{part-F70-editquality/graph-TS-SantaCruzBeachBoardwalk}}
\caption{The text survival graph for the text initially contributed
	as part of the article \textit{Santa Cruz Beach Boardwalk}.
	The majority of the editing to the contributed text happens
	in the next few revisions, before the text stabilizes.
	The graph also shows the text survival quality
	computed based on 20~revisions.
	}
\label{fig:ts-SantaCruzBeachBoardwalk}
\end{figure}

The explanation for this discrepancy turns out to be a flaw in our
thinking about the original model.
While the text survival for contributions does seem to have an
exponential look to it, exponentials do not approach some fixed
non-zero value --- they approach zero.
In order to fit the curve we are describing, the last value
(in the case of the data shown in Figure~\ref{fig:ts-GeorgeWBush},
the amount of text that survives after the $20^{th}$ revision)
should be taken as the ``zero reference point'' which is subtracted
from all the values.
Applying our exponential curve fitting technique to these new values
will give a much better approximation to the data.
The problem with this better fit is that it changes the meaning of
a score of zero; instead of meaning that the text was immediately deleted,
a score of zero would mean that the text immediately reached its
final survival level.
In other words, we would be measuring how quickly the text stabilizes,
rather than how much agreement there was that the text belonged in
the article.

\subsection{The Triangle Inequality}

We previously noted that our formulation of edit longevity does
not precisely stay within the desired range of $[-1,+1]$,
because our use of a greedy differencing algorithm does not
guarantee the shortest edit script.
In order to quantify how often such an error might crop up,
we counted how many triangles were evaluated for our PAN-WVC-10
revisions, and how many of those fell outside of the range $[-1,+1]$.

\mynote{Need to flesh out this section, and include references
to works that define when triangle inequality holds and when it
doesn't.  Seems to be related to generalized levenshtein distance (GLD)
and maybe normalized levenshtein distance.}

