\section{Evaluation}

A large challenge in creating quality measures is the
problem of evaluating the performance of the measure.
``Quality'' is an imprecise notion by itself, because it
necessarily must be evaluated with respect to some attribute.
For example, within the Wikipedia we might evaluate the quality
of a contribution along any of these dimensions:
\begin{itemize}
\item grammar
\item diction
\item neutral point of view
\item factual correctness
\end{itemize}
Our text and edit longevity measures try to go one step
further into the fuzzy world of human evaluations by using
later edits as a basis for inferring sentiment about earlier edits.
Is this a valid inference to make?
How can such a question be answered?

The surest way to measure sentiment would be to interview
users as they are making edits to the Wikipedia and documenting
their thought processes as they read an article and make the
decision to edit an article --- but this would require an enormous
effort to collect enough data for performance evaluation.
We propose that we can grossly measure the sentiment of the
community by recognizing that there is a generally agreed upon
standard of articles being of ``encyclopedic quality''
which allows people to recognize vandalism when they see it.
Accepting that premise allows us to use the PAN-WVC-10
corpus~\cite{Potthast2010a} as a manually annotated data set for
such an evaluation.

The PAN-WVC-10 corpus was used to compare the performance of
solutions for the 1st International Competition on Wikipedia
Vandalism Detection (PAN-WVD~2010)~\cite{Potthast2010b}.
We use it in a similar way here to compare how well our quality
measures are able to predict vandalism within the corpus,
but with an important distinction:
we use information ``from the future'' to calculate our
quality values for the annotated revisions.
Standard vandalism detection tools make their determination
immediately as the edit is made, so that any vandalism can be
quickly repaired by other users.
The necessity for a quick classification precludes waiting
for future edits or rating to corroborate the edit being judged;
we term this variation of the problem \textit{immediate vandalism detection}.
By construction, our two longevity metrics use later edits to
measure the quality of the revision being judged; we call
this the \textit{historical vandalism detection} problem.
Historical vandalism detection has its own set of important
applications, such as selecting high quality revisions for
DVD compilations or for presentation to school children.

The PAN-WVC-10 corpus contains 32,439 edits, where each revision was
manually reviewed by at least three annotators to assign a label
of either ``regular'' or ``vandalism.''
We used the dump of the English Wikipedia from January 30, 2010
to extract the text of each annotated revision, along with the revision
before and the ten filtered revisions following so that we could compute
our text longevity and edit longevity measures for each annotated edit.

We used the straight-forward transformation to convert
each quality score from its normal range into the range
$[0,1]$, to be interpreted as a probability that the named
revision was the result of vandalism.
As in the PAN-WVD~2010 competition~\cite{Potthast2010b}, we use the
\texttt{perf}\footnote{\url{http://osmot.cs.cornell.edu/kddcup/software.html}}
package to evaluate the performance of our quality measures
by computing the areas under the receiver operating characteristic
curve\footnote{\url{http://en.wikipedia.org/wiki/Receiver_operating_characteristic}},
and the precision-recall curve.

\subsection{Difference Algorithms}

The formula for edit distance we defined in
Section~\ref{sec:eq-distances} is calculated
from the operations within the edit script describing the transformation
from the source revision to the target revision.
This edit script is highly dependent on the algorithm used to
compute the difference between the revisions.
To provide a more complete picture of how the choice of difference
algorithms affects the performance of the quality measures,
we present an evaluation of some variations of the algorithms.
Our evaluation for this chapter was implemented in OCaml for performance
reasons\footnote{Compare the performance in Table~\ref{tab:difftiming}
with that in Tables~\ref{tab:comparediff} and~\ref{tab:comparetext}.};
the OCaml source representing these algorithms appears
in Appendix~\ref{app:diffsrc-ocaml}.
The list of algorithm variations that we tried are as follows:
%
\begin{description}

\item[\textbf{diff1}]
    Implementation of the Tichy greedy algorithm, including optimizations
    suggested by Obst~\cite{Obst1987} and
    Reichenberger~\cite{Reichenberger1991}.
    \begin{description}
    \item[Includes:]
        \textbf{min wordlen}, \textbf{max matches},
    \item[Excludes:]
        \textbf{header/trailer}.
        \textbf{longest match}, \textbf{prev matches}.
    \end{description}

\item[\textbf{diff2}]
    Implementation of the Tichy greedy algorithm, including optimizations
    suggested by Obst~\cite{Obst1987} and
    Reichenberger~\cite{Reichenberger1991}.
    Also includes the \textbf{header/trailer} optimization.
    \begin{description}
    \item[Includes:]
        \textbf{min wordlen}, \textbf{max matches}, \textbf{header/trailer}
    \item[Excludes:]
        \textbf{longest match}, \textbf{prev matches}.
    \end{description}

\item[\textbf{diff3}]
    This is a WikiTrust variation of difference algorithm, which prefers
    the globally \textbf{longest match} between the source and target
    revisions.
    Without the \textbf{prev matches} optimization, this version of the algorithm
    uses an associative array to track the matches made so far.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches},
	\textbf{header/trailer}, \textbf{longest match}.
    \item[Excludes:]
	\textbf{prev matches}.
    \end{description}

\item[\textbf{diff4}]
    This variation of our WikiTrust difference algorithm explores another
    combination of optimizations.
    We believe that the \textbf{header/trailer} optimization generates
    an edit script which is closer to a human understanding of text
    differencing, so that we expect this variation to perform worse
    than \textbf{diff5}, which includes that optimization.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches}, \textbf{longest match},
	\textbf{prev matches}.
    \item[Excludes:]
	\textbf{header/trailer}.
    \end{description}

\item[\textbf{diff5}]
    This is an extension of \textbf{diff4} that includes the
    \textbf{header/trailer} optimization.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches},
	\textbf{header/trailer}, \textbf{longest match}, \\
	\textbf{prev matches}.
    \item[Excludes:] \textit{none}.
    \end{description}

\item[\textbf{diff6}] This is the original difference algorithm
    for comparing two revisions in the WikiTrust system, as
    described in~\cite{Adler2007}.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches}, \textbf{longest match},
	\textbf{prev matches}.
    \item[Excludes:]
	\textbf{header/trailer}.
    \end{description}
    This version of the difference algorithm includes other
    functionality not described here.
    The source code implementing this version is available from
    Github,\footnote{\url{http://www.github.com/collaborativetrust/WikiTrust}}
    and corresponds to the function \texttt{edit\_diff\_core} in
    file \texttt{chdiff.ml}.

\item[\textbf{diff7}]
    This is the difference algorithm currently running on the live
    production
    WikiTrust system.
    It is the same as \textbf{diff6}, with the addition of the
    \textbf{header/trailer} optimization.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches}, \textbf{longest match},
	\textbf{prev matches}, \textbf{header trailer}.
    \item[Excludes:] \textit{none}.
    \end{description}
    %
    It is able to process user reputations and text trust fast enough
    to keep up with each change being made to the English Wikipedia.
    The source code implementing this version
    corresponds to the function \texttt{edit\_diff} in
    file \texttt{chdiff.ml}, available via Github.

\item[\textbf{diff8}]
    This diff algorithm implements exactly the matching
    we describe in Chapter~\ref{ch:diff}, with all optimizations except
    for the \textbf{header/trailer} optimization.
    Additionally, a Perl language implementation of
    this algorithm is listed in Appendix~\ref{app:fasterdiffsrc-perl}.
    %
    \begin{description}
    \item[Includes:]
	\textbf{min wordlen}, \textbf{max matches}, \textbf{longest match}.
    \item[Excludes:]
	\textbf{prev matches}, \textbf{header trailer}.
    \end{description}

\end{description}



\subsection{Match Quality Formulas}

The match quality formula is the key to our greedy algorithm
choosing matches which meet the desired properties outlined
in Chapter~\ref{ch:diff}.
We experimented with the nine definitions enumerated below,
which take as parameters the length of the match (parameter $k$),
the matching positions in the source and target revisions
(parameters $i_1$ and $i_2$), and the total length of
each revision (parameters $l_1$ and $l_2$);
a ``chunk index'' (parameter $c$
\footnote{The \textit{chunk index} refers
to which chunk a block of text comes from.
Chunk~0 refers to the full text of the older revision being compared.
Chunk~1 refers to text from the revision previous to that which was
deleted in the edit that resulted in the revision of chunk~0.
Chunk~2 refers to deleted text from the revision previous to the
revision of chunk~1, and so on.
This is an optimization described in Section~\ref{sec:diff-optimizations}
for more efficient calculation of text authorship.
In computing the edit distance, differences are only computed
between two revisions, so we always have $c=0$.
}) is also passed to the match quality function,
but is only used in the computation of text longevity.
The match qualities computed by our system are tuples,
with lexicographically smaller tuples considered to be
of higher quality.


The initial set of match quality functions we present form the baseline
functions of what would be achieved using the standard greedy
algorithms for text differencing.
They use the length of the match and the chunk index in various
combinations.
In these, and later match quality functions we define,
we mark with a dagger ($\dagger$) those functions which are
not compatible with the assumption that longest matches are
preferred, used in our \textbf{longest match} optimization
to the difference algorithms.
%
\begin{description}
\item[\textbf{mq1}]
    This version uses a match quality tuple of $(-k, c, 0)$.

\item[\textbf{mq2}]
    This version uses a match quality tuple of $(-k, -c, 0)$,
    changing the sign of the chunk index to instead prefer
    matches with older revisions.
    Since the chunks older than chunk~0 are only fragments of
    deleted text, potentially longer matches in more recent
    text will be missed; for that reason, we expect this
    match qualiy function to perform worse than \textbf{mq1}.

\item[\textbf{mq3}]
    This version uses a match quality tuple of $(c, -k, 0)$,
    making the matching revision the dominant factor.
    ${}^\dagger$

\item[\textbf{mq4}]
    This version uses a match quality tuple of $(-c, -k, 0)$,
    reversing the preference for matches in the most recent revision.
    We expect this match quality function to perform
    worse than \textbf{mq3}.
    ${}^\dagger$

\end{description}

The next set of match quality functions we present are
based on the match quality described in the
original WikiTrust publication~\cite{Adler2007}.
%
\begin{description}

\item[\textbf{mq5}]
    Define
    \begin{equation*}
    q := \cfrac{k}{\min{l_1, k_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The match quality tuple generated by \textbf{mq5} is $(0, -c, -q)$.
    ${}^\dagger$
    This is exactly the match quality used in~\cite{Adler2007}.

\item[\textbf{mq6}]
    As with \textbf{mq5}, define
    \begin{equation*}
    q := \cfrac{k}{\min{l_1, l_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    To make \textbf{mq5} compatible with the \textbf{longest match}
    optimization to the difference algorithm, we introduce the
    match length as the primary discriminant:
    the match quality tuple for \textbf{mq6} is $(-k, -c, -q)$.

\item[\textbf{mq7}] This is a modification of \textbf{mq6}
    to change the priority of the chunk index.
    Define
    \begin{equation*}
    q := \cfrac{k}{\min{l_1, k_2}} - 0.3 \cdot
        \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The match quality tuple is $(-k, c, -q)$.

\end{description}

The difficulty in working with the original WikiTrust
match quality functions, \textbf{mq5}--\textbf{mq7},
is that the definition of $q$ is doing too much.
It tries to balance length of the match as a fraction
of the document length, against the relative positioning
of the matches.
To get this balance right requires much experimentation across
many documents, and even then might require revision as
the dynamics of group collaboration in a document change over time.
As part of the \textbf{longest match} optimization for the
difference algorithm, it became necessary to separate the
concern for match length from that of relative positioning,
resulting in our current scheme of tuples to represent
the match quality.
This final set of match quality functions tests two variations
of that idea:
%
\begin{description}

\item[\textbf{mq8}] This is the quality function used in the
    live production WikiTrust system.
    Define
    \begin{equation*}
    q' := \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    This computes the midpoint of each end of the match,
    and then compares their relative positions within
    the source and target revisions.
    The closer to the same relative position each end of
    the match is, the closer to zero $q'$ gets.
    The final match quality tuple used is $(-k, -c, q')$.

\item[\textbf{mq9}] This modification of \textbf{mq8}
    changes the priority of the chunk index to test
    the effect on text longevity.
    \begin{equation*}
    q' := \abs{\cfrac{ i_1 + \cfrac{k}{2} }{l_1}
            - \cfrac{ i_2 + \cfrac{k}{2} }{l_2} }
    \end{equation*}
    The final match quality tuple returned is $(-k, c, q')$.

\end{description}

\subsection{Edit Distance Formulas}

The edit longevity measure uses edit distance as a proxy for the
amount of work that an author puts into an edit.
We tested the following definitions of edit distance to understand
how this choice impacts the quality of the results.
%
\begin{description}

\item[\textbf{ed1}] This edit distance computes the sum of the lengths
    of insertions and deletions, that is, the amount of text which
    is either added or deleted from one revisions to the next:
    \begin{equation*}
    I_{tot} + D_{tot}
    \end{equation*}

\item[\textbf{ed2}] Traditionally, edit distance functions incorporate
    all the elements of an edit script.  We define
    this edit distance to compute the sum of the lengths
    of insertions, deletions, and move operations.
    \begin{equation*}
    I_{tot} + D_{tot} + M_{sum}
    \end{equation*}
    where
    \begin{equation*}
    M_{sum}(m,n) = \sum_{\mathbf{Move}(i, j, k) \in E(m,n)} k \\
    \end{equation*}
    Note that any text\footnote{Technically, this should be ``most
    text,'' because of the \textbf{min words} optimization.}
    which appears in both revisions which are being
    compared will be counted as a move operation, even if it has not
    relocated within the document.
    For example, if there is no difference between two revisions of
    an article, the edit script will consist of a single $\mathbf{Move}$
    operation that is the size of the revisions.
    Thus, we expect that this function will perform poorly because
    it will be dominated by the size of the article.

\item[\textbf{ed3}]
    The WikiTrust difference algorithms do not keep track of replacements
    in the text; instead, a replacement appears in the edit script as
    both a deletion and corresponding insertion at the same point in the text.
    We modify \textbf{ed1} to apply a correction factor which
    assumes that some part of the work is always a replacement:
    \begin{equation*}
    I_{tot} + D_{tot} - \frac{\min(I_{tot}, D_{tot})}{2}
    \end{equation*}

\item[\textbf{ed4}] This is the edit distance described in
    Section~\ref{sec:eq-distances}, first proposed in
    the original WikiTrust paper~\cite{Adler2007}.
    \begin{equation*}
    \max(I_{tot}, D_{tot}) - \frac{\min(I_{tot}, D_{tot})}{2}
		+ M_{tot}
    \end{equation*}

\item[\textbf{ed5}] This is the edit distance computation used
    in the live production system of WikiTrust.
    Please refer to the source code, available at
    Github,\footnote{\url{http://www.github.com/collaborativetrust/WikiTrust}}
    for the details of this implementation.
    \mynote{Do I need to say more here?  This is using some
    code to calculate over connected components, but I don't see
    what the intuition of it is.}

\end{description}

\subsection{Results}

A complication in our evaluation is our restricted setting of
\textit{filtered} revisions, where sequential revisions by the
same author are filtered out to leave only the last revision
in the sequence.
This would limit us in evaluating the performance of our
quality measures, so we modified the system in the following way:
we do not filter the specific revisions annotated in the PAN-WVC-10 corpus,
or the immediately preceding revision (even when they have the
same author), but we do filter revisions \textit{after} the annotated
revision in the usual way.
Even with this loosening of the revision filtering, several
revisions are still not evaluated for quality; the two
primary reasons for no evaluation are a lack of subsequent
edits to base the evaluation on, or the revision was not
substantially different from the previous
revision.\footnote{That is, when $\dist{}{k-1,k} = 0$.
This typically happens when there are only whitespace changes
from one revision to the next, but some of the proposed edit distance
measures don't consider the entirety of the edit script and
will have a higher incidence where the distance from the
previous revision is zero.}

\subsubsection{Edit Longevity}

\input{part-F70-editquality/quality-table-editlongbyed}

The chief measure of performance that we consider is
the area under the precision-recall curve (PR-AUC);
we summarize the data here, with
the full data is available in Appendix~\ref{app:editlong-data}.
We chose this as the primary measure because it gives better
discrimination between predictive models for the PAN~2010
corpus, as explained in~\cite{Potthast2010b}.
In the case of our variations, PR-AUC is highly correlated
with ROC-AUC, so the choice does not largely impact the ordering
of the different algorithms.
Our methodology for calculating PR-AUC and ROC-AUC is important
to make clear: a standard evaluation (as was used in the
PAN-WVD~2010 competition) requires predictions for all 32,439
edits in the PAN-WVC-10 corpus; we measure the performance
for each combination of parameters only according to the number
of revisions that a prediction is available for, which varies
in a range from 27,500 to 28,500.
As mentioned previously, we will not calculate a prediction
when the edit distance from the previous revision is zero,
or when there are no suitable judging revisions after the
edit being judged.
By computing the performance according to the number of revisions
for which a prediction is available there is a slight bias towards
parameter combinations which make fewer predictions,
but this allows comparison of each combination in their best
possible light without the distortion of a default guess when
there is not enough data.

The most striking thing about the results presented
in Appendix~\ref{app:editlong-data} is the clustering
by edit distance (and to a much lesser extent, of difference
algorithm) that occurs.
Clearly revealed is that use of \textbf{ed5} leads to the most superior
predictions, while \textbf{ed2} makes the worst predictions
(and \textbf{ed4}, the second worst).
These results align with the intuition that the choice
of edit distance function measures of the amount
of \textit{effort} done by an author in making an edit;
the more closely that choice matches the human intuition,
the better the ability to compute how much effort is \textit{useful}.
In particular, \textbf{ed2} was expected to perform poorly
because its inclusion of all \textbf{Mov} operations leads
to a value that is dominated by the size of the revisions,
and \textbf{ed5} tries the hardest to estimate the amount of
effort that goes into rearranging blocks of text.

In Tables~\ref{tab:editlongbyed5} through~\ref{tab:editlongbyed1},
we present the predictive performance of the algorithms,
controlling for edit distance.
We find that match qualities \textbf{mq1}-\textbf{mq4}
and \textbf{mq6}-\textbf{mq7} are always grouped together.
That is because these match qualities only differ from each
other in how they treat the chunk index, which is not used in the
edit longevity computation.

\input{part-F70-editquality/quality-table-timing}

The typical measure used to compare difference algorithm is the
running time, which we also measured.
We find in Appendix~\ref{app:editlong-data} that the running time
is usually affected by either the
match quality or the edit distance calculations, so we
summarize the results in Table~\ref{tab:difftiming} by
computing the average and standard deviation of running times.
The running time measured is the time required to run the
WikiTrust analysis on the reduced corpus that includes
the PAN-WVC-10 revisions; this includes the time to compute
both edit longevity and text longevity for each revision,
but not the time to sort the resulting statistics and generate
reputation scores for authors.
(Text longevity uses a fixed difference algorithm and only varies
the match quality function, so that it is essentially a constant
overhead in the time.)
The most striking feature of Table~\ref{tab:difftiming} is the unusually
large standard deviations of \textbf{diff6} and \textbf{diff7}.
These two algorithms turn out to have a bimodal distribution.
\mynote{Why?  Connected component algorithm?}

With respect to the different optimizations, Table~\ref{tab:difftiming}
provides some insights.
The most notable conclusion is that Reichenberger's
difference algorithm is faster than the WikiTrust
greedy algorithm by nearly a factor of two.\footnote{The live
production version of our WikiTrust code includes another technique
for speeding up the difference computation, so that the performance
more closely rivals the runtime of the Reichenberger versions.}
With hindsignt, we expect this sort of result because, while both algorithms
inspect every possible match and then mark the actual
match as used, the WikiTrust algorithm additionally
re-traverses each match to verify it or search for
\textit{residual} matches.

In our original justification for the \textbf{longest match}
optimization, we argued that it would save CPU time to not have to test
all the partial matches.
We attempted to test this idea by implementing a version of the
WikiTrust algorithm without the \textbf{longest match} optimization, and
found that the memory requirements were too great to execute on our machines.
Although the optimization is clearly a useful one to make, we note that
Table~\ref{tab:difftiming} shows that the Tichy algorithm with
optimizations (\textbf{diff1} and \textbf{diff2})
still has a significant advantage in runtime performance,
and very similar vandalism prediction performance compared to the
WikiTrust variations.

\vspace{1pc}

The ordering of \textbf{diff3}, \textbf{diff4}, and \textbf{diff5}
allows us to infer that the \textbf{header/trailer} optimization
is more beneficial than the \textbf{prev matches} optimization.
Incorporated into the Reichenberger algorithm, the
\textbf{header/trailer} optimization makes little difference
to the running time.

The caveat to use of the \textbf{header/trailer} optimization
is that it can lead to different edit scripts being computed
to describe the transformation from one revision to the next.
Our hypothesis was that the \textbf{header/trailer} optimization
more naturally conforms to a human description of how to transform
one revision into another, leading to improved predictions of
vandalism; this is not born out by the data in
Tables~\ref{tab:editlongbyed5} through~\ref{tab:editlongbyed1}.
There are three pairs of difference algorithms that are the same
except for the use of the \textbf{header/trailer} optimization:
\textbf{diff1}/\textbf{diff2}, \textbf{diff4}/\textbf{diff5},
and \textbf{diff6}/\textbf{diff7}.
Examining the data, there is no clear advantage to including or
not including the \textbf{header/trailer} optimization in terms of
predictive ability, but it does make a significant improvement in
the running time for the WikiTrust difference algorithms.

\vspace{1pc}

We proposed the \textbf{prev matches} optimization with the expectation
that it would reduce the memory requirements of the algorithm without
substantially changing the performance with respect to predictive
ability.
Comparing \textbf{diff5}, which implements \textbf{prev matches},
with \textbf{diff3}, which does not, we find that the predictive ability
is slightly different for match qualities \textbf{mq1} through
\textbf{mq4}.
We examined this difference more carefully and found that the priority
queue implementation will sometimes return matches in a different
(but equivalent with respect to the priority) order.
Match quality \textbf{mq8} is defined in a way that eliminates this
reordering possibility, and we see that \textbf{diff3} and
\textbf{diff5} always perform the same in this case.

During our exploration of the differences between \textbf{diff5}
and \textbf{diff3}, we also found that the \textbf{prev matches}
optimization interacts with the \textbf{max matches} optimization.
By ignoring overly common string prefixes, the ``previous match'' list
is empty for the next starting position after the common string prefix.
If there is a long match which spans several overly common prefixes,
\textbf{prev matches} loses track that it is traversing a region which
already had a longer match and needlessly adds smaller matches to the
priority queue, creating extra work.
The timing information presented in Table~\ref{tab:difftiming} reveals
that \textbf{prev matches} still has the advantage, but there is perhaps
more performance gains to be had here.


\subsubsection{Text Longevity}

For measuring the performance of text longevity,
we fixed the text matching algorithm to be the one
used in the live production system of WikiTrust,
which is a variation of the difference algorithm
\textbf{diff7}.\footnote{The source code implementing
this functionality is available from Github,
\url{http://www.github.com/collaborativetrust/WikiTrust},
and corresponds to the function \texttt{text\_tracking}
in file~\texttt{chdiff.ml}.}
Edit distance does not affect the text longevity calculation,
so our only significant parameter is match quality;
results are presented in Table~\ref{tab:textshoutA}.
Each variation made predictions for 28,453 revisions.

The match quality function has only a very small influence on
the performance, mirroring the small influence it has on edit longevity.
We believe this is because the possible edit scripts for
each edit have a high degree of overlap; that is,
the different algorithms generally produce the same edit script
except for very minor differences.

\input{part-F70-editquality/quality-table-textlong}
