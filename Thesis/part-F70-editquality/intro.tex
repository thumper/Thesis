\section{Introduction}

One of the key problems in trying to build a reputation system
for the Wikipedia is that, while a massive amount of
data is available, there is little information about how well different
users or articles are performing.
Having such data is important because we would like to base the
descriptive aspect of reputation on the community behavior seen
in practice.
To describe an edit as bad or good, we have to know what the
community thinks of it.

One obvious way to do this is to examine reverts
in the article history.
A revert undoes the action of one or more edits, usually leaving
the article in a state exactly matching an older version.
For example, if user Alice vandalizes an article by blanking it,
user Bob can revert her changes by restoring the article to the
state before Alice's edit.
If there are constructive edits after a bad edit, it is also possible
to selectively undo just the bad edits.
When a revert happens, it is a clear indication that some member
of the community believes that the reverted edit is completely
inappropriate, so many researchers use this as an indicator of
community feedback~\cite{Adler2007,Smets2008,Itakura2009,Belani2010}.

There are two issues with using reverts and undos
as measures of community feedback.
The first is that there are no definitive
annotations\footnote{When using the MediaWiki software,
a standard notation appears in edit comments,
but this is not 100\% reliable, as reverts can be effected manually.}
indicating that reverts or undos have occurred,
but computation can detect
them.\footnote{Detecting undos can be fairly expensive, so
it is typical to restrict checking to a limited number of older revisions.}
The second, more significant issue is that reverts and undos
are a very blunt feedback mechanism; they indicate only complete disapproval.
There are no gradations in this evaluation, which does not make
it a good measure for the quality of a revision, except for judging the very
worst edits.

When we started the WikiTrust project in 2006, our initial thought was to
use page views to get a measure of how well reviewed an article is.
Since the Wikipedia works due to the collective action of everybody
reading the articles, we expected that pages that receive a lot of
views would be more accurate because of the higher collective amount
of scrutiny received; the more eyes looking over the text, the better
reviewed the article would be.
(At the time, we abandoned this line of thinking because page view
information was not available.
Since then, Priedhorsky~\etal show how page views can be estimated
from some available data sets~\cite{Priedhorsky2007}.
Log files are also now
available\footnote{\url{http://dammit.lt/wikistats/}} and
have a user-friendly front-end\footnote{\url{http://stats.grok.se/}}
for simple queries.)
Our experiences during the course of this research is even
bleaker than that: plentiful shallow review is not equivalent to
deeper review, and more eyes does not mean that anyone will take
the effort to correct a mistake.
For example, the \underline{South Pasadena, California} page was vandalized
in May 2008\footnote{\url{http://en.wikipedia.org/w/index.php?title=South_Pasadena,_California&oldid=211466067}}
to add the film ``Triumph of the Will'' as being filmed in that city;
it was not corrected until
April 2009.\footnote{\url{http://en.wikipedia.org/w/index.php?title=South_Pasadena,_California&oldid=282177714}}
This is remarkable because the Nazi propaganda film is well known in film studies,
and South Pasadena is in the Los Angeles area,
where film studies is very popular.
We know from the data available now that roughly 75 people a day
read this article, so we have to wonder at how this error might
have persisted for nearly a year.

The data from the history of revisions seemed to be our only
data source for community sentiment, so
we began to examine edits and attempt manually to track how each
edit fared in its future.
Although there is a great deal of variation in edits, this examination
led us to the hypothesis that text contributions might follow a pattern
of exponential decay.
That is, if an edit is not very good, the bulk of it will be removed
right away, with small amounts more being removed in subsequent edits
until some kernel of the original edit stabilizes and becomes a fixture
within the article.
This became our \intro{text quality} measure, explored further in
Section~\ref{sec:textquality}.

Of course, not all useful work consists of adding text to the Wikipedia.
The RC Patrol and other maintainers all do work that involves
deleting text or editing and rearranging text contributed by others,
and any measure that only looks at how text is added will not capture it.
How does one go about measuring how much of a delete is preserved
in future revisions?
We could answer this by treating it as the complement of insertion
(that is, counting words restored from the delete as a
penalty to the original delete), but measuring word rearrangement
was a completely different beast.
The answer came in thinking of the evolution of an article
like a drunken walk along a road from one village to the next:
there is a definite start and a definite end, but the steps along the way
do not always form a neat path and might even drift off the road for a time.
Defining a measure of forward progress led to our second
quality measure, \intro{edit quality}, which we explain in
Section~\ref{sec:editquality}.

Our motivation for the edit quality measure is that the future
versions of an article represent a consensus by the community about
which contributions were useful and which were not.
Each later author is implicitly making a commentary about the
existing text when they decide to make an edit.
This sort of inference is very like the idea of
\intro{revealed preferences} in economics~\cite{Samuelson1938,Varian2006},
in which consumer preference is inferred from data about their
actual purchases.
Similar to the economic setting, a valid criticism of our inference
is that later authors do not necessarily review the entire article
or its recent history, and so might not be making decisions about
all past contributions.
We believe that modeling user attention --- so that there is greater
belief in our inference for text near the edits made, and less belief
for portions of the article not modified ---
can improve our confidence in the inference made.
We leave the exploration of this idea to future work.

The analogy to software engineering raised in Chapter~\ref{ch:diff}
suggests a host of potential quality measures, such as
understandability, completeness, and reliability~\cite{wiki:SoftwareQuality}.
The difficulty of these measures is that they cannot be measured
in a programmatic way.
(As an example, the reliability of the Wikipedia was compared to that
of the Encyclop{\ae}dia Brittanica by employing experts to review
hand-selected articles from both reference works~\cite{Giles2005}.)

