\newcommand{\sign}{{{\textrm{sign}}}}

\subsection{Features and Classification}

The WikiTrust vandalism detection tool follows a standard two-phase machine learning architecture, consisting of a feature-extraction component followed by a classifier.

\subsubsection{Features}

In selecting the features to feed to the classifier, we have limited our consideration to the features that can be readily derived from the information available in the database tables used by WikiTrust, or by the Mediawiki software that serves the Wikipedia.
This constraint was imposed so that the resulting tool could work on-line, in real-time, providing vandalism detection for any Wikipedia revision in a fraction of a second.
As the WikiTrust database tables replicate some of the information present in the Mediawiki database tables, in practice we could derive all features from the WikiTrust tables alone: this enabled us to implement the vandalism detection tool as a self-contained web API on top of the WikiTrust database at UC~Santa~Cruz.

We describe below the features we extracted.
Not all features we extracted for use by a classifier ended up being used: many were discarded by the classifier training process, as they were of too little significance to be worth using.
%
\begin{itemize}

\item \textbf{Author reputation [Reputation].}  Author reputation is an obvious feature to use, since vandalism tends to be performed predominantly by anonymous or novice users, both of which have reputation 0 in the system.

\item \textbf{Author is anonymous [Anon].}  In addition to author reputation, we also considered the fact whether the author was anonymous or not.
Interestingly, whenever author reputation was included as a feature, the feature stating whether the author was anonymous or not was not used by the classifier.
Evidently, knowing that a revision was authored by a low-reputation author was enough information: whether the author was anonymous, or a novice, did not seem to matter.

\item \textbf{Time interval to the previous revision [Logtime\_prev].}
We provided as features the quantities $\log(1 + t)$, where $t$ is amount of time from the preceding revision, or to the following revision.
We thought this feature might be useful, as spam is usually reverted promptly.
Indeed, the Logtime\_next feature was used, but with a very low threshold of 2.74, corresponding to a delay of only a dozen seconds between a revision and the next one.

\item \textbf{Hour of day when revision was created [Hour\_of\_day].}
We observed a correlation between the probability of vandalism, and the hour of the day at which the revision was created (timing signals have been used in a more sophisticated way for vandalism detection in~\cite{West2010}).  The classifier did not use this feature: either it was unable to exploit it, or the information it contained was subsumed by that contained in other, more significant features.

\item \textbf{Delta [Delta].}  This feature measures the edit distance $d(r, r^{-})$ between a revision and the previous one.  This feature was used by the classifiers, mainly to treat very small edits in a more lenient way than longer ones.

\item \textbf{Revision comment length [Comment\_len].}  The length of the comment is another feature we considered, as we assumed that vandalism tended to be associated with short comments.

\item \textbf{Previous text trust histogram [P\_prev\_hist0 \ldots P\_prev\_hist9].}
Whenever a revision is created, WikiTrust computes the {\em
reputation\/} of each word of the revision, where the reputation is an
integer in the interval $0, \ldots, 9$~\cite{Adler2008b}.
The reputation of a word indicates how well the word has been revised by reputable authors; in particular, words that have been just entered or displaced by authors without reputation (including both novice and anonymous authors) are assigned a reputation of~0.
When the revision is created, WikiTrust also computes a 10-column histogram detailing how many words of the revision have each of the 10 possible reputation values, and stores the histogram in the database, in an entry associated with the revision.
We renormalized the histogram, so that the columns summed to~1, and we used the renormalized value of each column as a feature.

\item \textbf{Current text trust histogram [Hist0 \ldots Hist9].}  The current value of the text trust histogram was also provided as a feature, in this case without any renormalization.

\item \textbf{Histogram difference [L\_delta\_hist0 \ldots L\_delta\_hist9].}  For each possible text trust value $i \in \{0, \ldots, 9\}$, we also included a measure of $\log(1 + |h(i) - h^{-}(i)|) \cdot \sign(h(i) - h^{-}(i))$, where $h$ is the text trust histogram for the current revision, and $h^{-}$ is the text trust histogram for the previous revision.

\end{itemize}
%
\subsubsection{The Classifier}

We based our vandalism tools on standard classifiers, and precisely, on the alternating decision tree classifier available as part of Weka~\cite{Weka09}.
We experimented with various classifiers provided as part of Weka, and the alternating decision tree classifier (ADTree) was the one that at the same time peformed best, and let to the classification models that were the simplest, and the easiest to implement in a web-based API.
Since vandalism is relatively rare, we wished to achieve high recall of vandalism even at the expenses of precision.
To this end, we trained the classifier using a cost matrix that specified that the cost of misclassifying a vandalism as a regular revision was $\beta$ times as large as the cost of misclassifying a regular revision as vandalism.
After various experiments, we settled on very small decision trees, consisting of only 10 or 20 nodes.
In the PAN~2010 submission, we used $\beta=10$ and a 20-node classifier.
As we will see, the 20-node classifier used in the submission is no better than the simpler 10-node classifier we will present in detail.
Again, we attribute this to the predominance of a few, very strong features.
The classifier for the submission was trained with the following command:
%
{\small
\begin{verbatim}
  weka.classifiers.meta.CostSensitiveClassifier
     -cost-matrix "[0.0 1.0; 10.0 0.0]" -S 1
     -W weka.classifiers.trees.ADTree -- -B 20 -E -3
\end{verbatim}
}

