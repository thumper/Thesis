\section{Related Work}
\label{sec:vandalism-related}

Wikipedia's official statement of vandalism defines it as
``a \textit{deliberate} attempt to compromise the integrity
of Wikipedia.''\footnote{
\url{http://en.wikipedia.org/wiki/Wikipedia:Vandalism}}
It is, of course, impossible to know the motivations of individuals,
so this definition relies on human intelligence to determine
vandalism on a case-by-case basis --- that is, ``I know it
when I see it,''\footnote{Justice Potter Stewart in
\underline{Jacobellis v. Ohio}, 378 U.S. 184 (1964)}
but there is no precise definition.
Some researchers have undertaken the task of more formally defining a
taxonomy of vandalism~\cite{Viegas2004,Priedhorsky2007,Chin2010},
but nearly all research on vandalism detection uses one of a small
number of (convenient) definitions for purposes of obtaining an
annotated corpus: \textbf{manual annotation} uses human intelligence
to infer the intentions of the
editor~\cite{Potthast2008,Chin2010,West2010,Potthast2010a},
\textbf{reverts} are notations by the community when it feels that
vandalism has taken place~\cite{Smets2008,Itakura2009,Belani2010},
\textbf{rollbacks} are disapprovals by Wikipedia
Administrators~\cite{West2010},
and \textbf{edit quality} generalizes the idea of measuring the
sentiment of the community~\cite{Adler2007,Druck2008}.
There is an obvious variation from \textit{manual} to
\textit{automatic} annotation in these choices,
but there is another difference between them:
external judgement from outside the Wikipedia community,
internal explicit judgement from within the community,
and internal implicit judgement based on actions by the community.
Ultimately, it is the community itself which decides what is
vandalism (\eg observe the stark contrast between the communities of
Slashdot\footnote{\url{http://slashdot.org}} and
Hacker News\footnote{\url{http://news.ycombinator.com}}),
and this community standard is likely to change over time
(often described as the ``signal-to-noise'' ratio of the community;
examples of changing communities include USENET and Slashdot).
This argues strongly in favor of automated methods for measuring
the reaction of the community, and highlights the idea that vandalism
detection is a specialized form of trying to measure the ``noise'' in
a community.

The earliest attempts at vandalism detection within the Wikipedia come
directly from the user community, and try to encode a human intuition
of vandalism detection into an expert system (some examples
include~\cite{wiki:AntiVandalBot,wiki:MartinBot,wiki:ClueBot,Carter2007}).
The largest disadvantage to this class of solutions is that building
an expert system requires extensive human labor to produce the manual
annotation and analysis required to derive custom rules.
Primarily, the rules developed are based on features of the actual
content of the edit rather than on metadata (\eg an edit containing
profanity is indicative of vandalism).

The idea that the content reveals the intent of the author is a natural
one, and has been investigated by several different research groups
(\eg~\cite{Potthast2008,Smets2008,Druck2008,Itakura2009,Chin2010}).
Casting the problem as a binary classification problem to be solved by
machine learning, Potthast~\etal~\cite{Potthast2008} manually identify
and inspect 301~incidents of vandalism to generate a feature set based
on metadata and content, and build a classifier using
logistic regression.
Smets~\etal~\cite{Smets2008} applies the ``naive bayes'' machine
learning technique to a bag-of-words model of the edit text.
Chin~\etal~\cite{Chin2010} delve deeper into the field of
natural language processing by constructing statistical language
models of an article from its revision history.
(On the topic of manual annotation, they also describe how supervised
active learning can help the training process by requesting
annotations for examples which will make a significant difference to
the algorithm.)

A different way of looking at the content approach is the
realization that appropriate content somehow ``belongs together,'' and
one way to measure that is through compression of the successive
revisions of an article~\cite{Smets2008,Itakura2009}.
If inappropriate content is added to the article, then the compression
level is lower than it would be for text which is similar to text
already in the article.
This is much more powerful than the bag-of-words model, because
phrases are significant and lead to better compression; nonsensical
sentences that include some key words will not compress as well.
A significant drawback of these compression techniques is that they
require manipulation of the content of a large number of revisions
from the article being edited.

Content-based analysis has the burden of having to
inspect potentially large edits, but the alternative is to depend
on the paucity of information available in the metadata ---
many previous works have some small dependence on metadata
features~\cite{Potthast2008,Druck2008,Belani2010}, but only
as far as it encoded some aspect of human intuition about vandalism.
Drawing inspiration from other areas of research,
West~\etal~\cite{West2010} published astonishing results
based entirely on metadata (some of which are processed into
\textit{reputations}) that indicate there is more relatedness between
vandals than is readily apparent to the human eye.
One particularly interesting result was that using IP geolocation
to cluster users led to better predictions.

The first systematic review and organization of features appears
by Potthast~\etal~\cite{Potthast2010b} as part of the competition
associated with the PAN~2010 Workshop on vandalism
detection.
Belani~\cite{Belani2010} includes several metrics for evaluating
predictors, and Potthast~\etal take up the discussion with a thorough
comparison of nine competitors using both the area under the
precision-recall curve and the area under the receiver operating
characteristic curve.
Potthast~\etal conclude their analysis by building a meta-classifier
based on the nine entries and discover that the result performs
significantly better than any single entry.

User reputation systems~\cite{Zeng2006,WikiMTWtrust06,Adler2007}
have been proposed as an underlying technology for
vandalism prevention or detection, and the second place entry in
the PAN~2010 competition was a system based on the
WikiTrust project~\cite{Adler2010b}.
In that entry, the WikiTrust user reputation system was not
directly used due to not having a historical record
of the reputation values.
The work presented in this chapter updates the results of~\cite{Adler2010b} by
tracking the historical user reputation values and using that
as an additional feature to the machine learning algorithm.

The winner of the PAN~2010 competition, by a notable margin, was
an entry by Mola-Velasco~\cite{Mola2010} that extended the features
originally proposed by Potthast~\etal~\cite{Potthast2008}.
This entry was composed of 21~features (the largest in the
competition) that comprehensively model the content of the edit,
including features that rated use of language, formatting of text,
compressibility with earlier text, spelling,
and the size of the edit.

The combining of features used by Mola-Velasco~\cite{Mola2010},
WikiTrust~\cite{Adler2010b} and West~\cite{West2010}
is explored in~\cite{Adler2011a}.
That work improves on earlier results, and categorizes features
according to the difficulty of analysis.


