\chapter{Contribution Quality}

\section{Introduction}

\subsection{19-Mar-2011}

The basis for this chapter was developed in the original
WikiTrust paper, and then expanded in the AuthorContrib
paper.

Although the AuthorContrib paper shows that the different
proposed formulas can be used to partition the authors
(and thus are useful as features to a machine learning algorithm),
there is no attempt to perform any kind of evaluation.
In part, this is because we believe that each formula calculates
something different; another motivation was that there was
no benchmark of any kind to compare against.
The publication of the PAN2010 corpus, which annotates a set
of edits as either ``vandalism'' or ``not vandalism,''
now gives a benchmark which we can analyze the different
formulas against.


\section{Edit Distance}

\subsection{Feb-2010}

While speaking with Gary Wolf about Quantified Self materials,
we got to discussing wikipedia.
He told me a story about an article
(``The Battle of Genine'')
which he heard was being quietly manipulated by the Israelis.

\subsection{14-Apr-2010}

Erica has sent me a link to an article which has many edit
wars in it.

\url{http://www.doublex.com/blog/xxfactor/bieber-fans-vs-wikipedia}

\subsection{8-May-2011}

See note in diff chapter; we're using only the standard
two revision difference, because of performance.

\section{Edit Quality}

\subsection{19-Mar-2011}

The experiment that I'm proposing is to compute the edit longevity
and text longevity for each of the PAN2010 edits.
Then, for each metric, construct two histograms:
\begin{enumerate}
\item A histogram showing how many vandalism edits fall within
    each quality bucket.  Essentially, what is the distribution
    of vandalism over quality.
\item A histogram showing how many non-vandalism edits fall within
    each quality bucket.  Essentially, what is the distribution
    of good edits over quality.
\end{enumerate}
This is very like some histograms that we already used in
the AuthorContrib paper, but this time we have a ``golden annotation''
dataset.
I would also like to somehow compute a correlation, but that
feels odd since vandalism is a binary categorization;
maybe I can show the inverse histograms and compute the
average quality?

\subsection{6-Apr-2011}

I started writing code in the \program{projects/edit-quality} directory,
which is able to pull out all the PAN2010 revisions and their classes.

I am stil trying to figure out how to compute the text longevity
and edit longevity for particular revisions.
I have the output from analyzing the whole enwiki, which includes
the \texttt{stats} directory with its \textbf{EditLife}
and \textbf{EditInc} entires.
Are these useful for computing the longevities?

By inspecting the code, I find that we compute the text longevity
as \textbf{text\_life\_text\_decay} in file \file{wikidata.ml}.
And generating the \textbf{TextLife} entries seems to be off by default.

I'm not eager to reprocess the entire English Wikipedia yet again,
since this takes around 5~days.
And I also have this fear in the back of my head that the calculation
that I do in perl will not match the calculation we do in OCaml.
Perhaps the most efficient way to proceed write another script
which takes the PAN2010 revision IDs and then extracts those revisions
(and the following 20) from the dumps, to make a much smaller dump?

\subsection{16-Apr-2011}

I made a restricted dump.
I also wrote perl code to compute the longevities.
The problem I'm having now is that it takes about 30~seconds
to compute the text tracking for each revision, and over
5~minutes to compute the edit distances.
By my calculations, it would take over 22~days to do the
computations just for the PAN2010 revisions; but we actually
have many more revisions that we need to do the computation for.

Does this mean that the only solution is to use the OCaml code?
Can I distribute the work over multiple computers?
If I use the OCaml code, do I have to describe the zipping of
the diff lists?

Looking through the OCaml code, I see that the edit longevity
value for specific edits is calculated in
function \function{online\_eval\_oldest\_edit} of
file \file{reputation\_analysis.ml}.
This code does skip over equivalent authors,
and I want to not have anonymous users be considered equivalent.
The final value I want is the \textit{AvgSpecQ} field of
\textbf{EditLife} lines, where field \textit{rev0} matches
the revision that I care about.

Text longevity is computed when \textbf{TextLife} lines are read
into the system.
The \textbf{TextLife} lines are generated when the text analysis
option \textit{-do\_text} is included.

\begin{verbatim}
$ cd $(WIKITRUST)/util
$ 7za a /raid/thumper/pan2010dump.7z /raid/thumper/pan2010dump.xml
$ ./batch_process.py --cmd_dir ../analysis --dir output \
    --do_split --do_compute_stats --do_sort_stats --do_compute_rep \
    /raid/thumper/pan2010dump.7z
\end{verbatim}

\subsection{17-Apr-2011}

I wrote another program, \program{extract-ratingsFrepfile.pl}
to grab the text and edit longevities computed by the OCaml code.
Only about 18,000 edits get rated, which I think must be related
to the fact that the robust code doesn't want only the previous
revision when computing edit longevity.
I'm not sure why text longevity isn't fully computed.

Also, I need to re-implement the less-optimized versions
of text tracking and our diff algorithm.

Out of the results that were computed by the system,
the performance was pretty good --- as shown in
Table~\ref{table:robustlongevities}.

\begin{table}[htbp]
\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    & Revisions & ROC (AUC) & Average Precision \\
    \hline
    Text longevity & 18416 & 0.84914 & 0.38763 \\
    Edit longevity & 14521 & 0.91822 & 0.48467 \\
    \hline
    \end{tabular}
\end{center}
\caption{Performance of text and edit longevity at predicting
    vandalism in the PAN2010 corpus, using latest WikiTrust code
    that includes robustness.}
\label{table:robustlongevities}
\end{table}

\subsection{18-Apr-2011}

I tried to load the combined data into Weka to get an idea
of how well the two measures complement each other, and
I discovered that the edit longevity measure has some pretty
far out values.
I have compiled a list of some revision IDs and edit longevity
values that I think are suspicious and might need more investigation
in Table~\ref{table:badeditlong}.

\begin{table}[htbp]
\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Revision & Edit Longevity \\
    \hline
    326555367 & 1.875780 \\
    326794976 & -4.761800 \\
    326814933 & 5.223380 \\
    326828633 & -13.299140 \\
    326929173 & -3.166670 \\
    326935330 & -2.481480 \\
    326943048 & -6.000000 \\
    326943009 & 3.091370 \\
    327013725 & -4.444440 \\
    327017804 & -5.768310 \\
    327114649 & 32.036570 \\
    327178504 & -8.386600 \\
    327270892 & -82.386420 \\
    327613446 & -645.000000 \\
    327794162 & 610.598730 \\
    327954412 & 776.837560 \\
    \hline
    \end{tabular}
\end{center}
\caption{Suspicious edit longevity values computed using the
    current robust+zipping code in WikiTrust.}
\label{table:badeditlong}
\end{table}

The complete list of suspicious edit longevity values is extremely
long, and while most values are only slightly above 1.0, you can
see that some values are ridiculously large.

At any rate, I capped all the values to the proper range and
then ran the data through Weka.
I used a cost sensitive classifier, and put a cost of 10 on the vandalism
class.
For the actual classifier, I used an ADTree like we did in our
original PAN submittion.
This gives an ROC of 0.929,
and the model that's built primarily depends on edit longevity.
\begin{verbatim}
: 0.081
|  (1)editlong < 0.541: -1.055
|  |  (4)editlong < 0.215: 0.072
|  |  |  (6)textlong < 0: -0.749
|  |  |  |  (8)editlong < 0.023: 0.143
|  |  |  |  |  (9)editlong < 0.022: -0.339
|  |  |  |  |  (9)editlong >= 0.022: 1.854
|  |  |  |  (8)editlong >= 0.023: -1.625
|  |  |  (6)textlong >= 0: 0.064
|  |  |  (10)editlong < 0.195: 0.01
|  |  |  (10)editlong >= 0.195: -1.614
|  |  (4)editlong >= 0.215: 0.479
|  (1)editlong >= 0.541: 0.98
|  |  (3)editlong < 0.823: -0.731
|  |  |  (7)editlong < 0.643: -0.18
|  |  |  (7)editlong >= 0.643: 0.242
|  |  (3)editlong >= 0.823: -0.011
|  |  (5)textlong < 0.535: -0.384
|  |  (5)textlong >= 0.535: 0.087
|  (2)textlong < 0.038: -0.541
|  (2)textlong >= 0.038: 0.241
Legend: -ve = regular, +ve = vandalism
\end{verbatim}
Only the last rule has any real value, and it basically
says that if text longevity thinks that an edit is
very likely to be regular, then that's kinda true.
This is a pretty strong indictment for why text longevity
isn't very useful when you also have edit longevity.

\subsection{20-Apr-2011}

I wanted to dig into one of these bad values, so I chose
revision 327794162, and added some debugging.
What I found is that the edit distance we compute between
this revision and the revision
before\footnote{\url{http://en.wikipedia.org/w/index.php?title=Geotagging&diff=next&oldid=327469253}}
is quite low: 0.000765.
This leads to very large edit longevity values because of the
imprecision in our edit distance.

We don't compute any edit longevity when the edit distance to
the revision before is 0.0.  I wonder if we should set the edit
longevity to 0, or to 1, in these cases.
If we were to ``normalize'' the values after we do the calculation,
we arrive at an edit longevity of 1.0.
When there's no difference, it's true that this is perfectly
preserved in the future, but it's a very weird situation.
In the case where the value is close to zero, having
a longevity of 1.0 makes more sense to me, since the edit
is actually preserved.

The fractional distance is because the text moved within
the document, and the fraction represents how much of the
document the block moved across.

I briefly thought that the number of judges I selected
wasn't being respected.
Looking at my counter-example, I found that the real problem was
that the same author had edited the article a few times afterwards,
and then the dump ends.

There is also still the problem that longevities aren't being
calculated for every PAN revision.
One revision which has this problem is 327905056.
This revision was missing both text and edit longevities,
due to the revision being edited by the same author.
For the PAN corpus, we actually don't care about filtering the
previous revision, since the manual annotation doesn't check that.

Revision 328325509 is missing just the edit longevity.

\subsection{24-Apr-2011}

There are still plenty of edits missing scores.

Revision 327905056 is missing both scores.
Inspecting the dump, I find that the next ten revisions are all
by the same author, so they did not match the requirement for
no self-judgement.
I've already modified the code to not care that the immediately
preceding revision is by the same author.
To more closely match the our normal definition of revisions,
I think the best solution is to do our normal filtering on
the following revisions.

This filtering idea makes me realize that we don't actually compute
scores for every edit, only the filtered ones.
But there's something funny about it: in my definitions, I say
that we take the scores of the following ten filtered revisions,
but the reality is that we considering the next ten revisions
and then filter after the fact.
Wait!
Actually, we filter all revisions as they come in, but I had
commented that code out in order to try to get the preceding
revision to show up.


Revision 328038014 is missing only the edit longevity.
This turns out to be because the edit distance to the previous
revision is zero.
I went into Wikipedia to check it out, and the diff does say there's
a difference in one paragraph, but I checked each work and can't
detect the actual difference.
My best guess is that it's an accidental change in the whitespace.
What does this mean for predicting vandalism?
I'd say that the safe prediction for a zero distance change is
to say that it's not vandalism.

\subsection{27-Apr-2011}

After three days of running, I was finally able to build a new dump
that filters the revisions after one of the PAN2010 edits.
Weirdly, both 327905056 and 328038014 are still showing up in the same way,
as if nothing had changed.

Revision 328325509 is no longer a problem, so some change has happened.

Revision 327905056 is not listed because it has no other editors
afterwards besides the same author.

Revision 328038014 is still the zero edit distance issue.
I just tried assuming that the edit must then be good,
and got an ROC of 91.661\%, which is not substantially different
(but is a little lower).

Based on the revised filtering of revisions, edit longevity is computed
for 27,730 edits, and text longevity is computed for 28,453 edits.

\subsection{7-May-2011}

Christina Wodtke told me the story of Richard~III and how
he's portrayed as a villian but actually wasn't.
After reading through
Wikipedia,~\footnote{\url{http://en.wikipedia.org/wiki/Princes_in_the_Tower}}
I am unconvinced about the claim that he is not a ``villian.''
Too bad, because this would have been a good example of how
our consentual reality defines ``truth'' rather than actual facts.

\subsection{8-May-2011}

The Ocaml text tracking code is essentially a reimplementation
of the regular diff code.
The problem with this is that it means that I can't easily substitute
in other diff algorithms.
The one parameter I can easily adjust is the match quality function.

The way to do unit testing on the \file{chdiff.ml} module is:
\begin{verbatim}
make chdiff
./chdiff
\end{verbatim}

Found another point I didn't realize: there is no minimum match length
when computing edit distance.
That restriction appears to be limited to text tracking.
There's a caveat to that, though; because the index is built
by word pairs, there is actually a minimum match length of 2.
Confusing!

\section{Evaluation}

\subsection{9-May-2011}

I was initially recording my evaluations directly into my thesis
document, but there are too many experiments that I am running
and I think some of them won't be useful.
So I wanted to organize all the data here first, and then
figure out what parts to actually include in my thesis.

\subsubsection{Diff Algorithms}

\begin{description}
\item[diff0] This is the currently live diff algorithm.
	This first checks for matches at the front and end of
	the text, and eliminates those so that they don't go
	through the greedy matching phase.
\item[diff1] This is the core edit diff algorithm, including
	many of our optimizations.
\item[diff2] This is my version of our faster text difference algorithm.
    I originally tried the basic version, but brought the machine to
    its knees because of the memory requirements.
    The faster text difference also isn't sufficient; I had to
    use word pairs in the index made of the target string,
    and ignore any word pairs that occur more than 50~times
    in the target string.  Even with these optimizations, I
    see a 23GB process running, which now seems to have brought
    the machine to its knees again.
    To finally get the memory usage low enough to be practical,
    I had to use Luca's trick of just checking the previous
    matches to see if an entry was already in the heap,
    rather than keeping a big hashtable of every match that
    had been made so far.
\item[diff3] Same as diff2, but uses the header/trailer matching
    optimization.
\item[diff4] Same as diff3, but using the slow version of heap
    generation.  I expect the performance is this version to be
    the same as diff3, since the fast and slow heap generation
    should produce exactly the same results.
\item[diff5] Same as diff3, but the heap building step uses the older
    method of having a hashtable to track the previous matches.
    I expect the same results as diff3 and diff4, but more memory usage.
    I just wanted to verify that the results are the same when
    using the optimization of checking against the list of previous
    matches.
\end{description}

\subsubsection{Match Quality Functions}

\begin{description}
\item[mq0] This is the currently live match quality function.
	\begin{equation}
	(-k, -c, abs(\frac{i1}{l1} - \frac{i2}{l2}))
	\end{equation}

\item[mq1] This function was proposed in our original WikiTrust paper.
\begin{equation}
(-k, -c, 0.3  \cdot abs(\frac{i1}{l1} - \frac{i2}{l2}) - \frac{k}{min(l1,l2)})
\end{equation}

\item[mq2] This variation of our original proposal checks to see if the
	sign matters for the chunk index.
\begin{equation}
(-k, c, 0.3  \cdot abs(\frac{i1}{l1} - \frac{i2}{l2}) - \frac{k}{min(l1,l2)})
\end{equation}

\item[mq3] This variation checks to see if the sign matters.
	\begin{equation}
	(-k, c, abs(\frac{i1}{l1} - \frac{i2}{l2}))
	\end{equation}

\item[mq4] This variation of our original proposal checks to see
	we need to select longest match first, rather than letting
	quality function do the work.
\begin{equation}
(0, -c, 0.3  \cdot abs(\frac{i1}{l1} - \frac{i2}{l2}) - \frac{k}{min(l1,l2)})
\end{equation}

\end{description}


\subsubsection{Edit Distance}

\begin{description}
\item{\textbf{I+D+M}} --- The most straight-forward definition of an edit
        distance between revisions \version{m} and \version{n} would be one
        similar to the Levenshtein distance where
        the total number of operations is counted:
        \begin{equation}
            \dist{}{m,n} = I_{tot}(m,n) + D_{tot}(m,n) + M_{tot}(m,n)
        \label{eq:dist-idm}
        \end{equation}
\item{\textbf{I+D}} --- The \textbf{Move} operation just indicates
        matching text between the source and target revisions,
        so it does not usually amount to any work by the editor.
        \begin{equation}
            \dist{}{m,n} = I_{tot}(m,n) + D_{tot}(m,n)
        \label{eq:dist-id}
        \end{equation}
\item{\textbf{I+D-$\frac{1}{2}$}} --- The original definition of edit
        distance used in the WikiTrust project tried to account for
        \textit{replacements} by observing that some insertions and
        deletions are actually paired together and represent a
        replacement which we attempt to correct for.
\item{\textbf{WikiTrust 2009}} --- A further variation that our
        project experimented with reincorporates the number of
        move operations, but scaled by the number of words in
        the target string.
        \mynote{Fix numbering of this equation.}
        \begin{align}
            \dist{}{m,n} =& I_{tot}(m,n) + D_{tot}(m,n) \\
                & - \frac{1}{2}\min(I_{tot}(m,n), D_{tot}(m,n))
                + \frac{M_{tot}(m,n)}{| \words{\version{n}} |}
        \end{align}
\item{\textbf{replacements}} --- The currently live version of
        WikiTrust uses a more elaborate definition of edit distance
        which attempts to compute from the edit script
        which insertions and deletions
        are paired replacements.
        The exact details are beyond the scope of this work to present,
        but we include the performance evaluation for comparison purposes.
\end{description}

\subsubsection{Results}

In measuring the running time, I used the shell built-in
\texttt{time} command, which reports the elapsed time as well
as the user and system times.
Since this is running on an 8-core CPU, the real-time is about
one eighth as small as it would normally be.
I decided to sum the user and system times, and use that as
the running time.

Another issue in the measuring of running time is that
edit longevity and text longevity are calculated together.
Although the text longevity is affected by changes in the
match quality function, it isn't affected by the diff and
edit distance parameters.
My thinking is that this means that text longevity is part
of a fixed overhead of calculation, and that the trend
of running times is still useful to examine.

\input{part_Analysis/quality-table-textlong}

\input{part_Analysis/quality-table-editlong}

\subsection{13-May-2011}

The results on text longevity
(presented in Table~\ref{tab:textlongeval})
are interesting, because they are so consistent.
The match qualities are apparently not as different
as I expected.

The most startling thing for me is that mq2 and mq3
perform worse than mq0 and mq1.
The only difference between these two sets of functions
is that mq2/mq3 prefer to match against deleted chunks from
more recent revisions; mq0/mq1 prefer to match older deleted chunks.
I expected that more recently deleted chunks would do better
because they were more likely to be seen by the current editor;
also, I assumed that chunk~0 (the previous revision) would always
be the preferred match.

Since the function \texttt{make\_survival\_index} creates
an index based on work triples, I suspect that the performance
is nearly identical because deleted chunks are rarely restored
and rarely have so many words in a row.

The fact that there's any difference at all implies to me
that there is deleted text in some old revision that is
also found in chunk~0 (the immediately previous revision)\ldots
\textbf{and} that these are assigned to different authors?
Assigning credit to the older revision predicts vandalism
better than assigning credit to the newer revision.
That seems weird.
Maybe the right way to think of this is that the newer revision
already received credit, but the one that is still a deleted
chunk hasn't received any credit yet for being good;
and this difference is enough to push the revision from
being bad to good.

Edit longevity makes an even bolder statement: all the
match qualities we proposed are the same.
Well, there's no difference from mq2/mq3 because we only
take differences between two revisions, and there are never
any deleted chunks.

\subsection{14-May-2011}

On text longevity, since the match quality function matters so little,
does this mean that length of match is the dominating factor?
Can we try a match quality function that \textit{only} uses
length of match?

\subsection{18-May-2011}

Hung the machine over the weekend, and just hung it again.
Problem is that our basic and faster text diff algorithms
still use too much memory.
I just tried modifying the system to use word pairs and
a maximum match count, but the system choked up again.
Before the choke, I see a 23GB process running.

I should have put \texttt{ulimit} commands into the
experiment script, to prevent this from happening.

\subsection{31-May-2011}

It's been a while since I worked on this part of the project.
I have a script,
\texttt{projects/edit-quality/run-expt.sh} which runs all
the steps necessary for this experiment.
My memory is that I am still running out of memory while
trying to run \texttt{diff=2}.

\subsection{5-Jun-2011}

I eventually figured out that even the live code runs out of memory
at the limit I had set.
There seems to be a page (on Vampires?) that has a large number
of matches and creates a huge heap.
Luca's idea for removing the matching beginning and end of
an article goes a long way to reducing the number of matches
to consider.

One odd difference between the runs is that the CPU time
has changed a bit for the live system, even though I didn't
actually change that part of the code.
The timing is off by enough that I'm suspicious, but maybe the
difference comes from the fact that I'm using a smaller number
of CPUs now.

\subsection{12-Jun-2011}

Updated the performance tables for both text and edit longevity.
Realized that I hadn't fixed diff4 to use the slow heap method,
so I restarted that last night; and killed Gaston again, it seems.
My quick pass over the tables shows:
\begin{enumerate}
\item The diff algorithm doesn't matter to text longevity, but
    the match quality does.  The ranking of match quality is:
    mq4 is best, $1 = 0$, $2 = 3$.  All of them are relatively
    close, however.
\item In edit longevity, the match quality doesn't matter, but
    the difference algorithm does.
\item In edit longevity, using precise differences slows down
    the performance by more than a factor of three,
    but the performance is slightly ($< 1\%$ for both
    precision and recall) worse.
\item In edit longevity, the edit distance can make a big
    difference in performance.  The difference algorithm itself
    makes a small difference.
\item In edit longevity, the edit distance also makes a big difference
    in how many bad (and total) triangles there are.
\end{enumerate}

\section{Reverse Triangle Inequality}

\subsection{12-Jun-2011}

Luca has asked, ``why does the triangle inequality not hold?''

