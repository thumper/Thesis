\chapter{Contribution Quality}

\section{Introduction}

\subsection{19-Mar-2011}

The basis for this chapter was developed in the original
WikiTrust paper, and then expanded in the AuthorContrib
paper.

Although the AuthorContrib paper shows that the different
proposed formulas can be used to partition the authors
(and thus are useful as features to a machine learning algorithm),
there is no attempt to perform any kind of evaluation.
In part, this is because we believe that each formula calculates
something different; another motivation was that there was
no benchmark of any kind to compare against.
The publication of the PAN2010 corpus, which annotates a set
of edits as either ``vandalism'' or ``not vandalism,''
now gives a benchmark which we can analyze the different
formulas against.


\section{Edit Distance}

\subsection{Feb-2010}

While speaking with Gary Wolf about Quantified Self materials,
we got to discussing wikipedia.
He told me a story about an article
(``The Battle of Genine'')
which he heard was being quietly manipulated by the Israelis.

\subsection{14-Apr-2010}

Erica has sent me a link to an article which has many edit
wars in it.

\url{http://www.doublex.com/blog/xxfactor/bieber-fans-vs-wikipedia}

\subsection{8-May-2011}

See note in diff chapter; we're using only the standard
two revision difference, because of performance.

\section{Edit Quality}

\subsection{19-Mar-2011}

The experiment that I'm proposing is to compute the edit longevity
and text longevity for each of the PAN2010 edits.
Then, for each metric, construct two histograms:
\begin{enumerate}
\item A histogram showing how many vandalism edits fall within
    each quality bucket.  Essentially, what is the distribution
    of vandalism over quality.
\item A histogram showing how many non-vandalism edits fall within
    each quality bucket.  Essentially, what is the distribution
    of good edits over quality.
\end{enumerate}
This is very like some histograms that we already used in
the AuthorContrib paper, but this time we have a ``golden annotation''
dataset.
I would also like to somehow compute a correlation, but that
feels odd since vandalism is a binary categorization;
maybe I can show the inverse histograms and compute the
average quality?

\subsection{6-Apr-2011}

I started writing code in the \program{projects/edit-quality} directory,
which is able to pull out all the PAN2010 revisions and their classes.

I am stil trying to figure out how to compute the text longevity
and edit longevity for particular revisions.
I have the output from analyzing the whole enwiki, which includes
the \texttt{stats} directory with its \textbf{EditLife}
and \textbf{EditInc} entires.
Are these useful for computing the longevities?

By inspecting the code, I find that we compute the text longevity
as \textbf{text\_life\_text\_decay} in file \file{wikidata.ml}.
And generating the \textbf{TextLife} entries seems to be off by default.

I'm not eager to reprocess the entire English Wikipedia yet again,
since this takes around 5~days.
And I also have this fear in the back of my head that the calculation
that I do in perl will not match the calculation we do in OCaml.
Perhaps the most efficient way to proceed write another script
which takes the PAN2010 revision IDs and then extracts those revisions
(and the following 20) from the dumps, to make a much smaller dump?

\subsection{16-Apr-2011}

I made a restricted dump.
I also wrote perl code to compute the longevities.
The problem I'm having now is that it takes about 30~seconds
to compute the text tracking for each revision, and over
5~minutes to compute the edit distances.
By my calculations, it would take over 22~days to do the
computations just for the PAN2010 revisions; but we actually
have many more revisions that we need to do the computation for.

Does this mean that the only solution is to use the OCaml code?
Can I distribute the work over multiple computers?
If I use the OCaml code, do I have to describe the zipping of
the diff lists?

Looking through the OCaml code, I see that the edit longevity
value for specific edits is calculated in
function \function{online\_eval\_oldest\_edit} of
file \file{reputation\_analysis.ml}.
This code does skip over equivalent authors,
and I want to not have anonymous users be considered equivalent.
The final value I want is the \textit{AvgSpecQ} field of
\textbf{EditLife} lines, where field \textit{rev0} matches
the revision that I care about.

Text longevity is computed when \textbf{TextLife} lines are read
into the system.
The \textbf{TextLife} lines are generated when the text analysis
option \textit{-do\_text} is included.

\begin{verbatim}
$ cd $(WIKITRUST)/util
$ 7za a /raid/thumper/pan2010dump.7z /raid/thumper/pan2010dump.xml
$ ./batch_process.py --cmd_dir ../analysis --dir output \
    --do_split --do_compute_stats --do_sort_stats --do_compute_rep \
    /raid/thumper/pan2010dump.7z
\end{verbatim}

\subsection{17-Apr-2011}

I wrote another program, \program{extract-ratingsFrepfile.pl}
to grab the text and edit longevities computed by the OCaml code.
Only about 18,000 edits get rated, which I think must be related
to the fact that the robust code doesn't want only the previous
revision when computing edit longevity.
I'm not sure why text longevity isn't fully computed.

Also, I need to re-implement the less-optimized versions
of text tracking and our diff algorithm.

Out of the results that were computed by the system,
the performance was pretty good --- as shown in
Table~\ref{table:robustlongevities}.

\begin{table}[htbp]
\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    & Revisions & ROC (AUC) & Average Precision \\
    \hline
    Text longevity & 18416 & 0.84914 & 0.38763 \\
    Edit longevity & 14521 & 0.91822 & 0.48467 \\
    \hline
    \end{tabular}
\end{center}
\caption{Performance of text and edit longevity at predicting
    vandalism in the PAN2010 corpus, using latest WikiTrust code
    that includes robustness.}
\label{table:robustlongevities}
\end{table}

\subsection{18-Apr-2011}

I tried to load the combined data into Weka to get an idea
of how well the two measures complement each other, and
I discovered that the edit longevity measure has some pretty
far out values.
I have compiled a list of some revision IDs and edit longevity
values that I think are suspicious and might need more investigation
in Table~\ref{table:badeditlong}.

\begin{table}[htbp]
\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Revision & Edit Longevity \\
    \hline
    326555367 & 1.875780 \\
    326794976 & -4.761800 \\
    326814933 & 5.223380 \\
    326828633 & -13.299140 \\
    326929173 & -3.166670 \\
    326935330 & -2.481480 \\
    326943048 & -6.000000 \\
    326943009 & 3.091370 \\
    327013725 & -4.444440 \\
    327017804 & -5.768310 \\
    327114649 & 32.036570 \\
    327178504 & -8.386600 \\
    327270892 & -82.386420 \\
    327613446 & -645.000000 \\
    327794162 & 610.598730 \\
    327954412 & 776.837560 \\
    \hline
    \end{tabular}
\end{center}
\caption{Suspicious edit longevity values computed using the
    current robust+zipping code in WikiTrust.}
\label{table:badeditlong}
\end{table}

The complete list of suspicious edit longevity values is extremely
long, and while most values are only slightly above 1.0, you can
see that some values are ridiculously large.

At any rate, I capped all the values to the proper range and
then ran the data through Weka.
I used a cost sensitive classifier, and put a cost of 10 on the vandalism
class.
For the actual classifier, I used an ADTree like we did in our
original PAN submittion.
This gives an ROC of 0.929,
and the model that's built primarily depends on edit longevity.
\begin{verbatim}
: 0.081
|  (1)editlong < 0.541: -1.055
|  |  (4)editlong < 0.215: 0.072
|  |  |  (6)textlong < 0: -0.749
|  |  |  |  (8)editlong < 0.023: 0.143
|  |  |  |  |  (9)editlong < 0.022: -0.339
|  |  |  |  |  (9)editlong >= 0.022: 1.854
|  |  |  |  (8)editlong >= 0.023: -1.625
|  |  |  (6)textlong >= 0: 0.064
|  |  |  (10)editlong < 0.195: 0.01
|  |  |  (10)editlong >= 0.195: -1.614
|  |  (4)editlong >= 0.215: 0.479
|  (1)editlong >= 0.541: 0.98
|  |  (3)editlong < 0.823: -0.731
|  |  |  (7)editlong < 0.643: -0.18
|  |  |  (7)editlong >= 0.643: 0.242
|  |  (3)editlong >= 0.823: -0.011
|  |  (5)textlong < 0.535: -0.384
|  |  (5)textlong >= 0.535: 0.087
|  (2)textlong < 0.038: -0.541
|  (2)textlong >= 0.038: 0.241
Legend: -ve = regular, +ve = vandalism
\end{verbatim}
Only the last rule has any real value, and it basically
says that if text longevity thinks that an edit is
very likely to be regular, then that's kinda true.
This is a pretty strong indictment for why text longevity
isn't very useful when you also have edit longevity.

\subsection{20-Apr-2011}

I wanted to dig into one of these bad values, so I chose
revision 327794162, and added some debugging.
What I found is that the edit distance we compute between
this revision and the revision
before\footnote{\url{http://en.wikipedia.org/w/index.php?title=Geotagging&diff=next&oldid=327469253}}
is quite low: 0.000765.
This leads to very large edit longevity values because of the
imprecision in our edit distance.

We don't compute any edit longevity when the edit distance to
the revision before is 0.0.  I wonder if we should set the edit
longevity to 0, or to 1, in these cases.
If we were to ``normalize'' the values after we do the calculation,
we arrive at an edit longevity of 1.0.
When there's no difference, it's true that this is perfectly
preserved in the future, but it's a very weird situation.
In the case where the value is close to zero, having
a longevity of 1.0 makes more sense to me, since the edit
is actually preserved.

The fractional distance is because the text moved within
the document, and the fraction represents how much of the
document the block moved across.

I briefly thought that the number of judges I selected
wasn't being respected.
Looking at my counter-example, I found that the real problem was
that the same author had edited the article a few times afterwards,
and then the dump ends.

There is also still the problem that longevities aren't being
calculated for every PAN revision.
One revision which has this problem is 327905056.
This revision was missing both text and edit longevities,
due to the revision being edited by the same author.
For the PAN corpus, we actually don't care about filtering the
previous revision, since the manual annotation doesn't check that.

Revision 328325509 is missing just the edit longevity.

\subsection{24-Apr-2011}

There are still plenty of edits missing scores.

Revision 327905056 is missing both scores.
Inspecting the dump, I find that the next ten revisions are all
by the same author, so they did not match the requirement for
no self-judgement.
I've already modified the code to not care that the immediately
preceding revision is by the same author.
To more closely match the our normal definition of revisions,
I think the best solution is to do our normal filtering on
the following revisions.

This filtering idea makes me realize that we don't actually compute
scores for every edit, only the filtered ones.
But there's something funny about it: in my definitions, I say
that we take the scores of the following ten filtered revisions,
but the reality is that we considering the next ten revisions
and then filter after the fact.
Wait!
Actually, we filter all revisions as they come in, but I had
commented that code out in order to try to get the preceding
revision to show up.


Revision 328038014 is missing only the edit longevity.
This turns out to be because the edit distance to the previous
revision is zero.
I went into Wikipedia to check it out, and the diff does say there's
a difference in one paragraph, but I checked each work and can't
detect the actual difference.
My best guess is that it's an accidental change in the whitespace.
What does this mean for predicting vandalism?
I'd say that the safe prediction for a zero distance change is
to say that it's not vandalism.

\subsection{27-Apr-2011}

After three days of running, I was finally able to build a new dump
that filters the revisions after one of the PAN2010 edits.
Weirdly, both 327905056 and 328038014 are still showing up in the same way,
as if nothing had changed.

Revision 328325509 is no longer a problem, so some change has happened.

Revision 327905056 is not listed because it has no other editors
afterwards besides the same author.

Revision 328038014 is still the zero edit distance issue.
I just tried assuming that the edit must then be good,
and got an ROC of 91.661\%, which is not substantially different
(but is a little lower).

Based on the revised filtering of revisions, edit longevity is computed
for 27,730 edits, and text longevity is computed for 28,453 edits.

\subsection{7-May-2011}

Christina Wodtke told me the story of Richard~III and how
he's portrayed as a villian but actually wasn't.
After reading through
Wikipedia,~\footnote{\url{http://en.wikipedia.org/wiki/Princes_in_the_Tower}}
I am unconvinced about the claim that he is not a ``villian.''
Too bad, because this would have been a good example of how
our consentual reality defines ``truth'' rather than actual facts.

\subsection{8-May-2011}

The Ocaml text tracking code is essentially a reimplementation
of the regular diff code.
The problem with this is that it means that I can't easily substitute
in other diff algorithms.
The one parameter I can easily adjust is the match quality function.

The way to do unit testing on the \file{chdiff.ml} module is:
\begin{verbatim}
make chdiff
./chdiff
\end{verbatim}

Found another point I didn't realize: there is no minimum match length
when computing edit distance.
That restriction appears to be limited to text tracking.
There's a caveat to that, though; because the index is built
by word pairs, there is actually a minimum match length of 2.
Confusing!

\section{Evaluation}

\subsection{9-May-2011}

For comparison purposes, we compute the performance of edit longevity
using several different definitions of edit distance.
\begin{description}
\item{\textbf{I+D+M}} --- The most straight-forward definition of an edit
        distance between revisions \version{m} and \version{n} would be one
        similar to the Levenshtein distance where
        the total number of operations is counted:
        \begin{equation}
            \dist{}{m,n} = I_{tot}(m,n) + D_{tot}(m,n) + M_{tot}(m,n)
        \label{eq:dist-idm}
        \end{equation}
tem{\textbf{I+D}} --- The \textbf{Move} operation just indicates
        matching text between the source and target revisions,
        so it does not usually amount to any work by the editor.
        \begin{equation}
            \dist{}{m,n} = I_{tot}(m,n) + D_{tot}(m,n)
        \label{eq:dist-id}
        \end{equation}
\item{\textbf{I+D-$\frac{1}{2}$}} --- The original definition of edit
        distance used in the WikiTrust project tried to account for
        \textit{replacements} by observing that some insertions and
        deletions are actually paired together and represent a
        replacement which we attempt to correct for.
\item{\textbf{WikiTrust 2009}} --- A further variation that our
        project experimented with reincorporates the number of
        move operations, but scaled by the number of words in
        the target string.
        \mynote{Fix numbering of this equation.}
        \begin{align}
            \dist{}{m,n} =& I_{tot}(m,n) + D_{tot}(m,n) \\
                & - \frac{1}{2}\min(I_{tot}(m,n), D_{tot}(m,n))
                + \frac{M_{tot}(m,n)}{| \words{\version{n}} |}
        \end{align}
\item{\textbf{replacements}} --- The currently live version of
        WikiTrust uses a more elaborate definition of edit distance
        which attempts to compute from the edit script
        which insertions and deletions
        are paired replacements.
        The exact details are beyond the scope of this work to present,
        but we include the performance evaluation for comparison purposes.
\end{description}


\begin{sidewaystable}[!tp]
  \begin{center}
    \begin{tabular}{|c|c|c||c|c|c|}
\hline
Diff & Match Quality & Edit Distance & Num. Revisions
	& ROC AUC & Mean Precision \\
\hline
\hline
greedy & initial & live & xx & xx & xx \\
live & initial & live & 28,453 & 85.949\% & 29.235\% \\
\hline
    \end{tabular}
  \end{center}
\end{sidewaystable}

