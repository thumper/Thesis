\chapter{Vandalism Detection}

\section{Introduction}

\subsection{11-Apr-2010}

Both Ian and Luca have suggested that we enter the vandalism competition
part of the PAN~2010 LAB\footnote{\url{http://pan.webis.de}}.
While considering Ian's observation of the recent spike in
interest of our work, it occurred to me that the spike could
be because of this competition.

\section{Combining Signals}

\subsection{28-Sep-2010}

While in Poland for Wikimania, I spoke to several people about our
performance as a vandalism detection tool in the PAN~2010 competition.
Although there was some interest, most people referred me to (and were
excited about) the work of Andrew West presented at WikiSym a few days before.
Andrew's work uses a handful of signals (\eg some NLP, IP address, time of
day) and gets good results (he would have taken second place in
PAN~2010 if he had entered).
The most exciting thing about his work, however, is that he has a tool
(\program{STiki}) which is able to present his vandalism results to
the user so that they can inspect the edit and trigger a revert if
necessary --- and he collects the data about how his users ``voted''
on each edit.
This provides him with the opportunity to continue to build an
annotated corpus of edits\footnote{The problem with the current system
is that it only collects a single vote for each edit, so that the
annotations might not be very reliable.}, so that he can re-run the
machine learning algorithms.

With Luca's permission, I approached Andrew about a possible
collaboration where we try to combine the signals from our respective
solutions and see if we can get better performance.
Andrew hadn't entered PAN~2010 because his data didn't go back that
far, so we experimented on a corpus that Andrew had collected from
his tool.
First, we discovered that the \function{ADTree} algorithm under
\package{Weka} out-performed Andrew's own \function{SVM}
implementation.
We also saw that \wikitrust seemed to perform a bit better than
\package{STiki}.
And finally, we saw a noticeable improvement when we combined our
signals using \function{ADTree}.

Andrew got really excited by these results, and generated the data
necessary for the PAN~2010 corpus.
On this corpus, our combined signals was able to score an AUC of 95-97\%!

While working on this task, I was stymied by some \wikitrust bugs:
our update process skips some revisions, and our page titles are not
easy to search.
I decided that the time had come to fix these bugs, so I have been
working on a new branch to implement these changes.

\section{Including Reputation}

\subsection{2-Feb-2011}

For my thesis, I have this idea to redo the vandalism detection task
using our quality measures and user reputation score, to see how
our scoring changes.

I am trying to test out the batch process on the
``Santa Cruz Beach Boardwalk'' article that I downloaded.
To do this, I had to compress the file with \program{7z}:
\begin{verbatim}
$ 7z a dump.7z dump-SantaCruzBeachBoardwalk-fixed.xml
\end{verbatim}
and then run it through the batch process (modified on
my branch \texttt{thumper-vandalrep}):
\begin{verbatim}
$ cd WikiTrust/util
$ mkdir tmp
$ cd tmp
$ ../batch_process --cmd_dir ../../analysis --dir output dump.7z
\end{verbatim}
The modifications I've made are to add a line to the
user reputation history file to give the revision id
and user reputation before and after judging.

My modification leads to multiple lines being printed
for a single revision, as each judge modifies the authors reputation.
While looking at the output, I noticed that the author reputation
doesn't actually seem to change.
I tracked this down to the \texttt{Computerep.store\_rep}
function, which rounds reputations to the lower hundredth.
On my small test file, this results in no reputations being
adjusted --- but that's probably part of the ``robust reputation''
work that has been added in.

Need to check on larger dump that reputations actually increase.

\subsection{14-Feb-2011}

The batch process was killing Redherring, so I moved it to Gaston.
There, it was going so slowly that it seemed it would take months
to finish, so I spent the weekend working on a parallel process that
would \texttt{rsync} files to EC2 instances, do the computation,
and then bring the files back.
That completed this morning; about 36hrs using 20 instances plus
my 64-bit server in LA.
The program I wrote I call \program{dist\_stats.pl}.
(Note that Gaston is about halfway done with processing, so
it speeds up as we get further into the history.)

Now I need to sort the statistics and then generate the reputation.
\begin{verbatim}
$ ../batch_process --cmd_dir ./tools --dir output-enwiki --do_sort_stats /raid/dumps/enwiki-20100130-pages-meta-history.xml.bz2
\end{verbatim}

\subsection{20-May-2011}

So much time has passed that I don't recall where this was at.
A user reputation file is available on Gaston at
\texttt{/raid/thumper/util/tmp/output-enwiki/user\_reputations.txt},
so I seem to have completed the sorting and reputation steps.

The next step to completing this experiment is to write code to
extract features for each of the PAN2010 edits, and put them into
a single file for analysis by Weka.

Looking over the codebase, what I did was add a \texttt{VANDALREP}
line every time a user's reputation was adjusted.
Included in that line was a revid, a timestamp, the userid and username,
and the old and new reputation (and reputation bins).

Tragedy!  The \texttt{generate\_reputation} program was hard-coded
to not calculate reputations past 2008-10-30!
Now I have to recalculate the reputations.

\begin{verbatim}
$ cd /raid/thumper/util/tmp
$ ../batch_process.py --cmd_dir ./tools --dir output-enwiki --do_sort_stats --do_compute_rep /raid/dumps/enwiki-20100130-pages-meta-history.xml.bz2
\end{verbatim}

\subsection{31-May-2011}

Another tragedy: I needed to use the batch process script from
my \texttt{thumper-vandalrep} branch, not the version from the
mainline development.
The one from my branch turns on a few more options which generate
the \texttt{VANDALREP} messages.
I copied in the right file and am running the command again,
this time without the sorting of stats.

\subsection{9-Jul-2011}

Just getting back to this while the edit quality evaluation runs.
The reputations file is there and date 1-Jun-2011 @ 07:44,
so it must take less than a day to run.

Last weekend, I started looking again at the work we did
for the PAN~2010 competition.
This code is stored on SPCR, in
\texttt{archives/2010/UCSC/redherring/vandalism}.
Since I want to do a comparison of immediate vandalism
detection with what we generated for the competition,
I isolated the list of features we used for the immediate mode:
RevId, Anon, PrevSameAuthor, LogtimePrev, HourOfDay,
Delta, OverallTrust, Hist[0-9], LogPrevLength,
LogLength, PPrevHist[0-9], LDeltaHist[0-9].
In the PAN competition, this data comes from the mysql database.
Looks like I need to be able to parse through the SQL files
generated as part of the batch process.
These files are in
\texttt{gaston:/raid/thumper/util/tmp/output-enwiki/sql}.

\subsection{16-Jul-2011}

Oh no, it turns out that the SQL files are generated during the
\textit{trust} phase of the batch process.
But that phase crashes because it tries to read the user reputation
file, which I have corrupted with my \texttt{VANDALREP} lines.

\begin{verbatim}
$ cd /raid/thumper/util/tmp
$ rm -rf output-enwiki/sql/*
$ cd output-enwiki
$ mv user_reputations.txt user_reputations.txt-VANDALREP
$ grep -v VANDALREP user_reputations.txt-VANDALREP > user_reputations.txt
$ ..
$ ../batch_process.py --cmd_dir ./tools --dir output-enwiki --do_compute_trust /raid/dumps/enwiki-20100130-pages-meta-history.xml.bz2
\end{verbatim}

This still crashes after a few minutes, from a stack overflow.
Trying to get a stack trace now.
Tracked down the problem to the fact that the user rephistory file
is ridiculously large; 58GB!

\subsection{17-Jul-2011}

After thinking about it for a while, I realized that the problem
is that I had originally run the analysis back when not much about
the code had changed.
Then I made all my \texttt{VANDALREP} changes to track the increments
to user reputation at every moment in time.
And then somewhere along the line I allowed judgements to be made between
revisions even if the authors were the same; this was because
the PAN corpus was too small to get any real data for ever revision
unless I disabled this.
But then I reran the analysis for the whole enwiki because of
the normalization error that I found, which happened to include this
change.

I went through the code and fixed everything to write
\texttt{VANDALREP} to a separate file.
I also added a boolean flag to \file{reputation\_analysis.ml}
to decide whether to use the PAN style of judgements, or normal;
this is a bit dangerous since it depends on me remembering that
I did this.

\begin{verbatim}
$ time ../batch_process.py --cmd_dir ./tools --dir output-enwiki --do_split --do_compute_stats --do_sort_stats --do_compute_rep --do_compute_trust /raid/dumps/enwiki-20100130-pages-meta-history.xml.bz2
\end{verbatim}

\subsection{21-Sept-2011}

About that boolean flag to do PAN judgements, I added an option
\texttt{--do\_pan} which triggers the PAN style computation.
So the above command is safe to run.

\subsection{15-Jan-2012}

To make sure that the computations could run to completion, I did a test
run in October over the Italian Wikipedia.
Since I have been having problems with remembering the arguments I want
to use, I created the \file{projects/reputation-quality/run-expt.sh}
script.
It copies all the commands to a working directory so that I can't screw
up by recompiling something while there is a run going.
For this experiment, I want to use normal judgements rather than PAN
judgements
The run on the Italian Wikipedia looked to be successful, so I started
again with trying to compute for the English

After various problems, a run on the English Wikipedia recently finished
on Gaston, in \file{/raid/thumper/tmp-rep-enwiki}.
There are some problems:
\begin{itemize}
\item The \file{user\_reputations.txt} is empty; this doesn't seem
right.
\item There are no vandalrep files generated.
\end{itemize}
I looked into the vandalrep file issue; looking in directory
\texttt{\$WORKDIR/output/cmds} the executables all have a timestamp of
2011-10-09 14:15.
Running
\begin{verbatim}
$ cd output/cmds
$ strings * | grep -i vandal
\end{verbatim}
gives no results, so this appears to not be the correct branch of code.
Related to that, I see in my source tree the same executables but the
source tree is on branch \texttt{thumper} rather than
\texttt{thumper-vandalrep}.

I see that I committed to branch \texttt{thumper-vandalrep} on 2011-10-08.
It looks like I compiled a new version of the dispatcher on 2011-10-09
and installed it in \file{/home/wikitrust/bin}, and forgot that I was on
the wrong branch when I restarted my experiment.

That explains the second problem I discovered, by what about the first?
The user reputation file is created by the program
\program{generate\_reputations}, which I see ran without any reported error.
It was invoked with \texttt{-write\_final\_reps} which only writes
reputations at the end, so perhaps the program crashed and didn't leave
an error?
Ah, the program crashed as soon as it started, because the other branch
still invokes \program{git} but I am not running from a \program{git}
repository.
And the \program{batch\_process.py} script does not actually check for a
failed result from \program{generate\_reputations}, so this isn't
captured in the log file.

Now that I have figured out what happened, I have to see if I can reuse
the partial results to redo the experiment without lengthy waiting.
As part of my disk space saving attempts, I deleted part of the output
from \program{splitwiki}; I've modified that program to skip over files
that already exist (to avoid the I/O writes), and am regenerating those
files.  The rest of the programs require rerunning to get all the
reputation data, so I need to look at \program{dist\_stats.pl} again.

Created a new project directory called \file{dist-compute}.
Discovered a problem with using \program{git} on Gaston, where it won't
access github.com;
upgrading to a newer version of \program{git} improves this, but won't
let me push back to the server.

\begin{verbatim}
$ ssh gaston
...
$ cd Thesis/projects/dist-compute
$ make -f new-server.mk wikitrust_src SRCDIR=/store/thumper/research/WikiTrust WORKDIR=/store/thumper/tmp

$ ssh redherring
$ cd /giant/thumper/research/Thesis/projects/dist-compute
$ make -f new-server.mk wikitrust_src SRCDIR=/giant/thumper/research/WikiTrust WORKDIR=/big/thumper/tmp
$ export OUTPUT=/raid/thumper/tmp-rep-enwiki/output
$ ./dist-stats.pl --split $OUTPUT/split_wiki --stats $OUTPUT/stats dist.cfg
\end{verbatim}

