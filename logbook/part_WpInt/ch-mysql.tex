\chapter{Optimizing MySQL}

\section{Introduction}

\subsection{3-Nov-2009}

Luca recently got an offer of help to optimize \mysql from Deitmar Julius Gabby.
We had a conference call before our trip to WikiSym; we gave Julius copies
of the schema and data so that he could experiment on his machine at home.

This finally inspired me to make it a priority to try my idea at optimizing
\mysql by implementing a genetic algorithm to search the parameter space.


\section{Manual Optimization}

\subsection{27-Oct-2009}

After searching Google, I found a site~\cite{InnodbOpt} which had some
good information on optimizing an \package{InnoDB} installation.
The final configuration I came up with was:
\begin{verbatim}
[mysqld]
innodb_flush_method     = O_DIRECT
innodb_buffer_pool_size = 5G
innodb_log_file_size = 256M
innodb_log_buffer_size=8M
innodb_flush_log_at_trx_commit=1
innodb_thread_concurrency=8
innodb_file_per_table
innodb_autoextend_increment=1000                # in MB
innodb_data_file_path=ibdata1:2G:autoextend
innodb_log_group_home_dir=/big/mysql

tmpdir=/big/mysql/tmp
datadir=/giant/mysql
max_allowed_packet=64M
\end{verbatim}
This takes into account Julius's advice about using different disks
for the logdir and datadir.

\section{Optimizing Mysqldump}

\subsection{2-Nov-2009}

I wrote up a program, \program{mysql-GA.pl}, which uses genetic algorithm
techniques to explore parts of the configuration space.
As a test input file, I used a \program{mysqldump} of the Portuguese wikitrust
metadata; it is about 3GB large.
I seeded the program with the configuration I manually found,
and then let it evolve for 20 generations with a population of 30
at each generation.
This finally found a configuration that can load the dump file
in 303 seconds.

\begin{verbatim}
[mysqld]
innodb_autoextend_increment=1000
innodb_data_file_path=ibdata1:2G:autoextend
max_allowed_packet=64M
innodb_flush_method=O_DIRECT
innodb_buffer_pool_size=3733M
innodb_log_file_size=1874M
innodb_log_buffer_size=456M
innodb_flush_log_at_trx_commit=2
innodb_thread_concurrency=6
innodb_file_per_table
innodb_log_group_home_dir=/giant/mysql-ga/logdir
tmpdir=/tmp/mysql-ga/tmpdir
datadir=/big/mysql-ga/datadir
\end{verbatim}

\section{Optimizing Wikitrust Load}

\subsection{3-Nov-2009}

I initially had the idea, after pondering an email from Julius, that we weren't
taking advantage of the threading in \mysql while doing the load.
I wrote \program{load\_db.pl} to fork multiple processes which would
do the loading into the database.
I made some changes to how Ian was doing the processing, to give
the genetic algorithms more to explore.
I let this run for eight hours on a single test (of itwiki) before giving up.
Some email exchanges with Ian and Luca this morning had them
questioning why my new test was so slow.
Things that are different:
\begin{itemize}
\item \program{mysqldump} includes extra (commented?) commands at the top of the file
\item \program{mysqldump} is a single file, ian-loader is based on thousands of files (lots of seeking)
\item \program{mysqldump} packs many INSERTs into a single statement
\item \program{mysqldump} uses INSERT, and we use REPLACE.
\item \program{mysqldump} output is just over half the size of ours because it does not name fields.
\end{itemize}

I used Ian's original \program{load\_db.sh} to generate a single file with all the commands in,
and ran a timing test using the best configuration previously found by the GA.
This ran for over an hour before I gave up, and eliminates seeking as a serious problem.

Luca pointed out that he now uses INSERT so I am generating a new input file with this change.

\subsection{5-Nov-2009}

Continuing to pursue the loading time discrepancy, I experimented with making changes to Ian's
\program{load\_db.sh} script until the output was as much the same as I could make it.
Even with this effort, the load times were still vastly differently.
And then I realized that the sort order of the records was different.
The records from \program{mysqldump} were in order of the primary key (revision\_id),
but our load file was in (page\_id, revision\_id) order.
I was able to establish this the problem by removing the primary key from the table.

\subsection{18-Nov-2009}

After the last discovery, I modified the testing to remove the primary key.
According to the \mysql documentation, this creates a hidden primary key which
amounts to the row number.

After discussing this with Julius, he suggested that I also investigate varying
the number of threads used to load the data.
I wrote a new \program{load\_db.pl} which forks multiple mysql processes to
load the data, and then launched some new runs.
I first ran this on the Portuguese wikipedia, and after 29 hours found a best time of
112.8 seconds:
\begin{verbatim}
[1,1665,1666,216,1,54,0,1,2,1,5,1,1,10,4]
\end{verbatim}

I then ran a test on the Italian wikipedia data (about 8.6GB) with a population of 30 for 30 generations,
and received a result in 103 hours.
\begin{verbatim}
.../mysql-optimization# time ./mysql-GA.pl --dir /giant/thumper/itwiki-oct-09-v2/sql
\end{verbatim}
The best result runs in 125.2 seconds, and the configuration is:
\begin{verbatim}
[mysqld]
innodb_autoextend_increment=1000
innodb_data_file_path=ibdata1:2G:autoextend
innodb_additional_mem_pool_size=256M
max_allowed_packet=64M
thread_cache_size=64
innodb_flush_method=O_DSYNC
innodb_buffer_pool_size=2760M
innodb_log_file_size=1472M
innodb_log_buffer_size=538M
innodb_flush_log_at_trx_commit=1
innodb_thread_concurrency=61
innodb_file_per_table
innodb_log_group_home_dir=/giant/mysql-ga/logdir
tmpdir=/tmp/mysql-ga/tmpdir
datadir=/giant/mysql-ga/datadir
# loadThreads = 6
# notnull = 0
# indexbefore = 1
innodb_file_io_threads=52
innodb_log_files_in_group=4
\end{verbatim}

\subsection{23-Nov-2009}

Since the configuration space has changed a bit since I last ran for
the Portuguese wikipedia (about 5.4GB), I reran that test (runs in 109.2 seconds):
\texttt{time ./mysql-GA.pl --dir /giant/thumper/ptwiki-20091002/sql}.

\begin{verbatim}
[mysqld]
innodb_autoextend_increment=1000
innodb_data_file_path=ibdata1:2G:autoextend
innodb_additional_mem_pool_size=256M
max_allowed_packet=64M
thread_cache_size=64
innodb_flush_method=O_DIRECT
innodb_buffer_pool_size=1665M
innodb_log_file_size=1150M
innodb_log_buffer_size=343M
innodb_flush_log_at_trx_commit=2
innodb_thread_concurrency=54
innodb_file_per_table
innodb_log_group_home_dir=/big/mysql-ga/logdir
tmpdir=/tmp/mysql-ga/tmpdir
datadir=/giant/mysql-ga/datadir
# loadThreads = 5
# notnull = 1
# indexbefore = 1
innodb_file_io_threads=16
innodb_log_files_in_group=4
\end{verbatim}

This configuration took 52 hours to run.
\begin{verbatim}
root@redherring:/giant/thumper/research/wt-project/projects/mysql-optimization# time ./mysql-GA.pl --dir /giant/thumper/ptwiki-20091002/sql
real    3090m11.602s
user    1024m8.550s
sys     235m35.520s
\end{verbatim}

\subsection{13-Jan-2010}

We had our LANL mentor meeting today.
Beforehand, I downloaded \program{ggobi} and played around with
its visualization capabilities.
It is a really interesting program, but my initial reactions to the
mysql timing data that I have collected is that there isn't a clearcut
relation between any of the parameters and the resulting run time.
What I need is a way to cluster datapoints which are nearly identical
(for example, differing only in one parameter) and then try to establish
relations based on those parameters.
Then, the next step would be to find a different cluster which vary
only in that same parameter, and see if the same relation holds true.

At the LANL meeting, Shelley suggested running a multivariate
regression.
Gary Grider added that there is a technique in statistics known
as ``jackknifing'' which might help to find the parameters
which are most influential in the result.
Luca suggested that I turn it into a machine learning problem
and see if any of those algorithms could find a way to predict
the running time based on the parameters.

An idea that was floating around my head this morning was to look
at it from an information theoretic point of view.
In this case, parameters with the least mutual information would be the
ones that are the most correlated with the resulting running time.
However, given the seemingly random result with respect to any
single parameter, I fear that this won't lead to anything useful.


\section{Slow Query Log}

\subsection{29-Nov-2010}

After turning on the slow query log, we did a minor overhaul of
the code to change (or solidify) some of the assumptions being made.
Running our new code on Gaston, one issue I've seen is that my new
checks for a pre-existing page run very slowly because there is no good index
on the \textit{page\_title} field.

On Redherring, I've decided to be proactive about creating an index to
avoid full table scans:
\begin{verbatim}
mysql> create index page_title_prefix5 on page(page_title(5));
\end{verbatim}
I created an index on only the first five characters since this is enough
to make most tests have only a single row to check.

The code is now rolled out to redherring, and I am trying to recreate our
monitoring:
\begin{verbatim}
mysql> set global slow_query_log_file='/var/log/mysql/slow-query.log';
mysql> set global slow_query_log=1;
\end{verbatim}

