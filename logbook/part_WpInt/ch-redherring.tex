\chapter{Loading onto Redherring}

\section{Introduction}

\subsection{28-Apr-2009}

Luca has unpacked the 20080103 dump of the Wikipedia into
a set of flat files, separating the text from the metadata.
Now the metadata needs to be loaded into a mysql database.

\section{Initialization}

\subsection{28-Apr-2009}

To begin, a \mediawiki installation must be created and
initialized so that the database tables are created.
Ian documents how to do this in an email on 9-Apr-2009,
subject ``\textit{Directions to set up a wiki on redherring}.''
There are several errors in this email, and points that are
left to the imagination, but it conveys enough to figure
out the actual commands.

The particular instructions I followed were:
\begin{enumerate}
\item Download \mediawiki and rename the directory 
	to \url{/export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008}
\item Configure apache by doing the following:
\begin{verbatim}
$ sudo /usr/local/bin/wikitrust-ownwebconf
$ vi /etc/httpd/conf.d/wikitrust.conf
$ sudo /usr/local/bin/wikitrust-restartweb
\end{verbatim}
	and placed the following text into the file at the end:
\begin{verbatim}
Listen 10304
NameVirtualHost *:10304
<VirtualHost *:10304>
  ServerAdmin thumper@cs.ucsc.edu
  ServerName wikitrust.soe.ucsc.edu

  DocumentRoot /export/notbackedup/wikitrust1/hosted_wikis/wikipedia-2008
  <Directory />
    Options FollowSymLinks
    AllowOverride None
    Order allow,deny
    allow from all
  </Directory>
</VirtualHost>
\end{verbatim}

\item Then goto the mediawiki configuration page, at
    \url{http://redherring.cse.ucsc.edu:10304}.
    The configuration is standard, but a few items are worth noting:
\begin{table}[h]
\begin{tabular}{r l}
Wiki name & Wikipedia \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb-thumper \\
DB username & wikiuser \\
DB password & wikiword \\
Superuser name & wikidba \\
Superuser password & taken from \file{/etc/.mypass} \\
\end{tabular}
\end{table}


\end{enumerate}

\section{Configuring the Ocaml Environment}

\subsection{28-Apr-2009}

The simplest way to get a working \package{OCaml} environment is
to use the \package{godi} package management system.

Start by visiting the website:
\url{http://godi.camlcity.org/godi/index.html}.
Unpack the \package{rocketboost} package and run as:
\begin{verbatim}
$ cd /tmp/thumper/godi-rocketboost-20080630
$ ./bootstrap --prefix /export/notbackedup/wikitrust1/thumper/godi
$ set path= ( /export/notbackedup/wikitrust1/thumper/godi/sbin /export/notbackedup/wikitrust1/thumper/godi/bin /usr/local/bin /usr/sbin /usr/bin /sbin /bin )
$ vi /export/notbackedup/wikitrust1/thumper/godi/etc/godi.conf
$ ./bootstrap_stage2
\end{verbatim}
When editing the \file{godi.conf} file, uncomment the line
for PCRE.


\section{Configure Mediawiki with WikiTrust Extension}

\subsection{28-Apr-2009}

First, load the wikitrust extension into \mediawiki:
\begin{verbatim}
$ cd /export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008/extensions
$ git clone ssh://thumper@trust.cse.ucsc.edu/pub/git/WikiTrust.git
\end{verbatim}

Then we run Ian's script to build the wikitrust mysql tables:
\begin{verbatim}
$ cd sql
$ ./create_db.php /export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008 wikidba
\end{verbatim}

\section{Loading the data}

\subsection{28-Apr-2009}

To load the data, we need to use the \program{load\_data.py} script,
which needs a little configuration:
\begin{verbatim}
$ cd ../test-scripts
$ cp db_access_data.ini.sample db_access_data.ini
$ vi db_access_data.ini
\end{verbatim}
Fill the INI file with the information we configured above.
Also put a recent copy of \program{mwdumper.jar}~\cite{mwdumper}
into the directory.

As a performance tip, it is recommended to remove all indices
before loading data.  This can be done with the following set
of commands:
\begin{verbatim}
$ ./load_data.py --clear_db
$ mysqldump -u wikidba -p wikidb-thumper > wikidb-thumper.sql
$ mysql -u wikidba -p wikidb-thumper
mysql> alter table page drop index name_title;
mysql> alter table page drop index page_random;
mysql> alter table page drop index page_len;
mysql> alter table revision drop index rev_id;
mysql> alter table revision drop index rev_timestamp;
mysql> alter table revision drop index page_timestamp;
mysql> alter table revision drop index user_timestamp;
mysql> alter table revision drop index usertext_timestamp;
mysql> alter table revision drop primary key;
mysql> alter table revision change `rev_id` `rev_id` int(10) unsigned NOT NULL;
mysql> alter table page change `page_id` `page_id` int(10) unsigned NOT NULL;
mysql> alter table text change `old_id` `old_id` int(10) unsigned NOT NULL;
mysql> alter table page drop primary key;
mysql> alter table text drop primary key;

Now we can process all the metadata files to load them into the DB:
\begin{verbatim}
$ ./load_data.py --clear_db
$ find ~luca/wikitrust2/enwiki-20080103-metadata -name "*[0-9].xml" -print0 | xargs -0 ./load_data.py 
\end{verbatim}

\subsection{29-Apr-2009}

The process of loading all the revision metadata took less than 18 hours,
resulting in 118,272,519 loaded revisions.

To restore the system back to its previous functionality, we need to
restore the indices.  Place the following SQL sequence into
file \file{restore.sql}.
%
\begin{verbatim}
alter table revision change `rev_id` `rev_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`rev_page`, `rev_id`);
alter table text change `old_id` `old_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`old_id`);
alter table page change `page_id` `page_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`page_id`), ADD UNIQUE KEY `name_title` (`page_namespace`, `page_title`), ADD KEY `page_random` (`page_random`), ADD KEY `page_len` (`page_len`);
alter table revision ADD UNIQUE KEY `rev_id` (`rev_id`), ADD KEY `rev_timestamp` (`rev_timestamp`), ADD KEY `page_timestamp` (`rev_page`,`rev_timestamp`), ADD KEY `user_timestamp` (`rev_user`,`rev_timestamp`), ADD KEY `usertext_timestamp` (`rev_user_text`,`rev_timestamp`);
\end{verbatim}
%
And then run the following commands:
\begin{verbatim}
$ mysql -u wikidba -p wikidb-thumper
mysql> source restore.sql
\end{verbatim}

\section{Benchmarking the Analysis}

\subsection{9-May-2009}

At Friday's meeting, Luca and I discussed running some test timings
of an analysis of the Redherring data.
He pointed out to me that there are a few extra options that
I should specify, to store the colored information to disk
rather than to the DB.
\begin{verbatim}
-wt_db_rev_base_path
-wt_db_sig_base_path
-wt_db_colored_base_path
\end{verbatim}

\subsection{12-May-2009}

Ran some initial testing over the last few days on my
laptop (documented in Section~\ref{sec:up2date-testing}.
Got things to be basically running, so that I am finally
ready to run a benchmark on Redherring.
Made sure to recompile all code with ``\texttt{make allopt}''
to get the  best possible running time.
Following the command line described in
Section~\ref{sec:up2date-testing-eval}:
\begin{verbatim}
$ cd ~luca/wikitrust1/hosted_wikis/wikipedia-2008/extensions/WikiTrust
$ make clean
$ make allopt
$ time ./analysis/eval_online_wiki -n_events 1000 -wt_db_rev_base_path ~luca/wikitrust2/enwiki-20080103-revs -wt_db_sig_base_path ~luca/wikitrust1/enwiki-20080103-sigs -wt_db_colored_base_path ~luca/wikitrust1/enwiki-20080103-colored -db_user wikiuser -db_pass wikiword -db_name wikidb-thumper -log_file eval.log -delete_all
\end{verbatim}
At the beginning of the run, I observed heavy disk reads (about 45MB/sec,
using \program{dstat})
and that \program{mysqld} was using about 50\% of one CPU (using \program{top}).
No other processes were notably using CPU, and the \program{mysqld}
load seemed to be split across two CPUs.
Total running time was just shy of 49 minutes,
but the CPU time was about 5.5 seconds.
A did a second run, hoping that data would be cached in memory,
and measured almost identical results.

Clearly, there is some problem with \mysql accessing the
various tables.
Running a much shorter query while dumping the database calls
might shed some light on what access is causing the problem:
\begin{verbatim}
$ time ./analysis/eval_online_wiki -n_events 10 -wt_db_rev_base_path ~luca/wikitrust2/enwiki-20080103-revs -wt_db_sig_base_path ~luca/wikitrust1/enwiki-20080103-sigs -wt_db_colored_base_path ~luca/wikitrust1/enwiki-20080103-colored -db_user wikiuser -db_pass wikiword -db_name wikidb-thumper -log_file eval.log -delete_all -dump_db_calls -sync_log
\end{verbatim}
Dumping the database calls reveals the following slow queries:
\begin{verbatim}
SELECT rev_id, rev_page, rev_text_id, rev_timestamp, rev_user, rev_user_text, rev_minor_edit, rev_comment FROM revision  ORDER BY rev_timestamp ASC, rev_id ASC LIMIT 100
\end{verbatim}

Asking \mysql to \texttt{EXPLAIN} how it executes the query
reveals the problem: it does a filescan through the whole table!
\begin{verbatim}
mysql> explain SELECT rev_id, rev_page, rev_text_id, rev_timestamp, rev_user, rev_user_text, rev_minor_edit, rev_comment FROM revision  ORDER BY rev_timestamp ASC, rev_id ASC LIMIT 100;
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
| id | select_type | table    | type | possible_keys | key  | key_len | ref  | rows      | Extra          |
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
|  1 | SIMPLE      | revision | ALL  | NULL          | NULL | NULL    | NULL | 121184090 | Using filesort |
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
1 row in set (0.00 sec)
\end{verbatim}
I experimented with removing some of the fields from the query
and that helps slightly by allowing \mysql to use indices to
collect the data, but was still ridiculously slow.
I also experimented with sub-queries and temporary tables
to try to limit the number of rows that needed scanning;
these ideas didn't work because the \field{rev\_id} is out
of order with respect to \field{rev\_timestamp}, and relying
on the index for \field{rev\_id} can result in scanning
200K+ rows to check for membership in the sub-query.

Ultimately, the problem seems to boil down to the index
created by WikiTrust: \field{wikitrust\_revision\_id\_timestamp\_idx}.
This index has the order of the fields exactly backwards
to what the query requires.
I created a new index to attempt to solve this problem:
\begin{verbatim}
alter table revision add KEY `wikitrust_timestamp_revision_id_idx` (`rev_timestamp`, `rev_id`);
\end{verbatim}

\section{Fixing the Unpacked XML}

\subsection{21-May-2009}

In preparation for running an evaluation, I started looking
at the ``origin bug'' that Ian and I had noted a few months ago.
This uncovered a problem in the XML files we extracted from
the 20080103 Wikipedia dump: entities were not decoded as
part of the extraction.

See Section~\ref{sec:origin-fixxml} for information more information.

\section{Downloading Wikipedia 0.7}

\subsection{23-May-2009}

``Wikipedia 0.7'' is a collection of articles that are being used
to define a \textit{core} of useful information.
The group working on this project has asked us to help them
identify revisions from each article that are ``most trustworthy''
around 6-Dec-2008.
(This is all gleaned from an email exchange with
\index{Martin Walker}
Martin Walker, dated 24-Apr-2009 with
subject ``Re: presentation for Wikimania''.)

In Martin's last email, he gives links to two lists of
the articles that they have chosen: one in HTML format,
and another in a simple text format.

I chose the simple text format, which has records like:
\begin{verbatim}
ARTICLE "Weird_Al"_Yankovic
REVS    5909
REVID   256156220
REVUSER Esanchez7587
REVTIME 20081206015207
AUTHOR  17Drew
AUTHOR  Zach Cohen
IPAUTHORS
\end{verbatim}
but I found a questionable entry right near the beginning
of the file:
\begin{verbatim}
ARTICLE "Weird_Al"_Yankovic|Yankovic,_"Weird_Al"
REVS    0E0
REVID   0
REVUSER 
REVTIME 
\end{verbatim}
I checked the official list of articles and found that
there was no entry with this title.
\indexfile{extract-wp0.7-articles.pl}
I wrote a quick \program{perl} script
(see Figure~\ref{fig:redherring-wp0.7-scan.pl})
to print only valid articles.
(Removing invalid articles is important because
the \program{downloadwp} program aborts if it gets back
an error from the Wikipedia API.)
\begin{figure}
\lstset{language=Perl}
\begin{fminipage}
\begin{lstlisting}
#!/usr/bin/perl

my $last = undef;
while(<>) {
    my ($key, $val) = split(' ', $_, 2);
    $last = $val if $key eq 'ARTICLE';
    print $last if $key eq 'REVS' && $val+0 > 0;
}
\end{lstlisting}
\end{fminipage}
\caption[File \texttt{extract-wp0.7-articles.pl}]{
    Source code for \file{extract-wp0.7-articles.pl},
used to extract the titles of articles being included
in Wikipedia 0.7.
}
\label{fig:redherring-wp0.7-scan.pl}
\end{figure}


Currently, the process to fix the XML extracted files is
still running, but I realized today that I could start the
download and have the process write the files to a different
disk location (to be merged later).
\begin{verbatim}
$ cd ~luca/wikitrust2/thumper/wikipedia-0.7
$ ./extract-wp0.7-articles.pl release.20081213 > articles.txt
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wikiword \
    -db_name wikidb-thumper -log_file dlwp.log < articles.txt &
\end{verbatim}


Just as a note, while trying to get everything setup on Redherring,
I had to install some \program{godi} packages.
During this process, I ran into multiple problems compiling because
\indexprogram{pcre\_free}
the symbol \texttt{pcre\_free} was not defined.
It was clearly an instance of a bug discussed at
\url{http://www.mail-archive.com/godi-list@ocaml-programming.de/msg00236.html},
but without any satisfactory resolution.
After fighting with it, I managed to get things working by
using the built-in \package{godi-pcre} package, and then
compiling our WikiTrust code in optimized mode.

\subsection{31-May-2009}

Ran into problems with trying to download Wikipedia articles.
\begin{enumerate}
\item The article \article{``Weird Al'' Yankovic} seems to have changed
	its \field{page\_id}, from 104,146 to 189,382,265.
	This causes problems for our code because we don't expect
	this kind of change, so we don't catch it and update the tables.
\item The article \article{2005 Atlantic Hurricane Season} won't download.
\end{enumerate}

Luca declared that we should not waste time currently on these corner cases.
Instead, we should make a note of these problem articles
and download them later.

I made a few modifications to handle these cases:
\begin{enumerate}
\item I added code to \package{Online\_db} to get the \field{page\_id}
	of an article from its title.  An exception is thrown if the
	title doesn't exist in the database.
\item I added code to \package{Wikipedia\_api} to compare the
	\field{page\_id} downloaded with that in the database,
	and raise an exception if both exist but are different.
\item Modified \package{downloadwp} to catch exceptions and record
	the \field{page\_title} in the logger output.
\end{enumerate}

\subsection{14-Jun-2009}

Summary of last two weeks:
\begin{itemize}
\item ran into a new problem: one of the downloaded
files was throwing a UTF-8 error.
Decided to follow Luca's advice and not immediately pursue it;
error is caught and reported, and then program moves on.
\item After some monitoring, realized that user ID queries were dramatically
slowing the process because anonymous users were also getting looked up;
now a regex detects IP numbers and assumes uid is zero.
\end{itemize}
At this point, progress was about 1.5\% per day.
Divided download list in half, and launched second process.
Have currently downloaded about 32\% of articles.

\section{Templates}
\label{ch:redherring-templates}

\subsection{16-Aug-2009}

In order to have working template expansion, the templates need
to be loaded into the database (don't use the disk-based download).
We don't need every revision for the templates, only the latest revision.
(Even Wikipedia only uses the latest revision of a template when
viewing historical revisions of an article.)

The first step was to get a list of templates and the latest
revision for each template.
I used my recently acquired login to the Wikipedia Tool Server
to query the database and the information required:
\begin{verbatim}
thumper@nightshare:~$ sql enwiki_p
mysql> select count(*) from page where page_namespace=10;+----------+
| count(*) |
+----------+
|   233566 | 
+----------+
1 row in set (0.53 sec)

mysql> quit
Bye
thumper@nightshare:~$ sql enwiki_p > templates.txt
select page_title, page_latest from page where page_namespace=10;
\end{verbatim}

Examining the file, we obviously have to delete the first line.
But there are also several entries that are documentation for the templates,
which should be removed.
Also, templates are referred to by specifying the ``Template:'' prefix.
We can clean it up like so:
\begin{verbatim}
$ grep -v -P '/doc[\t]' templates.txt | sed -e 's/^/Template:/' > templates1.txt
\end{verbatim}

And finally, on Redherring we can download the templates and
place them into the database:
\begin{verbatim}
$ time ./downloadwp -db_name wikidb-thumper -db_pass wikiword -log_file templates.log < templates1.txt &
\end{verbatim}

