\chapter{Loading onto Redherring}

\section{Introduction}

\subsection{28-Apr-2009}

Luca has unpacked the 20080103 dump of the Wikipedia into
a set of flat files, separating the text from the metadata.
Now the metadata needs to be loaded into a mysql database.

\section{Initialization}

\subsection{28-Apr-2009}

To begin, a \mediawiki installation must be created and
initialized so that the database tables are created.
Ian documents how to do this in an email on 9-Apr-2009,
subject ``\textit{Directions to set up a wiki on redherring}.''
There are several errors in this email, and points that are
left to the imagination, but it conveys enough to figure
out the actual commands.

The particular instructions I followed were:
\begin{enumerate}
\item Download \mediawiki and rename the directory 
	to \url{/export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008}
\item Configure apache by doing the following:
\begin{verbatim}
$ sudo /usr/local/bin/wikitrust-ownwebconf
$ vi /etc/httpd/conf.d/wikitrust.conf
$ sudo /usr/local/bin/wikitrust-restartweb
\end{verbatim}
	and placed the following text into the file at the end:
\begin{verbatim}
Listen 10304
NameVirtualHost *:10304
<VirtualHost *:10304>
  ServerAdmin thumper@cs.ucsc.edu
  ServerName wikitrust.soe.ucsc.edu

  DocumentRoot /export/notbackedup/wikitrust1/hosted_wikis/wikipedia-2008
  <Directory />
    Options FollowSymLinks
    AllowOverride None
    Order allow,deny
    allow from all
  </Directory>
</VirtualHost>
\end{verbatim}

\item Then goto the mediawiki configuration page, at
    \url{http://redherring.cse.ucsc.edu:10304}.
    The configuration is standard, but a few items are worth noting:
\begin{table}[h]
\begin{tabular}{r l}
Wiki name & Wikipedia \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb-thumper \\
DB username & wikuser \\
DB password & wikiword \\
Superuser name & wikidba \\
Superuser password & taken from \file{/etc/.mypass} \\
\end{tabular}
\end{table}


\end{enumerate}

\section{Configuring the Ocaml Environment}

\subsection{28-Apr-2009}

The simplest way to get a working \package{OCaml} environment is
to use the \package{godi} package management system.

Start by visiting the website:
\url{http://godi.camlcity.org/godi/index.html}.
Unpack the \package{rocketboost} package and run as:
\begin{verbatim}
$ cd /tmp/thumper/godi-rocketboost-20080630
$ ./bootstrap --prefix /export/notbackedup/wikitrust1/thumper/godi
$ set path= ( /export/notbackedup/wikitrust1/thumper/godi/sbin /export/notbackedup/wikitrust1/thumper/godi/bin /usr/local/bin /usr/sbin /usr/bin /sbin /bin )
$ vi /export/notbackedup/wikitrust1/thumper/godi/etc/godi.conf
$ ./bootstrap_stage2
\end{verbatim}
When editing the \file{godi.conf} file, uncomment the line
for PCRE.


\section{Configure Mediawiki with WikiTrust Extension}

\subsection{28-Apr-2009}

First, load the wikitrust extension into \mediawiki:
\begin{verbatim}
$ cd /export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008/extensions
$ git clone ssh://thumper@trust.cse.ucsc.edu/pub/git/WikiTrust.git
\end{verbatim}

Then we run Ian's script to build the wikitrust mysql tables:
\begin{verbatim}
$ cd sql
$ ./create_db.php /export/notbackedup/wikitrust1/hosted\_wikis/wikipedia-2008 wikidba
\end{verbatim}

\section{Loading the data}

\subsection{28-Apr-2009}

To load the data, we need to use the \program{load\_data.py} script,
which needs a little configuration:
\begin{verbatim}
$ cd ../test-scripts
$ cp db_access_data.ini.sample db_access_data.ini
$ vi db_access_data.ini
\end{verbatim}
Fill the INI file with the information we configured above.
Also put a recent copy of \program{mwdumper.jar}~\cite{mwdumper}
into the directory.

As a performance tip, it is recommended to remove all indices
before loading data.  This can be done with the following set
of commands:
\begin{verbatim}
$ ./load_data.py --clear_db
$ mysqldump -u wikidba -p wikidb-thumper > wikidb-thumper.sql
$ mysql -u wikidba -p wikidb-thumper
mysql> alter table page drop index name_title;
mysql> alter table page drop index page_random;
mysql> alter table page drop index page_len;
mysql> alter table revision drop index rev_id;
mysql> alter table revision drop index rev_timestamp;
mysql> alter table revision drop index page_timestamp;
mysql> alter table revision drop index user_timestamp;
mysql> alter table revision drop index usertext_timestamp;
mysql> alter table revision drop primary key;
mysql> alter table revision change `rev_id` `rev_id` int(10) unsigned NOT NULL;
mysql> alter table page change `page_id` `page_id` int(10) unsigned NOT NULL;
mysql> alter table text change `old_id` `old_id` int(10) unsigned NOT NULL;
mysql> alter table page drop primary key;
mysql> alter table text drop primary key;

Now we can process all the metadata files to load them into the DB:
\begin{verbatim}
$ ./load_data.py --clear_db
$ find ~luca/wikitrust2/enwiki-20080103-metadata -name "*[0-9].xml" -print0 | xargs -0 ./load_data.py 
\end{verbatim}

\subsection{29-Apr-2009}

The process of loading all the revision metadata took less than 18 hours,
resulting in 118,272,519 loaded revisions.

To restore the system back to its previous functionality, we need to
restore the indices.  Place the following SQL sequence into
file \file{restore.sql}.
%
\begin{verbatim}
alter table revision change `rev_id` `rev_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`rev_page`, `rev_id`);
alter table text change `old_id` `old_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`old_id`);
alter table page change `page_id` `page_id` int(10) unsigned NOT NULL AUTO_INCREMENT, ADD PRIMARY KEY (`page_id`), ADD UNIQUE KEY `name_title` (`page_namespace`, `page_title`), ADD KEY `page_random` (`page_random`), ADD KEY `page_len` (`page_len`);
alter table revision ADD UNIQUE KEY `rev_id` (`rev_id`), ADD KEY `rev_timestamp` (`rev_timestamp`), ADD KEY `page_timestamp` (`rev_page`,`rev_timestamp`), ADD KEY `user_timestamp` (`rev_user`,`rev_timestamp`), ADD KEY `usertext_timestamp` (`rev_user_text`,`rev_timestamp`);
\end{verbatim}
%
And then run the following commands:
\begin{verbatim}
$ mysql -u wikidba -p wikidb-thumper
mysql> source restore.sql
\end{verbatim}

\section{Benchmarking the Analysis}

\subsection{9-May-2009}

At Friday's meeting, Luca and I discussed running some test timings
of an analysis of the Redherring data.
He pointed out to me that there are a few extra options that
I should specify, to store the colored information to disk
rather than to the DB.
\begin{verbatim}
-wt_db_rev_base_path
-wt_db_sig_base_path
-wt_db_colored_base_path
\end{verbatim}

\subsection{12-May-2009}

Ran some initial testing over the last few days on my
laptop (documented in Section~\ref{sec:up2date-testing}.
Got things to be basically running, so that I am finally
ready to run a benchmark on Redherring.
Made sure to recompile all code with ``\texttt{make allopt}''
to get the  best possible running time.
Following the command line described in
Section~\ref{sec:up2date-testing-eval}:
\begin{verbatim}
$ cd ~luca/wikitrust1/hosted_wikis/wikipedia-2008/extensions/WikiTrust
$ make clean
$ make allopt
$ time ./analysis/eval_online_wiki -n_events 1000 -wt_db_rev_base_path ~luca/wikitrust2/enwiki-20080103-revs -wt_db_sig_base_path ~luca/wikitrust1/enwiki-20080103-sigs -wt_db_colored_base_path ~luca/wikitrust1/enwiki-20080103-colored -db_user wikiuser -db_pass wikiword -db_name wikidb-thumper -log_file eval.log -delete_all
\end{verbatim}
At the beginning of the run, I observed heavy disk reads (about 45MB/sec,
using \program{dstat})
and that \program{mysqld} was using about 50\% of one CPU (using \program{top}).
No other processes were notably using CPU, and the \program{mysqld}
load seemed to be split across two CPUs.
Total running time was just shy of 49 minutes,
but the CPU time was about 5.5 seconds.
A did a second run, hoping that data would be cached in memory,
and measured almost identical results.

Clearly, there is some problem with \mysql accessing the
various tables.
Running a much shorter query while dumping the database calls
might shed some light on what access is causing the problem:
\begin{verbatim}
$ time ./analysis/eval_online_wiki -n_events 10 -wt_db_rev_base_path ~luca/wikitrust2/enwiki-20080103-revs -wt_db_sig_base_path ~luca/wikitrust1/enwiki-20080103-sigs -wt_db_colored_base_path ~luca/wikitrust1/enwiki-20080103-colored -db_user wikiuser -db_pass wikiword -db_name wikidb-thumper -log_file eval.log -delete_all -dump_db_calls -sync_log
\end{verbatim}
Dumping the database calls reveals the following slow queries:
\begin{verbatim}
SELECT rev_id, rev_page, rev_text_id, rev_timestamp, rev_user, rev_user_text, rev_minor_edit, rev_comment FROM revision  ORDER BY rev_timestamp ASC, rev_id ASC LIMIT 100
\end{verbatim}

Asking \mysql to \texttt{EXPLAIN} how it executes the query
reveals the problem: it does a filescan through the whole table!
\begin{verbatim}
mysql> explain SELECT rev_id, rev_page, rev_text_id, rev_timestamp, rev_user, rev_user_text, rev_minor_edit, rev_comment FROM revision  ORDER BY rev_timestamp ASC, rev_id ASC LIMIT 100;
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
| id | select_type | table    | type | possible_keys | key  | key_len | ref  | rows      | Extra          |
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
|  1 | SIMPLE      | revision | ALL  | NULL          | NULL | NULL    | NULL | 121184090 | Using filesort |
+----+-------------+----------+------+---------------+------+---------+------+-----------+----------------+
1 row in set (0.00 sec)
\end{verbatim}
I experimented with removing some of the fields from the query
and that helps slightly by allowing \mysql to use indices to
collect the data, but was still ridiculously slow.
I also experimented with sub-queries and temporary tables
to try to limit the number of rows that needed scanning;
these ideas didn't work because the \field{rev\_id} is out
of order with respect to \field{rev\_timestamp}, and relying
on the index for \field{rev\_id} can result in scanning
200K+ rows to check for membership in the sub-query.

Ultimately, the problem seems to boil down to the index
created by WikiTrust: \field{wikitrust\_revision\_id\_timestamp\_idx}.
This index has the order of the fields exactly backwards
to what the query requires.
I created a new index to attempt to solve this problem:
\begin{verbatim}
alter table revision add KEY `wikitrust_timestamp_revision_id_idx` (`rev_timestamp`, `rev_id`);
\end{verbatim}


