\chapter{Updating Wikipedia Download}

\section{Introduction}

\subsection{18-Apr-2009}

Luca is unpacking the 20080103 dump of the Wikipedia.

\section{Test Environment}

\subsection{18-Apr-2009}

Need to test code changes for downloading additional revisions
of the wikipedia.
Ian's interface, \file{wikipedia\_api.ml}, depends on the database
already exisiting for previously downloaded information.
Unfortunately, this leaves the little problem of how to create
this test database.

To setup a database, I installed \package{python-mysqldb}
and \mediawiki onto my Ubuntu instance.
Had to edit \file{/etc/mediawiki/mediawiki.conf} to 
enable the \texttt{Alias} for the installation.
Then navigated to \url{http://localhost/mediawiki/},
which prompted me to configure the installation.

Configuring the \mediawiki installation has a
bunch of questions to answer, most of them intuitive.
The only important ones to remember:
\begin{tabular}{r l}
Wiki name & WikiTrust \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb \\
DB username & wikuser \\
DB password & wkuser \\
Superuser name & debian-sys-maint \\
Superuser password & taken from \texttt{/etc/mysql/debian.cnf} \\
\end{tabular}

\indexprogramarg{evalwiki}{revisions\_to\_text}
To put some data into the database, I copied the
file \file{wiki-00114050.xml.gz} from Luca's collection
of files split out of the last dump
(\url{file://redherring.cse.ucsc.edu/~luca/wikitrust1/enwiki-20080103/114/wiki-00114050.xml.gz})
and placed it into \url{file://localhost/~thumper/research/dat/orig/}.
To unpack it, I ran a command modeled on Luca's activities from
\textit{redherring}:
\begin{verbatim}
$ cd ~/research/dat/
$ ./evalwiki -revisions_to_text -rev_base_name unpack -d ./meta ./orig/wiki-00114050.xml.gz
\end{verbatim}

\begin{verbatim}
$ cd ~/research/dat/
$ wget http://download.wikimedia.org/tools/mwdumper.jar
$ sudo aptitude install sun-java6-jre
\end{verbatim}
and copying a few files from the source code:
\begin{verbatim}
$ cp ../wikitrust/test-scripts/db_access_data.ini.sample db_access_data.ini
$ cp ../wikitrust/test-scripts/load_data.py .
\end{verbatim}
and filling in the proper data into the ini file.
Needed to add a blank value for the key ``prefix''.
Once that is set up, you can load the revisions:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
\end{verbatim}

\subsection{19-Apr-2009}

We also need to install the WikiTrust extension and
create the database tables that are used.
\begin{verbatim}
$ sudo ln -s ~/research/wikitrust /var/lib/mediawiki/extensions/
$ vi /var/lib/mediawiki/LocalSettings.php
$ sudo /etc/init.d/mysql stop
$ sudo /usr/sbin/mysqld --skip-grant-tables &
$ mysql -u root
mysql> UPDATE mysql.user SET Password=PASSWORD('info!age') WHERE User='root' AND Host='localhost';
mysql> exit
$ sudo /etc/init.d/mysql stop
$ sudo /etc/init.d/mysql start
$ cd sql
$ ./create_db.php /var/lib/mediawiki root
\end{verbatim}


\section{Testing}
\label{sec:up2date-testing}

\subsection{19-Apr-2009}

\indexprogram{downloadwp}
\begin{verbatim}
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file dlwp.log -sync_log -dump_db_calls
\end{verbatim}

\subsection{28-Apr-2009}

\indexprogram{load\_data.py}
\indexprogram{downloadwp}
Since I am still debugging the loading process, the typical sequence
of commands that I run is as follows:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}

\subsection{1-May-2009}

Based on the analysis done in Section~\ref{sec:up2date-xml},
I decided to switch away from the XML interface and
instead use the JSON interface.

While debugging the JSON code, I reached the point where exceptions
were getting thrown because of assertions failing in the code.
To get a backtrace from the exception, you run the program as such:
\begin{verbatim}
$ ocamlrun -b ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}

\subsection{9-May-2009}

The rendering of the downloaded pages was already tested,
as part of the demo described in Section~\ref{sec:genewiki-eval}.
Luca also wants timing information of a disk-based analysis.
\begin{verbatim}
$ cp ~/research/wikitrust/analysis/eval_online_wiki .
$ mkdir sigs
$ mkdir colors
$ time ./eval_online_wiki -n_events 100 -wt_db_rev_base_path unpack -wt_db_sig_base_path sigs -wt_db_colored_based_path colors -db_user wikiuser -db_pass wkuser -db_name wikidb -log_file eval.log -delete_all
\end{verbatim}
This command-line does not work.
It erases the \file{sigs} and \file{colors} directories, but does
not recreate them.

By adding the option ``\texttt{-dump\_db\_calls},''
I see that there are SQL states which are faiing.
The message displayed is:
\begin{verbatim}
INSERT INTO wikitrust_colored_markup (revision_id, revision_text) VALUES (2212643, ) ON DUPLICATE KEY UPDATE revision_text = 
Mysql.exec: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ') ON DUPLICATE KEY UPDATE revision_text =' at line 1
\end{verbatim}
There is no revision text being added to the database, which
is correct -- but the problem seems to be that the empty string
isn't being turned into SQL correctly.

\subsection{11-May-2009}

\label{sec:up2date-testing-eval}

The bad empty string isn't the only problem.
Now there is an exception raised by the \package{Gzip} module:
\begin{verbatim}
Fatal error: exception Gzip.Error("error during compression")
Called from file "filesystem_store.ml", line 67, characters 20-21
Called from file "online_page.ml", line 745, characters 48-69
Called from file "online_page.ml", line 1291, characters 11-25
Called from file "updater.ml", line 111, characters 9-14
Re-raised at file "updater.ml", line 103, characters 1-1130
Called from file "updater.ml", line 228, characters 62-63
Re-raised at file "updater.ml", line 235, characters 10-11
Called from file "eval_online_wiki.ml", line 134, characters 15-29
\end{verbatim}
The exception is being triggered because there is a zero-length
colored page which is trying to be compressed, but the \package{Gzip}
module does not support this.

I am wondering if it's reasonable to get a zero-length colored page.
Ah, but I see in the \file{colors} directory that the problem is
with \texttt{\{page 14895, rev 259084\}}, which isn't in the
corresponding \file{unpack} directory.
To fix this involves clearing the database and reloading the metadata
for the pages which have been unpacked:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ time ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log < titles.txt
$ time ./eval_online_wiki -n_events 100 -wt_db_rev_base_path unpack -wt_db_sig_base_path sigs -wt_db_colored_base_path colors -db_user wikiuser -db_pass wkuser -db_name wikidb -log_file eval.log -delete_all
\end{verbatim}

During the download process, I discovered that revision 10977256 of
\article{Insulin} is empty, and this still triggers the \package{Gzip}
error.
I modified the code to explicity check for this case and write
zero bytes to the file.  This results in a 20-byte file which
uncompresses to zero bytes, so it seems that it will do the
correct thing.

Now I can color articles with \program{eval\_online\_wiki}.
Running the optimized version on 100 revisions gives a running
time of 17.566 seconds.
A run over 1000 revisions gives a running time of 306.964 seconds.


\section{The XML Parsing Problem}
\label{sec:up2date-xml}

\subsection{28-Apr-2009}

\index{xml:space@\texttt{xml:space}}
Previously, Ian and I had discovered that paragraph separators
(double newline characters) were being lost in our code.
With some prodding by Luca, I looked harder at this issue.
The wikipedia API programmer added ``\texttt{xml:space=preserve}''
to try to help.
After looking at the \package{xml-light} source code, I've
determined that there is no support for \texttt{xml:space}
or CDATA sections or CDATA declarations in a DTD.
Multiple carriage returns can be set to be parsed as multiple
PCDATA sections (which you can then recombine with a newline
separator), which pretty much solves the problem.

I also discovered that the parser will mangle certain characters
(such as quotes) into their respective entities, which you
then have to decode to get the proper string.
This is troublesome to me, because then we can't guarantee
that the string we process is the one which is actually in
the Wikipedia database.  For example,
\begin{verbatim}
<foo>&quot;moo"</foo>
\end{verbatim}
will get parsed into
\begin{verbatim}
<foo>&quot;moo&quot;</foo>
\end{verbatim}
Although these are functionally equivalent, I'm concerned that
it would be easy for us to make a mistake and then have to
examine many parts to understand where it was introduced.

