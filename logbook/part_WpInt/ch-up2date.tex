\chapter{Updating Wikipedia Download}

\section{Introduction}

\subsection{18-Apr-2009}

Luca is unpacking the 20080103 dump of the Wikipedia.

\section{Test Environment}

\subsection{18-Apr-2009}

Need to test code changes for downloading additional revisions
of the wikipedia.
Ian's interface, \texttt{wikipedia\_api.ml}, depends on the database
already exisiting for previously downloaded information.
Unfortunately, this leaves the little problem of how to create
this test database.

To setup a database, I installed \texttt{python-mysqldb}
and \texttt{mediawiki} onto my Ubuntu instance.
Had to edit \texttt{/etc/mediawiki/mediawiki.conf} to 
enable the \texttt{Alias} for the installation.
Then navigated to \url{http://localhost/mediawiki/},
which prompted me to configure the installation.

Configuring the \textit{mediawiki} installation has a
bunch of questions to answer, most of them intuitive.
The only important ones to remember:
\begin{tabular}{r l}
Wiki name & WikiTrust \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb \\
DB username & wikuser \\
DB password & wkuser \\
Superuser name & debian-sys-maint \\
Superuser password & taken from \texttt{/etc/mysql/debian.cnf} \\
\end{tabular}

To put some data into the database, I copied the
file \texttt{wiki-00114050.xml.gz} from Luca's collection
of files split out of the last dump
(\url{file://redherring.cse.ucsc.edu/~luca/wikitrust1/enwiki-20080103/114/wiki-00114050.xml.gz})
and placed it into \url{file://localhost/~thumper/research/dat/orig/}.
To unpack it, I ran a command modeled on Luca's activities from
\textit{redherring}:
\begin{verbatim}
$ cd ~/research/dat/
$ ./evalwiki -revisions_to_text -rev_base_name unpack -d ./meta ./orig/wiki-00114050.xml.gz
\end{verbatim}

The next step is to load the unpacked revisions into the database,
which requires \texttt{mwdumper}:
\begin{verbatim}
$ cd ~/research/dat/
$ wget http://download.wikimedia.org/tools/mwdumper.jar
$ sudo aptitude install sun-java6-jre
\end{verbatim}
and copying a few files from the source code:
\begin{verbatim}
$ cp ../wikitrust/test-scripts/db_access_data.ini.sample db_access_data.ini
$ cp ../wikitrust/test-scripts/load_data.py .
\end{verbatim}
and filling in the proper data into the ini file.
Needed to add a blank value for the key ``prefix''.
Once that is set up, you can load the revisions:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
\end{verbatim}

\subsection{19-Apr-2009}

We also need to install the WikiTrust extension and
create the database tables that are used.
\begin{verbatim}
$ sudo ln -s ~/research/wikitrust /var/lib/mediawiki/extensions/
$ vi /var/lib/mediawiki/LocalSettings.php
$ sudo /etc/init.d/mysql stop
$ sudo /usr/sbin/mysqld --skip-grant-tables &
$ mysql -u root
mysql> UPDATE mysql.user SET Password=PASSWORD('info!age') WHERE User='root' AND Host='localhost';
mysql> exit
$ sudo /etc/init.d/mysql stop
$ sudo /etc/init.d/mysql start
$ cd sql
$ ./create_db.php /var/lib/mediawiki root
\end{verbatim}


\section{Testing}

\subsection{19-Apr-2009}

\begin{verbatim}
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file dlwp.log -sync_log -dump_db_calls
\end{verbatim}

\subsection{28-Apr-2009}

Since I am still debugging the loading process, the typical sequence
of commands that I run is as follows:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls
\end{verbatim}

