\chapter{Updating Wikipedia Download}

\section{Introduction}

\subsection{18-Apr-2009}

Luca is unpacking the 20080103 dump of the Wikipedia.

\section{Test Environment}
\label{sec:up2date-testenv}

\subsection{18-Apr-2009}

Need to test code changes for downloading additional revisions
of the wikipedia.
Ian's interface, \file{wikipedia\_api.ml}, depends on the database
already exisiting for previously downloaded information.
Unfortunately, this leaves the little problem of how to create
this test database.

To setup a database, I installed \package{python-mysqldb}
and \mediawiki onto my Ubuntu instance.
Had to edit \file{/etc/mediawiki/mediawiki.conf} to 
enable the \texttt{Alias} for the installation.
Then navigated to \url{http://localhost/mediawiki/},
which prompted me to configure the installation.

Configuring the \mediawiki installation has a
bunch of questions to answer, most of them intuitive.
The only important ones to remember:
\begin{tabular}{r l}
Wiki name & WikiTrust \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb \\
DB username & wikuser \\
DB password & wkuser \\
Superuser name & debian-sys-maint \\
Superuser password & taken from \texttt{/etc/mysql/debian.cnf} \\
\end{tabular}

\indexprogramarg{evalwiki}{revisions\_to\_text}
To put some data into the database, I copied the
file \file{wiki-00114050.xml.gz} from Luca's collection
of files split out of the last dump
(\url{file://redherring.cse.ucsc.edu/~luca/wikitrust1/enwiki-20080103/114/wiki-00114050.xml.gz})
and placed it into \url{file://localhost/~thumper/research/dat/orig/}.
To unpack it, I ran a command modeled on Luca's activities from
\textit{redherring}:
\begin{verbatim}
$ cd ~/research/dat/
$ ./evalwiki -revisions_to_text -rev_base_name unpack -d ./meta ./orig/wiki-00114050.xml.gz
\end{verbatim}

\begin{verbatim}
$ cd ~/research/dat/
$ wget http://download.wikimedia.org/tools/mwdumper.jar
$ sudo aptitude install sun-java6-jre
\end{verbatim}
and copying a few files from the source code:
\begin{verbatim}
$ cp ../wikitrust/test-scripts/db_access_data.ini.sample db_access_data.ini
$ cp ../wikitrust/test-scripts/load_data.py .
\end{verbatim}
and filling in the proper data into the ini file.
Needed to add a blank value for the key ``prefix''.
Once that is set up, you can load the revisions:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
\end{verbatim}

\subsection{19-Apr-2009}

We also need to install the WikiTrust extension and
create the database tables that are used.
\begin{verbatim}
$ sudo ln -s ~/research/wikitrust /var/lib/mediawiki/extensions/
$ vi /var/lib/mediawiki/LocalSettings.php
$ sudo /etc/init.d/mysql stop
$ sudo /usr/sbin/mysqld --skip-grant-tables &
$ mysql -u root
mysql> UPDATE mysql.user SET Password=PASSWORD('info!age') WHERE User='root' AND Host='localhost';
mysql> exit
$ sudo /etc/init.d/mysql stop
$ sudo /etc/init.d/mysql start
$ cd sql
$ ./create_db.php /var/lib/mediawiki root
\end{verbatim}


\section{Testing}
\label{sec:up2date-testing}

\subsection{19-Apr-2009}

\indexprogram{downloadwp}
\begin{verbatim}
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file dlwp.log -sync_log -dump_db_calls
\end{verbatim}

\subsection{28-Apr-2009}

\indexprogram{load\_data.py}
\indexprogram{downloadwp}
Since I am still debugging the loading process, the typical sequence
of commands that I run is as follows:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}

\subsection{1-May-2009}

Based on the analysis done in Section~\ref{sec:up2date-xml},
I decided to switch away from the XML interface and
instead use the JSON interface.

While debugging the JSON code, I reached the point where exceptions
were getting thrown because of assertions failing in the code.
To get a backtrace from the exception, you run the program as such:
\begin{verbatim}
$ ocamlrun -b ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}

\subsection{9-May-2009}

The rendering of the downloaded pages was already tested,
as part of the demo described in Section~\ref{sec:genewiki-eval}.
Luca also wants timing information of a disk-based analysis.
\begin{verbatim}
$ cp ~/research/wikitrust/analysis/eval_online_wiki .
$ mkdir sigs
$ mkdir colors
$ time ./eval_online_wiki -n_events 100 -wt_db_rev_base_path unpack -wt_db_sig_base_path sigs -wt_db_colored_base_path colors -db_user wikiuser -db_pass wkuser -db_name wikidb -log_file eval.log -delete_all
\end{verbatim}
This command-line does not work.
It erases the \file{sigs} and \file{colors} directories, but does
not recreate them.

By adding the option ``\texttt{-dump\_db\_calls},''
I see that there are SQL states which are faiing.
The message displayed is:
\begin{verbatim}
INSERT INTO wikitrust_colored_markup (revision_id, revision_text) VALUES (2212643, ) ON DUPLICATE KEY UPDATE revision_text = 
Mysql.exec: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ') ON DUPLICATE KEY UPDATE revision_text =' at line 1
\end{verbatim}
There is no revision text being added to the database, which
is correct -- but the problem seems to be that the empty string
isn't being turned into SQL correctly.

\subsection{11-May-2009}

\label{sec:up2date-testing-eval}

The bad empty string isn't the only problem.
Now there is an exception raised by the \package{Gzip} module:
\begin{verbatim}
Fatal error: exception Gzip.Error("error during compression")
Called from file "filesystem_store.ml", line 67, characters 20-21
Called from file "online_page.ml", line 745, characters 48-69
Called from file "online_page.ml", line 1291, characters 11-25
Called from file "updater.ml", line 111, characters 9-14
Re-raised at file "updater.ml", line 103, characters 1-1130
Called from file "updater.ml", line 228, characters 62-63
Re-raised at file "updater.ml", line 235, characters 10-11
Called from file "eval_online_wiki.ml", line 134, characters 15-29
\end{verbatim}
The exception is being triggered because there is a zero-length
colored page which is trying to be compressed, but the \package{Gzip}
module does not support this.

I am wondering if it's reasonable to get a zero-length colored page.
Ah, but I see in the \file{colors} directory that the problem is
with \texttt{\{page 14895, rev 259084\}}, which isn't in the
corresponding \file{unpack} directory.
To fix this involves clearing the database and reloading the metadata
for the pages which have been unpacked:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ time ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log < titles.txt
$ time ./eval_online_wiki -n_events 100 -wt_db_rev_base_path unpack -wt_db_sig_base_path sigs -wt_db_colored_base_path colors -db_user wikiuser -db_pass wkuser -db_name wikidb -log_file eval.log -delete_all
\end{verbatim}

During the download process, I discovered that revision 10977256 of
\article{Insulin} is empty, and this still triggers the \package{Gzip}
error.
I modified the code to explicity check for this case and write
zero bytes to the file.  This results in a 20-byte file which
uncompresses to zero bytes, so it seems that it will do the
correct thing.

Now I can color articles with \program{eval\_online\_wiki}.
Running the optimized version on 100 revisions gives a running
time of 17.566 seconds.
A run over 1000 revisions gives a running time of 306.964 seconds.


\section{The XML Parsing Problem}
\label{sec:up2date-xml}

\subsection{28-Apr-2009}

\index{xml:space@\texttt{xml:space}}
Previously, Ian and I had discovered that paragraph separators
(double newline characters) were being lost in our code.
With some prodding by Luca, I looked harder at this issue.
The wikipedia API programmer added ``\texttt{xml:space=preserve}''
to try to help.
After looking at the \package{xml-light} source code, I've
determined that there is no support for \texttt{xml:space}
or CDATA sections or CDATA declarations in a DTD.
Multiple carriage returns can be set to be parsed as multiple
PCDATA sections (which you can then recombine with a newline
separator), which pretty much solves the problem.

I also discovered that the parser will mangle certain characters
(such as quotes) into their respective entities, which you
then have to decode to get the proper string.
This is troublesome to me, because then we can't guarantee
that the string we process is the one which is actually in
the Wikipedia database.  For example,
\begin{verbatim}
<foo>&quot;moo"</foo>
\end{verbatim}
will get parsed into
\begin{verbatim}
<foo>&quot;moo&quot;</foo>
\end{verbatim}
Although these are functionally equivalent, I'm concerned that
it would be easy for us to make a mistake and then have to
examine many parts to understand where it was introduced.

\section{Templates}

\subsection{16-Aug-2009}

In preparation for our demo, we would like to include templates
in our copy of Wikipedia so that they are expanded properly in
the colored version of the text.
I modified \program{downloadwp} to accept a revision id to start
requests from, separated from the title by a tab character.
It is important to remember that template expansion happens
inside the \program{mediawiki} wiki parsing engine;
therefore, the templates need to be available inside the
database (instead of using our disk-based system for storing files).

For information on how I loaded the templates onto Redherring,
see Section~\ref{ch:redherring-templates}.

\subsection{27-Aug-2009}

Ian had the great idea to use the Wikipedia API to convert our
markup into HTML, thus getting the template expansion ``for free.''
He modified the PHP code to do this, and it worked great, even
giving us access to the Wikipedia images.

\section{The English Wikipedia}

\subsection{13-Sept-2009}

The English Wikipedia is still too big to do anything reasonable with.
In preparation for our Argentina demo, I tried to move the \mysql
stored metadata to a server at Amazon EC2.
The file was 5GB compressed (18GB uncompressed), so it was easy
to transfer.
The difficulty was in reloading the data into \mysql.
I even set up a 4-disk RAID array, but the process was so slow
that after four days I terminated the instance.
I expect that it was the indices that were causing trouble,
but didn't want to experiment so close to our trip.

In the longer run, I think it's impractical to try to keep a ``live''
copy of the English Wikipedia running --- it would be too difficult
to transfer the data to another server, so it would only be useful
for generating dumps.
But then, why not just generate dump files directly?

To that end, I wrote a script which would take the \program{splitwiki}
output chunks of the \enwiki dataset, and generate a new set
of chunks\ldots one per article.
On host Redherring, I ran the following sequence:
\begin{verbatim}
$ cd ~luca/wikitrust2/thumper
$ ./split-chunks2articles.pl
\end{verbatim}
This ran overnight, and was complete by morning.

\subsection{13-Dec-2009}

Unfortunately, I ran into a problem which I didn't document here.
Basically, the \program{splitwiki} code had transformed the XML entities
so that the resulting files could not be parsed by an XML parser.

I have written a variety of tools to experiment with how to handle the data,
both directly from the dump and from an article tree that I created.
None have performed very quickly, which is disappointing.
\begin{itemize}
\item \program{bzip2} only decompresses at about 2MB/s.
	There is a variant, \program{pbzip2}, which uses multiple cores,
	but the compressed file must have used this parallel version.
\item An early version of my article tree placed 10K+ files per directory
	(using only the last three digits to name the directory),
	but this turns out to be very slow when accessing the directories.
\item I forgot that the complete dump includes pages in other namespaces,
	which is a significant overhead when handling the files.
\item Perl's \function{substr} function can't handle strings larger than 2GB.
\item Other perl functions (like regex) seem to fail at 2GB.
\end{itemize}

Running some timings on a smaller 1GB set of data:
\begin{verbatim}
redherring:/giant/thumper> time bash -c 'bzcat enwiki2.xml.bz2 | ./research/wt-project/projects/enwiki-up2date/split-dump2articles.pl'
processed 15 pages
51.130u 4.140s 0:38.99 141.7%   0+0k 2712+2103856io 5pf+0w
\end{verbatim}
This has the problem that it doesn't track the children processes.
When I add in that time, \program{bzip2} requires 4 minutes total
and \program{pbzip2} uses 69 seconds.

The \textit{WideFinder}-based solution also runs in 38 seconds,
when using \program{pbzip2} at maximum compression (but this time
the children processes are included in the time).
Turning this down to the fastest compression yields a 22 second result:
\begin{verbatim}
time bash -c './research/wt-project/projects/enwiki-up2date/merge-dump3articles.pl enwiki2.xml'
157.650u 9.150s 0:22.51 741.0%  0+0k 0+492904io 0pf+0w
\end{verbatim}

This argues that I should fix the WideFinder solution to handle files larger than 2GB.
Except that the perl limitation of 2GB for \function{mmap} makes it
quite cumbersome to write correct code for every case.
But really, it seems to me that the linear version makes several assumptions
that are similar to the problems I'm trying to solve in the sliding-window case.

\subsection{11-Jan-2010}

I did the changes to implement a sliding window,
with the assumption that no single page would go over 1GB in revision size.
This didn't quite work, as two pages failed that assumption
(``Ronald Reagan'' and ``Talk::Intelligent Design''),
so I modified the code to skip over such pages.

Despite all these performance enhancements, the processing time
was still quite slow.
It eventually finished, probably in about 7 days of running.
With all the debugging, this was right around the end of the year.

Just before Christmas, I spoke with Luca and he suggested that
he should modify the batch code to read from my article tree
and that I should focus only on getting the newer revisions.
I agreed that this was maybe a good idea, because I had some worries
about still trying to merge the dump data with the partial download
from summer.
By focusing on just the downloads, I could skip a step or
two that were likely to require some debugging.

I worked on the downloading code over Christmas and New Year's,
and this finally completed updating yesterday.
Even with the download, there were various errors I encountered
like partial JSON results and file corruption from hitting ctrl-C.
I put in some error checking and a way to gracefully stop the
programs, and was eventually able to do away with all the errors.

Performance wise, I've discovered that the \program{sort-one-article.pl}
script uses a lot of CPU.
I added checks so that it does not write out a new file if
the input was already sorted, but it still uses too much CPU.
My conclusion is that the XML parsing is quite slow.

Rather than fix the speed of XML parsing, an alternate solution
is to better know when a file needs resorting.
(Most of the time it doesn't.)
This would require a change to \program{download-by-title.pl}
so that it tracks the last timestamp of the file,
and verifies that each new revision downloaded is monotonically
increasing.

\section{Fixing Page Titles}

\subsection{18-Oct-2010}

Over the last month, we've come to discover that the ambiguity
of whether page titles use spaces or underscores was causing us
some performance problems.

After a substantial reworking of the code, tonight we will try
rolling out the new code onto the foreign language instances we run.
Before we do that, it's important to fix the existing page titles.
To do that, we use the following command:
\begin{verbatim}
mysql> update page set page_title = replace(page_title, '_', ' ');
\end{verbatim}

I forgot that I also needed to \texttt{chown} the revision cache
to the ``wikitrust'' user.  Now trying to correct that.

The Polish wikipedia was my first test case.
It doesn't seem to be downloading any page updates at all.
Some investigation turned up that it has 25~pages from the
Fresh wikipedia, with revisions so high that they are beyond
what is available in the Polish wikipedia.

In order to fix the situation, I had to drop the old
``wikitrust\_queue'' table and recreate it so that it used
\textbf{page\_id} as the primary key.
Then I had to load all the bad pages into the queue, which
I did with this command:
\begin{verbatim}
mysql> insert into wikitrust_queue (page_id) select distinct rev_page from revision where rev_id > 25000000;
\end{verbatim}

\subsection{19-Oct-2010}

An overnight run gave good results.
Once I fixed the permission errors, the only error I saw was
the ``Missing\_trust'' error.

I also created a script to fetch pages from the \textbf{Recent
Changes} list on Wikipedia.
I placed it into \file{projects/up2date} for my reference, so that
it is not confused with Ian's PHP script to do a similar task.
The purpose of my script was to get around the dependence on the
Wikipedia Toolserver, but it's inadequate because it will only fetch
500~revisions at a time (not pages, so it's very wasteful).

Ian has since patched his script to accept a \textit{max} parameter
which limits how many pages are returned by the query.
This turns out to be necessary because the script has recently started
failing (either because the result is too long or maybe because the
script is taking too long).

Also, I found the same problem of having bad revisions in the
Portugeuse wiki.
I used the same trick for adding it to the dispatcher queue,
but discovered that I had left out a step.
It's important to change the page title to something bogus to force
the dispatcher to delete the current revision data and download new data.
\begin{verbatim}
mysql> update page set page_title=concat('XXX', page_id) where page_id
in (select distinct rev_page from revision where rev_id > 25000000);
mysql> insert into wikitrust_queue (page_id) select distinct rev_page from revision where rev_id > 25000000;
\end{verbatim}

