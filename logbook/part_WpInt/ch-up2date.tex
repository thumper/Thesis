\chapter{Updating Wikipedia Download}

\section{Introduction}

\subsection{18-Apr-2009}

Luca is unpacking the 20080103 dump of the Wikipedia.

\section{Test Environment}

\subsection{18-Apr-2009}

Need to test code changes for downloading additional revisions
of the wikipedia.
Ian's interface, \file{wikipedia\_api.ml}, depends on the database
already exisiting for previously downloaded information.
Unfortunately, this leaves the little problem of how to create
this test database.

To setup a database, I installed \program{python-mysqldb}
and \mediawiki onto my Ubuntu instance.
Had to edit \file{/etc/mediawiki/mediawiki.conf} to 
enable the \texttt{Alias} for the installation.
Then navigated to \url{http://localhost/mediawiki/},
which prompted me to configure the installation.

Configuring the \mediawiki installation has a
bunch of questions to answer, most of them intuitive.
The only important ones to remember:
\begin{tabular}{r l}
Wiki name & WikiTrust \\
Admin username & WikiSysop \\
Admin password & wksysop \\
Database name & wikidb \\
DB username & wikuser \\
DB password & wkuser \\
Superuser name & debian-sys-maint \\
Superuser password & taken from \texttt{/etc/mysql/debian.cnf} \\
\end{tabular}

\index{evalwiki!revisions\_to\_text}
To put some data into the database, I copied the
file \file{wiki-00114050.xml.gz} from Luca's collection
of files split out of the last dump
(\url{file://redherring.cse.ucsc.edu/~luca/wikitrust1/enwiki-20080103/114/wiki-00114050.xml.gz})
and placed it into \url{file://localhost/~thumper/research/dat/orig/}.
To unpack it, I ran a command modeled on Luca's activities from
\textit{redherring}:
\begin{verbatim}
$ cd ~/research/dat/
$ ./evalwiki -revisions_to_text -rev_base_name unpack -d ./meta ./orig/wiki-00114050.xml.gz
\end{verbatim}

\begin{verbatim}
$ cd ~/research/dat/
$ wget http://download.wikimedia.org/tools/mwdumper.jar
$ sudo aptitude install sun-java6-jre
\end{verbatim}
and copying a few files from the source code:
\begin{verbatim}
$ cp ../wikitrust/test-scripts/db_access_data.ini.sample db_access_data.ini
$ cp ../wikitrust/test-scripts/load_data.py .
\end{verbatim}
and filling in the proper data into the ini file.
Needed to add a blank value for the key ``prefix''.
Once that is set up, you can load the revisions:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
\end{verbatim}

\subsection{19-Apr-2009}

We also need to install the WikiTrust extension and
create the database tables that are used.
\begin{verbatim}
$ sudo ln -s ~/research/wikitrust /var/lib/mediawiki/extensions/
$ vi /var/lib/mediawiki/LocalSettings.php
$ sudo /etc/init.d/mysql stop
$ sudo /usr/sbin/mysqld --skip-grant-tables &
$ mysql -u root
mysql> UPDATE mysql.user SET Password=PASSWORD('info!age') WHERE User='root' AND Host='localhost';
mysql> exit
$ sudo /etc/init.d/mysql stop
$ sudo /etc/init.d/mysql start
$ cd sql
$ ./create_db.php /var/lib/mediawiki root
\end{verbatim}


\section{Testing}

\subsection{19-Apr-2009}

\index{downloadwp}
\begin{verbatim}
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file dlwp.log -sync_log -dump_db_calls
\end{verbatim}

\subsection{28-Apr-2009}

\index{load\_data.py}
\index{downloadwp}
Since I am still debugging the loading process, the typical sequence
of commands that I run is as follows:
\begin{verbatim}
$ ./load_data.py --clear_db meta/wiki-00114050.xml
$ cp ~/research/wikitrust/remote/analysis/downloadwp .
$ ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}

\subsection{1-May-2009}

While debugging the JSON code, I reached the point where exceptions
were getting thrown because of assertions failing in the code.
To get a backtrace from the exception, you run the program as such:
\begin{verbatim}
$ ocamlrun -b ./downloadwp -wt_db_rev_base_path unpack -db_pass wkuser -log_file download.log -dump_db_calls > download-sql.log
\end{verbatim}



\section{The XML Parsing Problem}

\subsection{28-Apr-2009}

Previously, Ian and I had discovered that paragraph separators
(double newline characters) were being lost in our code.
With some prodding by Luca, I looked harder at this issue.
The wikipedia API programmer added ``xml:space=preserve''
to try to help.
After looking at the \textbf{xml-light} source code, I've
determined that there is no support for \texttt{xml:space}
or CDATA sections or CDATA declarations in a DTD.
Multiple carriage returns can be set to be parsed as multiple
PCDATA sections (which you can then recombine with a newline
separator), which pretty much solves the problem.

I also discovered that the parser will mangle certain characters
(such as quotes) into their respective entities, which you
then have to decode to get the proper string.
This is troublesome to me, because then we can't guarantee
that the string we process is the one which is actually in
the Wikipedia database.  For example,
\begin{verbatim}
<foo>&quot;moo"</foo>
\end{verbatim}
will get parsed into
\begin{verbatim}
<foo>&quot;moo&quot;</foo>
\end{verbatim}
Although these are functionally equivalent, I'm concerned that
it would be easy for us to make a mistake and then have to
examine many parts to understand where it was introduced.

