\section{Conclusions}

When reading content online, librarians caution us to cross-check
information with published materials.
Although trumpeted as a triumph for free speech, the essentially
anonymous nature of the Internet happens to also be a good
mouth-piece for unpopular, fringe ideas.
The equal opportunity for expression and exposure
in this medium makes critical evaluation by
the reader an important skill.
A key component of this evaluation is assessing
the quality of a site.

The problem of measuring the quality of web pages
has gone in two directions.
Some sites act as authorities (\eg Dmoz.org and Yahoo.com),
and are primarily driven by human assessments.
Other sites have developed automated methods (or relevance rankings)
for rating web content (\eg Google.com and Yahoo.com).
Both methods provide popular results to users
searching for content, but don't provide direct quality
measurements to the user.

The quality of content on group collaboration sites
is even more problematic to judge, since the content
can be changed at any moment.
This is a significant issue for the Wikipedia,
which attempts to provide encyclopedic information
through the combined (and sometimes anonymous)
efforts of everyone on the Internet.
In the future, it might be possible to use
natural language processing techniques to identify and
automatically verify factual content, as well as flag biased content.
To date, the project has had to rely on human assessments
of the content, and uses a simplistic system
to label the quality of a portion of their content.

We have proposed automated methods for evaluating the
quality of authors, and (separately) the text they create.
By relying on the history of edits, we can draw inferences
about the quality of the text that comes earlier in the history.
Instead of measuring ``truth,'' which is a slippery concept,
we measure ``group consensus.''
As article text stabilizes over time, we conclude that it
has reached a form which most members of the community can
reasonably agree on.
Visualization of the text trust information gives direct
feedback to users about the age (and thus, agreement level)
of text, and highlights regions that might still be contentious.
As group collaboration increases in prominence on the Internet,
we feel that this research will open the door on new applications
and quality measurements.

