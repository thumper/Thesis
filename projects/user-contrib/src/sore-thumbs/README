
======================
    Treat each user in isolation, and calculate the average and variance
of their scores.
"Type 1" output is when any one of the contrib scores is beyond
a the allowed multiple of this variance.

Once all the variances are calculated, we then compute the average
of the variances.  The input is then reprocessed:
"Type 2" is output when any of the contributions is beyond the
allowed mutiple of this average-variance.
    This model presumes that there is a correlation between the
scores, and then looks for points which are beyond the normal limit.
In essense, it should detect when all the scores but one "agree".

	average-variance.pl data-rankings.txt > output.txt

======================
    To do a linear regression, we can look at each contribution as
a component of a multi-dimensional point.  Assuming that all the
contributions are correlated with each other, do a linear regression
to find the correlation and then find the outliers.
    First, we need the raw data in a way that matlab can read:
	compile-rawdata.pl --normalize /Volumes/LucaWikiMac/stats-vishwa/contrib-* > output.txt

======================

    Another idea for finding users that are not ranked the same
by different contribution methods is to compare the rankings
generated by each method.
	./compile-rankings.pl /Volumes/LucaWikiMac/stats-vishwa/contrib-* > data-rankings.txt

======================

    To compare how the ReputationExact metric behaves with
the reputation bins we create, run:
	./compile-repbin.pl /Volumes/LucaWikiMac/stats-vishwa/contrib-reputationexact.txt

======================

To visualize the outliers from average-variance, do:

	./average-variance.pl rankings.txt > outliers.txt
	./extract-outliers.pl outliers.txt rankings.txt outliers2

======================

To pick out a file of UIDs...

	cat uid.txt | ./extract-entries.pl rankings.txt > special-rankings.txt

=====================

Extract bot uids:
	./extract-botuids.pl < bots.txt > uids-bots.txt

